Selected network (train.py) = bimodal
bimodal selected
Training seq
DEVICE = cpu
####################
Total Parameters = 605185
Total Trainable Parameters = 605185
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
####################
Epochs = 15
EPOCH 1:
  batch 1 loss: 0.6936571598052979
  batch 2 loss: 0.6932485699653625
  batch 3 loss: 0.6935972372690836
  batch 4 loss: 0.6933631002902985
  batch 5 loss: 0.6938181757926941
  batch 6 loss: 0.6941258410612742
  batch 7 loss: 0.6941588691302708
  batch 8 loss: 0.6942578107118607
  batch 9 loss: 0.6943691770235697
  batch 10 loss: 0.6943984627723694
  batch 11 loss: 0.6941839402372186
  batch 12 loss: 0.6941633075475693
  batch 13 loss: 0.6938788386491629
  batch 14 loss: 0.6934604261602674
  batch 15 loss: 0.6932145039240519
  batch 16 loss: 0.692933663725853
  batch 17 loss: 0.6926536910674151
  batch 18 loss: 0.69243985414505
  batch 19 loss: 0.6925167159030312
  batch 20 loss: 0.692168390750885
  batch 21 loss: 0.6919612430390858
  batch 22 loss: 0.691539856520566
  batch 23 loss: 0.6913761159648066
  batch 24 loss: 0.6908316190044085
  batch 25 loss: 0.6906672167778015
  batch 26 loss: 0.689571357690371
  batch 27 loss: 0.6891838466679608
  batch 28 loss: 0.687951324241502
  batch 29 loss: 0.6877604476336775
  batch 30 loss: 0.6872230549653371
  batch 31 loss: 0.6864893551795713
  batch 32 loss: 0.6859887037426233
  batch 33 loss: 0.6858275153420188
  batch 34 loss: 0.6847503220333773
  batch 35 loss: 0.6843728882925851
  batch 36 loss: 0.6839120884736379
  batch 37 loss: 0.6836387950020868
  batch 38 loss: 0.6838137052561107
  batch 39 loss: 0.683538019657135
  batch 40 loss: 0.6832263618707657
  batch 41 loss: 0.6826778970113615
  batch 42 loss: 0.6818012396494547
  batch 43 loss: 0.6810348851736202
  batch 44 loss: 0.6810523271560669
  batch 45 loss: 0.6806526223818461
  batch 46 loss: 0.6799620234447977
  batch 47 loss: 0.6795407305372522
  batch 48 loss: 0.6794274263083935
  batch 49 loss: 0.6786134826893709
  batch 50 loss: 0.67782439827919
  batch 51 loss: 0.6770354719722972
  batch 52 loss: 0.6768087389377447
  batch 53 loss: 0.6760478615760803
  batch 54 loss: 0.6748376290003458
  batch 55 loss: 0.673792961510745
  batch 56 loss: 0.6734273646559034
  batch 57 loss: 0.6724748716019747
  batch 58 loss: 0.6715105895338387
  batch 59 loss: 0.6711440086364746
  batch 60 loss: 0.6704919288555781
  batch 61 loss: 0.6698659707288273
  batch 62 loss: 0.668790614412677
  batch 63 loss: 0.6677679211374313
  batch 64 loss: 0.6668874127790332
  batch 65 loss: 0.6660400106356694
  batch 66 loss: 0.6646107444257447
  batch 67 loss: 0.6638322917383108
  batch 68 loss: 0.6631228897501441
  batch 69 loss: 0.6622120971265046
  batch 70 loss: 0.6614088109561376
  batch 71 loss: 0.6607180632336039
  batch 72 loss: 0.6599144198828273
  batch 73 loss: 0.6591893524339755
  batch 74 loss: 0.6583866307864318
  batch 75 loss: 0.6574143195152282
  batch 76 loss: 0.656174372685583
  batch 77 loss: 0.6549756573392199
  batch 78 loss: 0.6539315695946033
  batch 79 loss: 0.6528564691543579
  batch 80 loss: 0.6518400274217129
  batch 81 loss: 0.6510470693494067
  batch 82 loss: 0.6499442683487404
  batch 83 loss: 0.6486834201468042
  batch 84 loss: 0.6475824472450075
  batch 85 loss: 0.6464543812415179
  batch 86 loss: 0.6454527530559274
  batch 87 loss: 0.644557940548864
  batch 88 loss: 0.6432499932971868
  batch 89 loss: 0.6420503841357285
  batch 90 loss: 0.6409129467275407
  batch 91 loss: 0.6393735441532764
  batch 92 loss: 0.6380631826494051
  batch 93 loss: 0.6369940997451864
  batch 94 loss: 0.636141927952462
  batch 95 loss: 0.6345352822228482
  batch 96 loss: 0.6333132848764459
  batch 97 loss: 0.6322126133540242
  batch 98 loss: 0.6311927051568518
  batch 99 loss: 0.6298796417135181
  batch 100 loss: 0.6288549152016639
  batch 101 loss: 0.627815440444663
  batch 102 loss: 0.6271519757368985
  batch 103 loss: 0.6255532682520671
  batch 104 loss: 0.6245431103385412
  batch 105 loss: 0.6232691305024284
  batch 106 loss: 0.6224046169586901
  batch 107 loss: 0.621455503958408
  batch 108 loss: 0.6202921447930513
  batch 109 loss: 0.6193696594019549
  batch 110 loss: 0.6183386650952426
  batch 111 loss: 0.6173763366432877
  batch 112 loss: 0.6162539803023849
  batch 113 loss: 0.6148981499460946
  batch 114 loss: 0.6139258146286011
  batch 115 loss: 0.6125044776045758
  batch 116 loss: 0.6117324212501789
  batch 117 loss: 0.6109144483876024
  batch 118 loss: 0.6102998574911538
  batch 119 loss: 0.6094277270701753
  batch 120 loss: 0.6083232864737511
  batch 121 loss: 0.6075656330289919
  batch 122 loss: 0.6067255940593657
  batch 123 loss: 0.60555013553883
  batch 124 loss: 0.6046124819305635
  batch 125 loss: 0.6033290159702301
  batch 126 loss: 0.6024099590759429
  batch 127 loss: 0.6014301736993114
  batch 128 loss: 0.600153747247532
  batch 129 loss: 0.5991911698681439
  batch 130 loss: 0.5983377944964628
  batch 131 loss: 0.5974337069587853
  batch 132 loss: 0.5967041829770262
  batch 133 loss: 0.5956925270252658
  batch 134 loss: 0.5946003040715829
  batch 135 loss: 0.5937440578584318
  batch 136 loss: 0.5928100599523853
  batch 137 loss: 0.5917413893842349
  batch 138 loss: 0.5909027861080308
  batch 139 loss: 0.5900139066812803
  batch 140 loss: 0.5892697525875908
  batch 141 loss: 0.5882990421555566
  batch 142 loss: 0.5873030776708899
  batch 143 loss: 0.5863552633282187
  batch 144 loss: 0.5849853296660714
  batch 145 loss: 0.5841054301837395
  batch 146 loss: 0.5831944856741657
  batch 147 loss: 0.5823837518692017
  batch 148 loss: 0.5814169184178919
  batch 149 loss: 0.5802538700951826
  batch 150 loss: 0.5793117890755336
  batch 151 loss: 0.578283782234255
  batch 152 loss: 0.5771766950033212
  batch 153 loss: 0.576432920554105
  batch 154 loss: 0.5755608633354112
  batch 155 loss: 0.5747121603258194
  batch 156 loss: 0.5738713584649258
  batch 157 loss: 0.5728935523397604
  batch 158 loss: 0.5723330655807182
  batch 159 loss: 0.571714645474212
  batch 160 loss: 0.5705399617552758
  batch 161 loss: 0.5694997640129942
  batch 162 loss: 0.5686200489232569
  batch 163 loss: 0.5678867162005302
  batch 164 loss: 0.5674849038080472
  batch 165 loss: 0.5668061660997795
  batch 166 loss: 0.5660738727773529
  batch 167 loss: 0.5652974711920686
  batch 168 loss: 0.5643841212704068
  batch 169 loss: 0.5637343942060978
  batch 170 loss: 0.5629863993209951
  batch 171 loss: 0.5622989172823945
  batch 172 loss: 0.5614948622709097
  batch 173 loss: 0.5606398305107403
  batch 174 loss: 0.5600767654591593
  batch 175 loss: 0.55914456146104
  batch 176 loss: 0.5586125115779313
  batch 177 loss: 0.5579426443509463
  batch 178 loss: 0.5568579596080138
  batch 179 loss: 0.5560557384397731
  batch 180 loss: 0.555177842742867
  batch 181 loss: 0.55442506190163
  batch 182 loss: 0.5535916464996862
  batch 183 loss: 0.5528278150519387
  batch 184 loss: 0.5520309281090031
  batch 185 loss: 0.5511456505672352
  batch 186 loss: 0.5504991465037868
  batch 187 loss: 0.5500268805473246
  batch 188 loss: 0.5491793783421212
  batch 189 loss: 0.5486393949342152
  batch 190 loss: 0.5478920164861177
  batch 191 loss: 0.5472574691185776
  batch 192 loss: 0.546433322597295
  batch 193 loss: 0.5457372599006317
  batch 194 loss: 0.54496172608174
  batch 195 loss: 0.5445199471253616
  batch 196 loss: 0.5439140384598654
  batch 197 loss: 0.5432711136220071
  batch 198 loss: 0.5427642706969772
  batch 199 loss: 0.5422361656349508
  batch 200 loss: 0.5415261343121529
  batch 201 loss: 0.5406139368441567
  batch 202 loss: 0.5397723072236127
  batch 203 loss: 0.5391882439258651
  batch 204 loss: 0.5384999089965633
  batch 205 loss: 0.537741891930743
  batch 206 loss: 0.5369514548084111
  batch 207 loss: 0.5362767729782252
  batch 208 loss: 0.5354513629124715
  batch 209 loss: 0.5349447865520368
  batch 210 loss: 0.534238986600013
  batch 211 loss: 0.5335942423456653
  batch 212 loss: 0.5329354224058817
  batch 213 loss: 0.5321692673253341
  batch 214 loss: 0.531603995606164
  batch 215 loss: 0.5309283945449563
  batch 216 loss: 0.5305855761247652
  batch 217 loss: 0.529732667081367
  batch 218 loss: 0.5291039909518093
  batch 219 loss: 0.5283519801211684
  batch 220 loss: 0.5279171395030888
  batch 221 loss: 0.5274538738964909
  batch 222 loss: 0.5269673888210777
  batch 223 loss: 0.5263587554474048
  batch 224 loss: 0.5259461450789656
  batch 225 loss: 0.5254605072074466
  batch 226 loss: 0.5249529991265947
  batch 227 loss: 0.5243586749494864
  batch 228 loss: 0.5237539018455305
  batch 229 loss: 0.5232982995989021
  batch 230 loss: 0.5226706561834916
  batch 231 loss: 0.5221945945318643
  batch 232 loss: 0.5214976932981918
  batch 233 loss: 0.5208571775788401
  batch 234 loss: 0.5203849320483004
  batch 235 loss: 0.5198233050234774
  batch 236 loss: 0.5192238055800987
  batch 237 loss: 0.5189916465101363
  batch 238 loss: 0.518438105823613
  batch 239 loss: 0.5180875084140809
  batch 240 loss: 0.5178094590703647
  batch 241 loss: 0.5172977433907046
  batch 242 loss: 0.5168641821658315
  batch 243 loss: 0.5164097280168729
  batch 244 loss: 0.5160125545302375
  batch 245 loss: 0.5156721780494767
  batch 246 loss: 0.5153432506613616
  batch 247 loss: 0.5148948776094537
  batch 248 loss: 0.5144815637219337
  batch 249 loss: 0.514041755453171
  batch 250 loss: 0.513496689915657
  batch 251 loss: 0.5128578721764553
  batch 252 loss: 0.5123095437884331
  batch 253 loss: 0.5118714077434993
  batch 254 loss: 0.511449879783345
  batch 255 loss: 0.5107536221251768
  batch 256 loss: 0.5102986972779036
  batch 257 loss: 0.5100489817704672
  batch 258 loss: 0.5094426545754883
  batch 259 loss: 0.5088475933406343
  batch 260 loss: 0.5083853255097682
  batch 261 loss: 0.5079973180175285
  batch 262 loss: 0.5074927091371012
  batch 263 loss: 0.5069738092757903
  batch 264 loss: 0.5067164292841246
  batch 265 loss: 0.5062979715050392
  batch 266 loss: 0.5058454188861345
  batch 267 loss: 0.5052382754029406
  batch 268 loss: 0.5047053922690562
  batch 269 loss: 0.5042382067685677
  batch 270 loss: 0.5039216932323244
  batch 271 loss: 0.5035231921725607
  batch 272 loss: 0.5030225634794024
  batch 273 loss: 0.5026418436796237
  batch 274 loss: 0.502018507923523
  batch 275 loss: 0.5015480778434059
  batch 276 loss: 0.5011347115471743
  batch 277 loss: 0.5006972847217257
  batch 278 loss: 0.5002372391361127
  batch 279 loss: 0.4999532675016738
  batch 280 loss: 0.499497976154089
  batch 281 loss: 0.4991451111552554
  batch 282 loss: 0.4986485220650409
  batch 283 loss: 0.49817522580969037
  batch 284 loss: 0.49777531036188905
  batch 285 loss: 0.4972882751832929
  batch 286 loss: 0.49690340855321685
  batch 287 loss: 0.4964084616936873
  batch 288 loss: 0.4960448840219114
  batch 289 loss: 0.49551179138846874
  batch 290 loss: 0.4950685450742985
  batch 291 loss: 0.4946565890230264
  batch 292 loss: 0.4942372200423724
  batch 293 loss: 0.4938613186517266
  batch 294 loss: 0.4934068188375356
  batch 295 loss: 0.4930090993137683
  batch 296 loss: 0.49276492200993205
  batch 297 loss: 0.49246549977597964
  batch 298 loss: 0.49208597178827196
  batch 299 loss: 0.4918371661251604
  batch 300 loss: 0.49134758005539575
  batch 301 loss: 0.49106218420785924
  batch 302 loss: 0.4907797616443887
  batch 303 loss: 0.4904169512070445
  batch 304 loss: 0.49012572739861515
  batch 305 loss: 0.4897540789158618
  batch 306 loss: 0.4895568963359384
  batch 307 loss: 0.48914651732879666
  batch 308 loss: 0.488828259435567
  batch 309 loss: 0.4884247740302657
  batch 310 loss: 0.48815804181560396
  batch 311 loss: 0.48774575794241437
  batch 312 loss: 0.487385525344274
  batch 313 loss: 0.4870788240775514
  batch 314 loss: 0.48680815043722747
  batch 315 loss: 0.48654535015424094
  batch 316 loss: 0.48621415856141076
  batch 317 loss: 0.48606561307651386
  batch 318 loss: 0.4856640632797337
  batch 319 loss: 0.4851413242121849
  batch 320 loss: 0.48498416487127544
  batch 321 loss: 0.484656252872164
  batch 322 loss: 0.4843162289872673
  batch 323 loss: 0.48398706898231625
  batch 324 loss: 0.4835197177750093
  batch 325 loss: 0.4832678913153135
  batch 326 loss: 0.483084961771965
  batch 327 loss: 0.4827470838659036
  batch 328 loss: 0.4824568981804499
  batch 329 loss: 0.48221934593557225
  batch 330 loss: 0.4819094427607276
  batch 331 loss: 0.4816209943092842
  batch 332 loss: 0.4814101285245045
  batch 333 loss: 0.48103848493492996
  batch 334 loss: 0.48091761410950185
  batch 335 loss: 0.4806146794290685
  batch 336 loss: 0.4802834836854821
  batch 337 loss: 0.48000264344654026
  batch 338 loss: 0.47963445832038065
  batch 339 loss: 0.4792822003364563
  batch 340 loss: 0.47887394419487783
  batch 341 loss: 0.4785027353644721
  batch 342 loss: 0.47813598820340564
  batch 343 loss: 0.4779521971506558
  batch 344 loss: 0.47774203424883444
  batch 345 loss: 0.477529439891594
  batch 346 loss: 0.47721155564909035
  batch 347 loss: 0.47688678113115623
  batch 348 loss: 0.476614047655429
  batch 349 loss: 0.4764008367471504
  batch 350 loss: 0.4761581471988133
  batch 351 loss: 0.4758647953682815
  batch 352 loss: 0.4755244025621902
  batch 353 loss: 0.47526625271559436
  batch 354 loss: 0.4749907297266405
  batch 355 loss: 0.4747971265248849
  batch 356 loss: 0.47450967894846136
  batch 357 loss: 0.4741170405506754
  batch 358 loss: 0.47377372154310426
  batch 359 loss: 0.4735633647043393
  batch 360 loss: 0.4731982068055206
  batch 361 loss: 0.4728784852576058
  batch 362 loss: 0.47255225537231615
  batch 363 loss: 0.47239493658719967
  batch 364 loss: 0.4720940812603458
  batch 365 loss: 0.4718992079773994
  batch 366 loss: 0.47163473411661677
  batch 367 loss: 0.47139323837750613
  batch 368 loss: 0.47110790532568225
  batch 369 loss: 0.4709227498630844
  batch 370 loss: 0.4707278846888929
  batch 371 loss: 0.4704161067053957
  batch 372 loss: 0.47022175836947655
  batch 373 loss: 0.46994794693453384
  batch 374 loss: 0.46962218480632906
  batch 375 loss: 0.46937980206807456
  batch 376 loss: 0.4691702455599257
  batch 377 loss: 0.4689261132273181
  batch 378 loss: 0.46859527808963936
  batch 379 loss: 0.4683038065490119
  batch 380 loss: 0.46805905684044485
  batch 381 loss: 0.4677954623549003
  batch 382 loss: 0.4675779352793519
  batch 383 loss: 0.4671539269757333
  batch 384 loss: 0.46693441139844555
  batch 385 loss: 0.46665527712215077
  batch 386 loss: 0.4664130691110779
  batch 387 loss: 0.4661834165261389
  batch 388 loss: 0.4659706347996427
  batch 389 loss: 0.4658329869144067
  batch 390 loss: 0.4655923122778917
  batch 391 loss: 0.46519955779280503
  batch 392 loss: 0.4650695533016507
  batch 393 loss: 0.4649276474808312
  batch 394 loss: 0.46474228269860224
  batch 395 loss: 0.46448564921753316
  batch 396 loss: 0.4643142589113929
  batch 397 loss: 0.46410729332594786
  batch 398 loss: 0.4638173690243582
  batch 399 loss: 0.463559608635747
  batch 400 loss: 0.4634719241410494
  batch 401 loss: 0.46317169426979865
  batch 402 loss: 0.46290621228182494
  batch 403 loss: 0.4625728662789016
  batch 404 loss: 0.46230224549475285
  batch 405 loss: 0.4622006719495043
  batch 406 loss: 0.4619481603206672
  batch 407 loss: 0.46181246960485306
  batch 408 loss: 0.46148251727515577
  batch 409 loss: 0.46124062364725144
  batch 410 loss: 0.4609170015265302
  batch 411 loss: 0.4606764120777158
  batch 412 loss: 0.4606386092536658
  batch 413 loss: 0.4603545735736736
  batch 414 loss: 0.4601202652506206
  batch 415 loss: 0.4598586628235966
  batch 416 loss: 0.459625655785203
  batch 417 loss: 0.45935580098657586
  batch 418 loss: 0.45912316077919096
  batch 419 loss: 0.45887695185324456
  batch 420 loss: 0.4586712725815319
  batch 421 loss: 0.45853660367163795
  batch 422 loss: 0.4582715650305364
  batch 423 loss: 0.4580847024213056
  batch 424 loss: 0.45793757404921187
  batch 425 loss: 0.4576243908966289
  batch 426 loss: 0.4572862534315933
  batch 427 loss: 0.4570893440090242
  batch 428 loss: 0.4568307412979759
  batch 429 loss: 0.4565915601514714
  batch 430 loss: 0.456420654682226
  batch 431 loss: 0.4562253597302669
  batch 432 loss: 0.4559648590921252
  batch 433 loss: 0.4556447173926902
  batch 434 loss: 0.4553600629353853
  batch 435 loss: 0.4552123395190842
  batch 436 loss: 0.4550344299422492
  batch 437 loss: 0.4547888114877915
  batch 438 loss: 0.4545613830764544
  batch 439 loss: 0.45427965126711034
  batch 440 loss: 0.45425354438749227
  batch 441 loss: 0.4541107446563487
  batch 442 loss: 0.4538700660159685
  batch 443 loss: 0.453633325161837
  batch 444 loss: 0.45348390547541884
  batch 445 loss: 0.4532885339822662
  batch 446 loss: 0.4531821788292829
  batch 447 loss: 0.4530061940085435
  batch 448 loss: 0.4528930693465684
  batch 449 loss: 0.45267329861167277
  batch 450 loss: 0.45248300181494816
  batch 451 loss: 0.45229886634122507
  batch 452 loss: 0.45208471255228583
  batch 453 loss: 0.4518741113986927
  batch 454 loss: 0.45168312202466215
  batch 455 loss: 0.4514836669623197
  batch 456 loss: 0.45128479593417103
  batch 457 loss: 0.45108675441543694
  batch 458 loss: 0.45083106416541935
  batch 459 loss: 0.45059160307082213
  batch 460 loss: 0.45041744650706
  batch 461 loss: 0.4502705148140417
  batch 462 loss: 0.4501221501207971
  batch 463 loss: 0.44998604149334376
  batch 464 loss: 0.44971506978417264
  batch 465 loss: 0.449531158708757
  batch 466 loss: 0.4493388410162005
  batch 467 loss: 0.4491939656994838
  batch 468 loss: 0.4489799880573892
  batch 469 loss: 0.4487033410748439
  batch 470 loss: 0.4486335400571214
  batch 471 loss: 0.44844602696961405
  batch 472 loss: 0.44819524757942913
LOSS train 0.44819524757942913 valid 0.24478119611740112
LOSS train 0.44819524757942913 valid 0.23882025480270386
LOSS train 0.44819524757942913 valid 0.2463156282901764
LOSS train 0.44819524757942913 valid 0.23626264557242393
LOSS train 0.44819524757942913 valid 0.2376464545726776
LOSS train 0.44819524757942913 valid 0.24542410671710968
LOSS train 0.44819524757942913 valid 0.2389706415789468
LOSS train 0.44819524757942913 valid 0.23773616179823875
LOSS train 0.44819524757942913 valid 0.2374308705329895
LOSS train 0.44819524757942913 valid 0.23514177650213242
LOSS train 0.44819524757942913 valid 0.23179781301455063
LOSS train 0.44819524757942913 valid 0.23327302684386572
LOSS train 0.44819524757942913 valid 0.23204596226031965
LOSS train 0.44819524757942913 valid 0.2301172156419073
LOSS train 0.44819524757942913 valid 0.22948998709519705
LOSS train 0.44819524757942913 valid 0.23144798446446657
LOSS train 0.44819524757942913 valid 0.2315247917876524
LOSS train 0.44819524757942913 valid 0.23174381918377346
LOSS train 0.44819524757942913 valid 0.23310283767549614
LOSS train 0.44819524757942913 valid 0.23293940350413322
LOSS train 0.44819524757942913 valid 0.23313048340025402
LOSS train 0.44819524757942913 valid 0.23165924169800498
LOSS train 0.44819524757942913 valid 0.23121270666951718
LOSS train 0.44819524757942913 valid 0.23164009178678194
LOSS train 0.44819524757942913 valid 0.23086942315101625
LOSS train 0.44819524757942913 valid 0.2300227295893889
LOSS train 0.44819524757942913 valid 0.23042292948122378
LOSS train 0.44819524757942913 valid 0.23009672973837172
LOSS train 0.44819524757942913 valid 0.22966251188311085
LOSS train 0.44819524757942913 valid 0.2290743832786878
LOSS train 0.44819524757942913 valid 0.22899984303982027
LOSS train 0.44819524757942913 valid 0.22964986274018884
LOSS train 0.44819524757942913 valid 0.22884944487701764
LOSS train 0.44819524757942913 valid 0.2280353281427832
LOSS train 0.44819524757942913 valid 0.2282747051545552
LOSS train 0.44819524757942913 valid 0.22878803726699617
LOSS train 0.44819524757942913 valid 0.22865214984159213
LOSS train 0.44819524757942913 valid 0.22821931305684542
LOSS train 0.44819524757942913 valid 0.22798574620332474
LOSS train 0.44819524757942913 valid 0.2283520121127367
LOSS train 0.44819524757942913 valid 0.22809303206641499
LOSS train 0.44819524757942913 valid 0.22930841325294404
LOSS train 0.44819524757942913 valid 0.22933649496976719
LOSS train 0.44819524757942913 valid 0.22890714894641528
LOSS train 0.44819524757942913 valid 0.22839446928766038
LOSS train 0.44819524757942913 valid 0.2281163227946862
LOSS train 0.44819524757942913 valid 0.22779417989101816
LOSS train 0.44819524757942913 valid 0.22877005177239576
LOSS train 0.44819524757942913 valid 0.22825055797489321
LOSS train 0.44819524757942913 valid 0.22873124212026597
LOSS train 0.44819524757942913 valid 0.22828385555276684
LOSS train 0.44819524757942913 valid 0.22807133427033058
LOSS train 0.44819524757942913 valid 0.2285836580789314
LOSS train 0.44819524757942913 valid 0.2284959880290208
LOSS train 0.44819524757942913 valid 0.2285300076007843
LOSS train 0.44819524757942913 valid 0.22863092379910605
LOSS train 0.44819524757942913 valid 0.22815099175561937
LOSS train 0.44819524757942913 valid 0.22877065863074927
LOSS train 0.44819524757942913 valid 0.22896747796212213
LOSS train 0.44819524757942913 valid 0.22858705073595048
LOSS train 0.44819524757942913 valid 0.2284459882095212
LOSS train 0.44819524757942913 valid 0.2280667594844295
LOSS train 0.44819524757942913 valid 0.2280815447133685
LOSS train 0.44819524757942913 valid 0.22782089235261083
LOSS train 0.44819524757942913 valid 0.2269416384972059
LOSS train 0.44819524757942913 valid 0.22664880301013138
LOSS train 0.44819524757942913 valid 0.2269691673677359
LOSS train 0.44819524757942913 valid 0.22636512272498188
LOSS train 0.44819524757942913 valid 0.22683781990106555
LOSS train 0.44819524757942913 valid 0.2272981515952519
LOSS train 0.44819524757942913 valid 0.2275097330271358
LOSS train 0.44819524757942913 valid 0.2278746347874403
LOSS train 0.44819524757942913 valid 0.22839050974747907
LOSS train 0.44819524757942913 valid 0.2284477202070726
LOSS train 0.44819524757942913 valid 0.22805404365062715
LOSS train 0.44819524757942913 valid 0.22819583686558823
LOSS train 0.44819524757942913 valid 0.22823062635861435
LOSS train 0.44819524757942913 valid 0.2280949694223893
LOSS train 0.44819524757942913 valid 0.22839138062694406
LOSS train 0.44819524757942913 valid 0.22812456600368022
LOSS train 0.44819524757942913 valid 0.2282980551690231
LOSS train 0.44819524757942913 valid 0.22839583601893448
LOSS train 0.44819524757942913 valid 0.228504051465586
LOSS train 0.44819524757942913 valid 0.22844305208751134
LOSS train 0.44819524757942913 valid 0.2285853014272802
LOSS train 0.44819524757942913 valid 0.22848377359467884
LOSS train 0.44819524757942913 valid 0.2282240656258046
LOSS train 0.44819524757942913 valid 0.22808490710502322
LOSS train 0.44819524757942913 valid 0.2285104589850715
LOSS train 0.44819524757942913 valid 0.22871445351176792
LOSS train 0.44819524757942913 valid 0.22870711867625898
LOSS train 0.44819524757942913 valid 0.22856427324206932
LOSS train 0.44819524757942913 valid 0.22838472518869626
LOSS train 0.44819524757942913 valid 0.22858383078524408
LOSS train 0.44819524757942913 valid 0.22865088550668014
LOSS train 0.44819524757942913 valid 0.22881022142246366
LOSS train 0.44819524757942913 valid 0.22891806250380486
LOSS train 0.44819524757942913 valid 0.22929185309580394
LOSS train 0.44819524757942913 valid 0.22940807721831583
LOSS train 0.44819524757942913 valid 0.22951098293066025
LOSS train 0.44819524757942913 valid 0.2294612709838565
LOSS train 0.44819524757942913 valid 0.22982397178808847
LOSS train 0.44819524757942913 valid 0.22964914114151186
LOSS train 0.44819524757942913 valid 0.2294124265989432
LOSS train 0.44819524757942913 valid 0.2295289314928509
LOSS train 0.44819524757942913 valid 0.22951839217599831
LOSS train 0.44819524757942913 valid 0.2294292475018546
LOSS train 0.44819524757942913 valid 0.22947014895854173
LOSS train 0.44819524757942913 valid 0.22914548286604225
LOSS train 0.44819524757942913 valid 0.22904555797576903
LOSS train 0.44819524757942913 valid 0.22921660569337038
LOSS train 0.44819524757942913 valid 0.22934512235224247
LOSS train 0.44819524757942913 valid 0.2292879624704344
LOSS train 0.44819524757942913 valid 0.2291627226952921
LOSS train 0.44819524757942913 valid 0.22963405705016593
LOSS train 0.44819524757942913 valid 0.22949891678732018
LOSS train 0.44819524757942913 valid 0.22992209325998259
LOSS train 0.44819524757942913 valid 0.23005777041790848
LOSS train 0.44819524757942913 valid 0.22981598297087083
LOSS train 0.44819524757942913 valid 0.22964463010430336
LOSS train 0.44819524757942913 valid 0.22977970232648298
LOSS train 0.44819524757942913 valid 0.22983037448320232
LOSS train 0.44819524757942913 valid 0.2298123720215588
LOSS train 0.44819524757942913 valid 0.22984124227396904
LOSS train 0.44819524757942913 valid 0.2298760117292404
LOSS train 0.44819524757942913 valid 0.22995073513852227
LOSS train 0.44819524757942913 valid 0.2299528340185721
LOSS train 0.44819524757942913 valid 0.22984139621257782
LOSS train 0.44819524757942913 valid 0.22971264498178348
LOSS train 0.44819524757942913 valid 0.2296264349268033
LOSS train 0.44819524757942913 valid 0.22945708889542646
LOSS train 0.44819524757942913 valid 0.22942663514704414
LOSS train 0.44819524757942913 valid 0.22936680106292093
LOSS train 0.44819524757942913 valid 0.2295636708167062
LOSS train 0.44819524757942913 valid 0.2295874419035735
LOSS train 0.44819524757942913 valid 0.22966569092343836
LOSS train 0.44819524757942913 valid 0.22957813794160412
LOSS train 0.44819524757942913 valid 0.2295038872870846
LOSS train 0.44819524757942913 valid 0.22945918796731413
LOSS train 0.44819524757942913 valid 0.22951695493289404
LOSS train 0.44819524757942913 valid 0.22959346128693708
LOSS train 0.44819524757942913 valid 0.22974737828046504
LOSS train 0.44819524757942913 valid 0.22976737130771985
LOSS train 0.44819524757942913 valid 0.22987808990809652
LOSS train 0.44819524757942913 valid 0.22971683730339182
LOSS train 0.44819524757942913 valid 0.22965824634653248
LOSS train 0.44819524757942913 valid 0.22968147816706677
LOSS train 0.44819524757942913 valid 0.22950032172170845
LOSS train 0.44819524757942913 valid 0.22949625481695138
LOSS train 0.44819524757942913 valid 0.22952001174290976
LOSS train 0.44819524757942913 valid 0.22939114548907374
LOSS train 0.44819524757942913 valid 0.2293824825043741
LOSS train 0.44819524757942913 valid 0.2291131596160091
LOSS train 0.44819524757942913 valid 0.22921916890841026
LOSS train 0.44819524757942913 valid 0.22919216973166312
LOSS train 0.44819524757942913 valid 0.22936866441980386
LOSS train 0.44819524757942913 valid 0.22922896038574778
LOSS train 0.44819524757942913 valid 0.22919798604672467
LOSS train 0.44819524757942913 valid 0.2294015478600496
LOSS train 0.44819524757942913 valid 0.22942683966830374
LOSS train 0.44819524757942913 valid 0.22937822739900268
LOSS train 0.44819524757942913 valid 0.2293994090071431
LOSS train 0.44819524757942913 valid 0.22912175167191978
LOSS train 0.44819524757942913 valid 0.2291971083457877
LOSS train 0.44819524757942913 valid 0.22923604385419324
LOSS train 0.44819524757942913 valid 0.22908596218709487
LOSS train 0.44819524757942913 valid 0.22916900192549128
LOSS train 0.44819524757942913 valid 0.22897804945352532
LOSS train 0.44819524757942913 valid 0.2287817958896682
LOSS train 0.44819524757942913 valid 0.22876717632307725
LOSS train 0.44819524757942913 valid 0.22865629344307192
LOSS train 0.44819524757942913 valid 0.2286587546384612
LOSS train 0.44819524757942913 valid 0.2286535053583928
LOSS train 0.44819524757942913 valid 0.22880192871751456
LOSS train 0.44819524757942913 valid 0.22869803292410715
LOSS train 0.44819524757942913 valid 0.22857196976176716
LOSS train 0.44819524757942913 valid 0.22854409084818458
LOSS train 0.44819524757942913 valid 0.2286172661553608
LOSS train 0.44819524757942913 valid 0.22865385541369795
LOSS train 0.44819524757942913 valid 0.22859854226311047
LOSS train 0.44819524757942913 valid 0.22857517860212379
LOSS train 0.44819524757942913 valid 0.22845542160691795
LOSS train 0.44819524757942913 valid 0.22848799857285504
LOSS train 0.44819524757942913 valid 0.22853468617667322
LOSS train 0.44819524757942913 valid 0.22858614406070193
LOSS train 0.44819524757942913 valid 0.22866937165619225
LOSS train 0.44819524757942913 valid 0.22854493526532688
LOSS train 0.44819524757942913 valid 0.2285989574453932
LOSS train 0.44819524757942913 valid 0.22842803573797618
LOSS train 0.44819524757942913 valid 0.22849925905466079
LOSS train 0.44819524757942913 valid 0.228688956321222
LOSS train 0.44819524757942913 valid 0.22869361331686378
LOSS train 0.44819524757942913 valid 0.22859146604266192
LOSS train 0.44819524757942913 valid 0.2285640660634975
LOSS train 0.44819524757942913 valid 0.22852999422794734
LOSS train 0.44819524757942913 valid 0.22873899211384813
LOSS train 0.44819524757942913 valid 0.2287827749088936
LOSS train 0.44819524757942913 valid 0.22901997278736094
LOSS train 0.44819524757942913 valid 0.22898685205821415
LOSS train 0.44819524757942913 valid 0.22907668933272363
LOSS train 0.44819524757942913 valid 0.2289677971038059
LOSS train 0.44819524757942913 valid 0.22900422146119695
LOSS train 0.44819524757942913 valid 0.22921742043765309
LOSS train 0.44819524757942913 valid 0.22919239520150073
LOSS train 0.44819524757942913 valid 0.22912277935481654
LOSS train 0.44819524757942913 valid 0.22909651674981257
LOSS train 0.44819524757942913 valid 0.22911472712162037
LOSS train 0.44819524757942913 valid 0.22892389131280091
LOSS train 0.44819524757942913 valid 0.22895314861712843
LOSS train 0.44819524757942913 valid 0.22901193229925065
LOSS train 0.44819524757942913 valid 0.22920373633009564
LOSS train 0.44819524757942913 valid 0.22917351400795974
LOSS train 0.44819524757942913 valid 0.22913225316945376
LOSS train 0.44819524757942913 valid 0.22911041849684494
LOSS train 0.44819524757942913 valid 0.228901925059252
LOSS train 0.44819524757942913 valid 0.22874885804399295
LOSS train 0.44819524757942913 valid 0.22854400642456546
LOSS train 0.44819524757942913 valid 0.22849187459967552
LOSS train 0.44819524757942913 valid 0.22854356025451938
LOSS train 0.44819524757942913 valid 0.22856276035308837
LOSS train 0.44819524757942913 valid 0.2285167301132668
LOSS train 0.44819524757942913 valid 0.22858351630133553
LOSS train 0.44819524757942913 valid 0.22854431568239836
LOSS train 0.44819524757942913 valid 0.2284174912742206
LOSS train 0.44819524757942913 valid 0.22834290663401285
LOSS train 0.44819524757942913 valid 0.2283139791900078
LOSS train 0.44819524757942913 valid 0.2283491475860453
LOSS train 0.44819524757942913 valid 0.2282663802697993
LOSS train 0.44819524757942913 valid 0.22812546965336694
LOSS train 0.44819524757942913 valid 0.22820491803729015
LOSS train 0.44819524757942913 valid 0.22818149271465482
LOSS train 0.44819524757942913 valid 0.2282523412642808
LOSS train 0.44819524757942913 valid 0.2284179649639539
LOSS train 0.44819524757942913 valid 0.22835968523962885
LOSS train 0.44819524757942913 valid 0.22838092833123308
LOSS train 0.44819524757942913 valid 0.2283350915600688
LOSS train 0.44819524757942913 valid 0.22843614616726018
LOSS train 0.44819524757942913 valid 0.22850345626097768
LOSS train 0.44819524757942913 valid 0.22844992821186655
LOSS train 0.44819524757942913 valid 0.22815331127494573
LOSS train 0.44819524757942913 valid 0.22798213919168686
LOSS train 0.44819524757942913 valid 0.22800610503874535
LOSS train 0.44819524757942913 valid 0.22795468617859202
LOSS train 0.44819524757942913 valid 0.22797154964970762
LOSS train 0.44819524757942913 valid 0.22810244840018604
LOSS train 0.44819524757942913 valid 0.22815051032760278
LOSS train 0.44819524757942913 valid 0.22814303316809387
LOSS train 0.44819524757942913 valid 0.22817648640803753
LOSS train 0.44819524757942913 valid 0.2280398560575692
LOSS train 0.44819524757942913 valid 0.22822898614406587
LOSS train 0.44819524757942913 valid 0.22831770694588285
LOSS train 0.44819524757942913 valid 0.22835375350855647
LOSS train 0.44819524757942913 valid 0.22823360297284107
LOSS train 0.44819524757942913 valid 0.22833998730098168
LOSS train 0.44819524757942913 valid 0.2282986133706336
LOSS train 0.44819524757942913 valid 0.22822501650080085
LOSS train 0.44819524757942913 valid 0.22826827523309434
LOSS train 0.44819524757942913 valid 0.22836357874925747
LOSS train 0.44819524757942913 valid 0.228292004194499
LOSS train 0.44819524757942913 valid 0.2281697138570822
LOSS train 0.44819524757942913 valid 0.2281740638373912
LOSS train 0.44819524757942913 valid 0.22832910597096873
LOSS train 0.44819524757942913 valid 0.22826556124841305
LOSS train 0.44819524757942913 valid 0.22831632834718082
LOSS train 0.44819524757942913 valid 0.2282802436149345
LOSS train 0.44819524757942913 valid 0.22827808306853575
LOSS train 0.44819524757942913 valid 0.22825669522365827
LOSS train 0.44819524757942913 valid 0.22824271117795758
LOSS train 0.44819524757942913 valid 0.22828420287614418
LOSS train 0.44819524757942913 valid 0.22822278572453392
LOSS train 0.44819524757942913 valid 0.22822828706340156
LOSS train 0.44819524757942913 valid 0.22807767899597392
LOSS train 0.44819524757942913 valid 0.2279658609039181
LOSS train 0.44819524757942913 valid 0.22791558678132773
LOSS train 0.44819524757942913 valid 0.22801359935240312
LOSS train 0.44819524757942913 valid 0.22799816344311272
LOSS train 0.44819524757942913 valid 0.22806741308003986
LOSS train 0.44819524757942913 valid 0.22803987503694972
LOSS train 0.44819524757942913 valid 0.22803235972654007
LOSS train 0.44819524757942913 valid 0.22795453843261515
LOSS train 0.44819524757942913 valid 0.22788524150424155
LOSS train 0.44819524757942913 valid 0.2279093859994665
LOSS train 0.44819524757942913 valid 0.2279456206745478
LOSS train 0.44819524757942913 valid 0.22786829269058267
LOSS train 0.44819524757942913 valid 0.22789833493400038
LOSS train 0.44819524757942913 valid 0.22785291951019448
LOSS train 0.44819524757942913 valid 0.22788765746126607
LOSS train 0.44819524757942913 valid 0.22787402803078294
LOSS train 0.44819524757942913 valid 0.2278133890104954
LOSS train 0.44819524757942913 valid 0.22778629460211458
LOSS train 0.44819524757942913 valid 0.22773752538199277
LOSS train 0.44819524757942913 valid 0.22771784627478417
LOSS train 0.44819524757942913 valid 0.2277646700981941
LOSS train 0.44819524757942913 valid 0.2277582301473131
LOSS train 0.44819524757942913 valid 0.22778189045897984
LOSS train 0.44819524757942913 valid 0.22776796024393392
LOSS train 0.44819524757942913 valid 0.22777934713596446
LOSS train 0.44819524757942913 valid 0.227807120038759
LOSS train 0.44819524757942913 valid 0.2278000195588555
LOSS train 0.44819524757942913 valid 0.22778644233942033
LOSS train 0.44819524757942913 valid 0.22775424180236767
LOSS train 0.44819524757942913 valid 0.22774878469918738
LOSS train 0.44819524757942913 valid 0.22768174721659606
LOSS train 0.44819524757942913 valid 0.22765432920699058
LOSS train 0.44819524757942913 valid 0.2276673259793735
LOSS train 0.44819524757942913 valid 0.22776836249368643
LOSS train 0.44819524757942913 valid 0.22773316089998238
LOSS train 0.44819524757942913 valid 0.22778980356532258
LOSS train 0.44819524757942913 valid 0.22782139499403511
LOSS train 0.44819524757942913 valid 0.22785969789951077
LOSS train 0.44819524757942913 valid 0.2279251218800376
LOSS train 0.44819524757942913 valid 0.22788634409125036
LOSS train 0.44819524757942913 valid 0.2279775963423732
LOSS train 0.44819524757942913 valid 0.22797968409433486
LOSS train 0.44819524757942913 valid 0.2279395759578735
LOSS train 0.44819524757942913 valid 0.22799982908594457
LOSS train 0.44819524757942913 valid 0.2279625261901681
LOSS train 0.44819524757942913 valid 0.22801029658729924
LOSS train 0.44819524757942913 valid 0.228030680068608
LOSS train 0.44819524757942913 valid 0.22799216248095036
LOSS train 0.44819524757942913 valid 0.22803829735684616
LOSS train 0.44819524757942913 valid 0.22805212521404952
LOSS train 0.44819524757942913 valid 0.22797108088669024
LOSS train 0.44819524757942913 valid 0.22794068733483186
LOSS train 0.44819524757942913 valid 0.2277969724398393
LOSS train 0.44819524757942913 valid 0.2278396473066207
LOSS train 0.44819524757942913 valid 0.22774729570117566
LOSS train 0.44819524757942913 valid 0.22779459769769414
LOSS train 0.44819524757942913 valid 0.2277065500571735
LOSS train 0.44819524757942913 valid 0.22770542561104803
LOSS train 0.44819524757942913 valid 0.22758940253913224
LOSS train 0.44819524757942913 valid 0.2275419607309692
LOSS train 0.44819524757942913 valid 0.22759389640153707
LOSS train 0.44819524757942913 valid 0.22754316335309765
LOSS train 0.44819524757942913 valid 0.22751192819716326
LOSS train 0.44819524757942913 valid 0.22741971559645163
LOSS train 0.44819524757942913 valid 0.22753134091282456
LOSS train 0.44819524757942913 valid 0.22746158144354114
LOSS train 0.44819524757942913 valid 0.22742737253858628
LOSS train 0.44819524757942913 valid 0.2274605117738247
LOSS train 0.44819524757942913 valid 0.2274044075232447
LOSS train 0.44819524757942913 valid 0.2273829982381815
LOSS train 0.44819524757942913 valid 0.22728735999185212
LOSS train 0.44819524757942913 valid 0.22734276213964752
LOSS train 0.44819524757942913 valid 0.22744610594666523
LOSS train 0.44819524757942913 valid 0.22738697280773537
LOSS train 0.44819524757942913 valid 0.22736234864171712
LOSS train 0.44819524757942913 valid 0.22739845034719883
LOSS train 0.44819524757942913 valid 0.2273109258502124
LOSS train 0.44819524757942913 valid 0.22734114455325263
LOSS train 0.44819524757942913 valid 0.22737136699705043
LOSS train 0.44819524757942913 valid 0.22737824912606316
LOSS train 0.44819524757942913 valid 0.2274505815576899
LOSS train 0.44819524757942913 valid 0.2273993091815609
LOSS train 0.44819524757942913 valid 0.22734670038794128
LOSS train 0.44819524757942913 valid 0.22738197274254948
LOSS train 0.44819524757942913 valid 0.22743130368845804
LOSS train 0.44819524757942913 valid 0.22750697676196444
LOSS train 0.44819524757942913 valid 0.2274749198082762
LOSS train 0.44819524757942913 valid 0.22754662049313387
LOSS train 0.44819524757942913 valid 0.22750012386703755
LOSS train 0.44819524757942913 valid 0.2275625100534265
LOSS train 0.44819524757942913 valid 0.22761489886061875
LOSS train 0.44819524757942913 valid 0.22754091208616456
LOSS train 0.44819524757942913 valid 0.22759667467581082
LOSS train 0.44819524757942913 valid 0.22754298032470088
LOSS train 0.44819524757942913 valid 0.22755614024579363
LOSS train 0.44819524757942913 valid 0.22749699595505776
LOSS train 0.44819524757942913 valid 0.2275470071207217
EPOCH 2:
  batch 1 loss: 0.41023141145706177
  batch 2 loss: 0.4157787561416626
  batch 3 loss: 0.3964237968126933
  batch 4 loss: 0.3938264846801758
  batch 5 loss: 0.39308249950408936
  batch 6 loss: 0.3881918539603551
  batch 7 loss: 0.38342113154275076
  batch 8 loss: 0.38242345303297043
  batch 9 loss: 0.3795761565367381
  batch 10 loss: 0.3754694491624832
  batch 11 loss: 0.37445892799984326
  batch 12 loss: 0.37164897471666336
  batch 13 loss: 0.37134907566584074
  batch 14 loss: 0.37046743077891214
  batch 15 loss: 0.37233123977979027
  batch 16 loss: 0.3713815715163946
  batch 17 loss: 0.3701657144462361
  batch 18 loss: 0.3690199338727527
  batch 19 loss: 0.36775745373023183
  batch 20 loss: 0.36509205102920533
  batch 21 loss: 0.36839094048454646
  batch 22 loss: 0.3696927536617626
  batch 23 loss: 0.3673327059849449
  batch 24 loss: 0.3678103672961394
  batch 25 loss: 0.368716869354248
  batch 26 loss: 0.3676772186389336
  batch 27 loss: 0.3676971075711427
  batch 28 loss: 0.36816584531749996
  batch 29 loss: 0.3679165141335849
  batch 30 loss: 0.36819413006305696
  batch 31 loss: 0.3676310085481213
  batch 32 loss: 0.3688796740025282
  batch 33 loss: 0.36998687368450744
  batch 34 loss: 0.3702365983934963
  batch 35 loss: 0.37159993818828035
  batch 36 loss: 0.37141914831267464
  batch 37 loss: 0.37095671569978866
  batch 38 loss: 0.3708851753096831
  batch 39 loss: 0.3714548349380493
  batch 40 loss: 0.3710772730410099
  batch 41 loss: 0.3712701579419578
  batch 42 loss: 0.370069030494917
  batch 43 loss: 0.3694099610628084
  batch 44 loss: 0.3689303215254437
  batch 45 loss: 0.36801740858289933
  batch 46 loss: 0.3668471315632696
  batch 47 loss: 0.36652904939144215
  batch 48 loss: 0.36635798774659634
  batch 49 loss: 0.3667937401606112
  batch 50 loss: 0.3667018085718155
  batch 51 loss: 0.36629581977339354
  batch 52 loss: 0.36607907024713665
  batch 53 loss: 0.36549109895274323
  batch 54 loss: 0.36516586001272555
  batch 55 loss: 0.36566985628821636
  batch 56 loss: 0.36606220954230856
  batch 57 loss: 0.36543498959457665
  batch 58 loss: 0.3648704480508278
  batch 59 loss: 0.3652133401167595
  batch 60 loss: 0.3650003289182981
  batch 61 loss: 0.3655097264735425
  batch 62 loss: 0.36609201517797285
  batch 63 loss: 0.3655890647381071
  batch 64 loss: 0.36504590744152665
  batch 65 loss: 0.36432998730586125
  batch 66 loss: 0.36384707102269837
  batch 67 loss: 0.3637017011642456
  batch 68 loss: 0.3639121222145417
  batch 69 loss: 0.36402083177497424
  batch 70 loss: 0.3634456941059658
  batch 71 loss: 0.3633941034196128
  batch 72 loss: 0.36332108535700375
  batch 73 loss: 0.3630765680580923
  batch 74 loss: 0.36253843718283885
  batch 75 loss: 0.36240147749582924
  batch 76 loss: 0.3627520858457214
  batch 77 loss: 0.36252311646164237
  batch 78 loss: 0.36258288224538165
  batch 79 loss: 0.36246975615054744
  batch 80 loss: 0.3623412325978279
  batch 81 loss: 0.36203797658284503
  batch 82 loss: 0.36195545443674415
  batch 83 loss: 0.3616224311920534
  batch 84 loss: 0.3613794896574247
  batch 85 loss: 0.36210293173789976
  batch 86 loss: 0.36202324303083644
  batch 87 loss: 0.36203692973345175
  batch 88 loss: 0.36195905845273624
  batch 89 loss: 0.3614200119222148
  batch 90 loss: 0.36180822915501065
  batch 91 loss: 0.36126627044363335
  batch 92 loss: 0.36123168079749396
  batch 93 loss: 0.3607973940269921
  batch 94 loss: 0.36079399826678826
  batch 95 loss: 0.36068261799059415
  batch 96 loss: 0.36036223483582336
  batch 97 loss: 0.36085005980177026
  batch 98 loss: 0.3606066925793278
  batch 99 loss: 0.3601823855530132
  batch 100 loss: 0.3602167630195618
  batch 101 loss: 0.3599435008398377
  batch 102 loss: 0.3598939110251034
  batch 103 loss: 0.35956834936604914
  batch 104 loss: 0.3600395078269335
  batch 105 loss: 0.36004566976002284
  batch 106 loss: 0.3602622769913583
  batch 107 loss: 0.3601778191383754
  batch 108 loss: 0.35990435078188227
  batch 109 loss: 0.3601666639704223
  batch 110 loss: 0.36038398471745575
  batch 111 loss: 0.36036009267643765
  batch 112 loss: 0.3601701421929257
  batch 113 loss: 0.35977445476878006
  batch 114 loss: 0.3598357993259765
  batch 115 loss: 0.35954865683679993
  batch 116 loss: 0.35952727907690507
  batch 117 loss: 0.35950190440202373
  batch 118 loss: 0.3595546226380235
  batch 119 loss: 0.35983561167196065
  batch 120 loss: 0.35972689390182494
  batch 121 loss: 0.3598291851764868
  batch 122 loss: 0.3597561294426683
  batch 123 loss: 0.3595215326886836
  batch 124 loss: 0.359436021937478
  batch 125 loss: 0.3590262143611908
  batch 126 loss: 0.3590522415581204
  batch 127 loss: 0.35910574681176916
  batch 128 loss: 0.3588643616531044
  batch 129 loss: 0.35867676115775293
  batch 130 loss: 0.35866000652313235
  batch 131 loss: 0.3584015865362328
  batch 132 loss: 0.35841681243795337
  batch 133 loss: 0.3580789870785591
  batch 134 loss: 0.35784259305071475
  batch 135 loss: 0.35780051085684034
  batch 136 loss: 0.3578231827739407
  batch 137 loss: 0.35796598274342334
  batch 138 loss: 0.3579521442669025
  batch 139 loss: 0.3579025223529596
  batch 140 loss: 0.35811808492456165
  batch 141 loss: 0.35796017744016984
  batch 142 loss: 0.35788715998051873
  batch 143 loss: 0.35809539143855756
  batch 144 loss: 0.3577888252006637
  batch 145 loss: 0.3577753143063907
  batch 146 loss: 0.35774986629616723
  batch 147 loss: 0.3578476062437304
  batch 148 loss: 0.3578180026363682
  batch 149 loss: 0.35750592094939826
  batch 150 loss: 0.3574022861321767
  batch 151 loss: 0.3571047670399116
  batch 152 loss: 0.3569588670997243
  batch 153 loss: 0.35721934893551993
  batch 154 loss: 0.3572048121071481
  batch 155 loss: 0.35723452952600293
  batch 156 loss: 0.35702801342958057
  batch 157 loss: 0.35691020689952146
  batch 158 loss: 0.35705850414837464
  batch 159 loss: 0.3571883207597073
  batch 160 loss: 0.35693629626184703
  batch 161 loss: 0.3566818983288285
  batch 162 loss: 0.35663211106518167
  batch 163 loss: 0.35649853250000374
  batch 164 loss: 0.3565739785752645
  batch 165 loss: 0.35662605347055376
  batch 166 loss: 0.35672182993716506
  batch 167 loss: 0.3565167452760799
  batch 168 loss: 0.3563570502613272
  batch 169 loss: 0.3564244439968696
  batch 170 loss: 0.35645594474147346
  batch 171 loss: 0.356599112526018
  batch 172 loss: 0.35647517252106997
  batch 173 loss: 0.35640630332720763
  batch 174 loss: 0.35638614016017695
  batch 175 loss: 0.35638046622276304
  batch 176 loss: 0.35653913393616676
  batch 177 loss: 0.35666630059312293
  batch 178 loss: 0.3562613680456462
  batch 179 loss: 0.35629003956997196
  batch 180 loss: 0.35614035957389406
  batch 181 loss: 0.35601308612533694
  batch 182 loss: 0.3559291655873204
  batch 183 loss: 0.35580729330823724
  batch 184 loss: 0.35569712373873463
  batch 185 loss: 0.3555152160090369
  batch 186 loss: 0.3554968027978815
  batch 187 loss: 0.35568696945746314
  batch 188 loss: 0.35567348529683784
  batch 189 loss: 0.3557111638248282
  batch 190 loss: 0.3556243204756787
  batch 191 loss: 0.35575612412073226
  batch 192 loss: 0.35550401111443836
  batch 193 loss: 0.35549088864746486
  batch 194 loss: 0.3554143771990058
  batch 195 loss: 0.355582263530829
  batch 196 loss: 0.35559792907870547
  batch 197 loss: 0.3555710285448181
  batch 198 loss: 0.35556728430468626
  batch 199 loss: 0.35554174202770444
  batch 200 loss: 0.35545054271817206
  batch 201 loss: 0.3552425262050249
  batch 202 loss: 0.3550828144987031
  batch 203 loss: 0.3550605543434914
  batch 204 loss: 0.3550040013357705
  batch 205 loss: 0.3549226990560206
  batch 206 loss: 0.35482360263472623
  batch 207 loss: 0.35478513237934756
  batch 208 loss: 0.3545632196160463
  batch 209 loss: 0.35458553721459857
  batch 210 loss: 0.3545171171426773
  batch 211 loss: 0.3543984004389053
  batch 212 loss: 0.3544852822175566
  batch 213 loss: 0.35423711823745513
  batch 214 loss: 0.35414613825138486
  batch 215 loss: 0.35421285726303275
  batch 216 loss: 0.35436141063217763
  batch 217 loss: 0.3540599666158175
  batch 218 loss: 0.3538846588189449
  batch 219 loss: 0.3536049942719882
  batch 220 loss: 0.35366590063680303
  batch 221 loss: 0.3536664875923778
  batch 222 loss: 0.3536747632262943
  batch 223 loss: 0.35358532100514983
  batch 224 loss: 0.353567935526371
  batch 225 loss: 0.35367615633540683
  batch 226 loss: 0.35363707750771955
  batch 227 loss: 0.35350124649539394
  batch 228 loss: 0.35339732821050446
  batch 229 loss: 0.3534384367247336
  batch 230 loss: 0.3533638606900754
  batch 231 loss: 0.35338579550449983
  batch 232 loss: 0.35317585565920534
  batch 233 loss: 0.35303341255167525
  batch 234 loss: 0.35290740634131634
  batch 235 loss: 0.3527419750994824
  batch 236 loss: 0.35270214750099993
  batch 237 loss: 0.3527890838651214
  batch 238 loss: 0.352648405217323
  batch 239 loss: 0.3526057961346215
  batch 240 loss: 0.3527139409134785
  batch 241 loss: 0.3525947595780321
  batch 242 loss: 0.352622179453038
  batch 243 loss: 0.35270842308860745
  batch 244 loss: 0.35266655268239194
  batch 245 loss: 0.3527782948649659
  batch 246 loss: 0.35281978002408654
  batch 247 loss: 0.3527622181876951
  batch 248 loss: 0.3528066930030623
  batch 249 loss: 0.35269053094837083
  batch 250 loss: 0.3525653475522995
  batch 251 loss: 0.3523189559638263
  batch 252 loss: 0.3521913847043401
  batch 253 loss: 0.35210443909460376
  batch 254 loss: 0.3520626318971003
  batch 255 loss: 0.3518077358311298
  batch 256 loss: 0.3516410266747698
  batch 257 loss: 0.3516889158853761
  batch 258 loss: 0.3515047320099764
  batch 259 loss: 0.3513347332772141
  batch 260 loss: 0.3512165388235679
  batch 261 loss: 0.3513670600237061
  batch 262 loss: 0.3512334022813171
  batch 263 loss: 0.35113006399611557
  batch 264 loss: 0.35129261366797215
  batch 265 loss: 0.35127922903816655
  batch 266 loss: 0.3512398617384129
  batch 267 loss: 0.3510566502251429
  batch 268 loss: 0.3509444615511752
  batch 269 loss: 0.3508844783314985
  batch 270 loss: 0.35088336136606
  batch 271 loss: 0.3508706879131908
  batch 272 loss: 0.35075112092582617
  batch 273 loss: 0.3506731164979411
  batch 274 loss: 0.3504265903556434
  batch 275 loss: 0.35040603084997696
  batch 276 loss: 0.3503789609109146
  batch 277 loss: 0.35030793476621164
  batch 278 loss: 0.35017906397366694
  batch 279 loss: 0.35011727557814676
  batch 280 loss: 0.3499608640159879
  batch 281 loss: 0.3499243214885535
  batch 282 loss: 0.3498661990495438
  batch 283 loss: 0.34975559109091336
  batch 284 loss: 0.3497322752652034
  batch 285 loss: 0.3496030663189135
  batch 286 loss: 0.3495749350819554
  batch 287 loss: 0.34942561585313353
  batch 288 loss: 0.34940740642034346
  batch 289 loss: 0.3492322173291837
  batch 290 loss: 0.349128051359078
  batch 291 loss: 0.3490655144670165
  batch 292 loss: 0.34902967747351893
  batch 293 loss: 0.3490473498866827
  batch 294 loss: 0.3489973239955448
  batch 295 loss: 0.3489905446262683
  batch 296 loss: 0.349151476009472
  batch 297 loss: 0.34919942790008957
  batch 298 loss: 0.3491825740409377
  batch 299 loss: 0.3492917421470119
  batch 300 loss: 0.34922884305318197
  batch 301 loss: 0.34923473307460645
  batch 302 loss: 0.3493233900393871
  batch 303 loss: 0.3492995356962626
  batch 304 loss: 0.3492908233678655
  batch 305 loss: 0.34923000482262156
  batch 306 loss: 0.3493162870796677
  batch 307 loss: 0.34925524332236
  batch 308 loss: 0.349203066102096
  batch 309 loss: 0.3490705946309667
  batch 310 loss: 0.3490795867096993
  batch 311 loss: 0.34891888403432547
  batch 312 loss: 0.34882491750594896
  batch 313 loss: 0.3486996991946674
  batch 314 loss: 0.34868315317828186
  batch 315 loss: 0.34870027682137866
  batch 316 loss: 0.3486614950661418
  batch 317 loss: 0.34878543752601093
  batch 318 loss: 0.34862467805919406
  batch 319 loss: 0.3484212914043833
  batch 320 loss: 0.34852430000901224
  batch 321 loss: 0.3484413553249799
  batch 322 loss: 0.34832333435553203
  batch 323 loss: 0.34832374534739796
  batch 324 loss: 0.34827409703054546
  batch 325 loss: 0.34831091825778665
  batch 326 loss: 0.3484080696215659
  batch 327 loss: 0.3483552468843781
  batch 328 loss: 0.3483189321690943
  batch 329 loss: 0.34830449988052115
  batch 330 loss: 0.34832718661337186
  batch 331 loss: 0.3482359815220098
  batch 332 loss: 0.3483082529112517
  batch 333 loss: 0.3482026844411283
  batch 334 loss: 0.34836167480774266
  batch 335 loss: 0.3482865447428689
  batch 336 loss: 0.34826208784111906
  batch 337 loss: 0.3482686599567311
  batch 338 loss: 0.3481572161059408
  batch 339 loss: 0.34805239899671886
  batch 340 loss: 0.3479572285624111
  batch 341 loss: 0.34789036244940547
  batch 342 loss: 0.34788223482363406
  batch 343 loss: 0.3479287743394646
  batch 344 loss: 0.3479381021718646
  batch 345 loss: 0.3479616765526758
  batch 346 loss: 0.34784094231321633
  batch 347 loss: 0.3477194093661624
  batch 348 loss: 0.34767490548306496
  batch 349 loss: 0.3476818129633764
  batch 350 loss: 0.34766706390040264
  batch 351 loss: 0.3476275604674619
  batch 352 loss: 0.3475228707221421
  batch 353 loss: 0.34749836717381355
  batch 354 loss: 0.3474975319231971
  batch 355 loss: 0.34747928716767
  batch 356 loss: 0.3474788148416562
  batch 357 loss: 0.34725265225776436
  batch 358 loss: 0.34704806734729743
  batch 359 loss: 0.3470092642440105
  batch 360 loss: 0.34692377373576166
  batch 361 loss: 0.34682876730229384
  batch 362 loss: 0.34675412662121474
  batch 363 loss: 0.3467719508104088
  batch 364 loss: 0.34666887801754603
  batch 365 loss: 0.3467267636566946
  batch 366 loss: 0.34675930220572676
  batch 367 loss: 0.34679269790649414
  batch 368 loss: 0.3467097918138556
  batch 369 loss: 0.3467020954543013
  batch 370 loss: 0.34677746585897495
  batch 371 loss: 0.3467140678928869
  batch 372 loss: 0.34671383254951044
  batch 373 loss: 0.34676761178164955
  batch 374 loss: 0.34667114594722176
  batch 375 loss: 0.34667068990071614
  batch 376 loss: 0.34671108567334236
  batch 377 loss: 0.3466688044349458
  batch 378 loss: 0.34654902536717674
  batch 379 loss: 0.34653367860965173
  batch 380 loss: 0.34648129422413676
  batch 381 loss: 0.3464223432728625
  batch 382 loss: 0.34637678574517133
  batch 383 loss: 0.34626477707770725
  batch 384 loss: 0.346271281518663
  batch 385 loss: 0.3461934786338311
  batch 386 loss: 0.34615349962612507
  batch 387 loss: 0.3461051980654399
  batch 388 loss: 0.3461061008658606
  batch 389 loss: 0.34610439902099666
  batch 390 loss: 0.34610707393059365
  batch 391 loss: 0.34590088856189755
  batch 392 loss: 0.34595538721400865
  batch 393 loss: 0.3460484710510147
  batch 394 loss: 0.34610884103375644
  batch 395 loss: 0.3460830656033528
  batch 396 loss: 0.3460898986368468
  batch 397 loss: 0.3461893477127594
  batch 398 loss: 0.3461161729828197
  batch 399 loss: 0.346080051627673
  batch 400 loss: 0.3461959011107683
  batch 401 loss: 0.34608380232665903
  batch 402 loss: 0.34603034730870924
  batch 403 loss: 0.34590780431816065
  batch 404 loss: 0.34584333186987604
  batch 405 loss: 0.3458786251368346
  batch 406 loss: 0.34580032872449
  batch 407 loss: 0.3458965896681427
  batch 408 loss: 0.34576232409944724
  batch 409 loss: 0.3457169232566666
  batch 410 loss: 0.34561787320346365
  batch 411 loss: 0.3456014418689004
  batch 412 loss: 0.34568752051846496
  batch 413 loss: 0.3456176988437736
  batch 414 loss: 0.34562510165615357
  batch 415 loss: 0.3455851547689323
  batch 416 loss: 0.3455269719975499
  batch 417 loss: 0.34547997021274884
  batch 418 loss: 0.3454100535959718
  batch 419 loss: 0.34540019506201824
  batch 420 loss: 0.34536256045103075
  batch 421 loss: 0.34536032219397483
  batch 422 loss: 0.3453088188058392
  batch 423 loss: 0.3453316148150334
  batch 424 loss: 0.34537402378781784
  batch 425 loss: 0.3452674637121313
  batch 426 loss: 0.3451170673672582
  batch 427 loss: 0.34510040017983395
  batch 428 loss: 0.3450332393991613
  batch 429 loss: 0.3449203393537126
  batch 430 loss: 0.3449157625436783
  batch 431 loss: 0.3448667300826159
  batch 432 loss: 0.34483036481671864
  batch 433 loss: 0.3447036990254911
  batch 434 loss: 0.3446081083628439
  batch 435 loss: 0.3446900624653389
  batch 436 loss: 0.34470366782278095
  batch 437 loss: 0.3446215858312166
  batch 438 loss: 0.3445411442213407
  batch 439 loss: 0.3444279553814195
  batch 440 loss: 0.34452293637123976
  batch 441 loss: 0.34453162104905055
  batch 442 loss: 0.3444649850890647
  batch 443 loss: 0.34438822850685896
  batch 444 loss: 0.3444250490482863
  batch 445 loss: 0.3444029810723294
  batch 446 loss: 0.3444237591439833
  batch 447 loss: 0.34442920799490057
  batch 448 loss: 0.34448476295386043
  batch 449 loss: 0.3444395809370054
  batch 450 loss: 0.34437584572368196
  batch 451 loss: 0.34433295305182293
  batch 452 loss: 0.34424037394006696
  batch 453 loss: 0.34417367889391665
  batch 454 loss: 0.34414459745264264
  batch 455 loss: 0.3440547791811136
  batch 456 loss: 0.3440258870914317
  batch 457 loss: 0.34399172559347924
  batch 458 loss: 0.3438810793983884
  batch 459 loss: 0.34382302893532646
  batch 460 loss: 0.3438107807999072
  batch 461 loss: 0.3438115902810707
  batch 462 loss: 0.3438302810703005
  batch 463 loss: 0.3438355481779086
  batch 464 loss: 0.34375153912295553
  batch 465 loss: 0.343724388012322
  batch 466 loss: 0.3436651022649118
  batch 467 loss: 0.3436939002870237
  batch 468 loss: 0.3436512121150636
  batch 469 loss: 0.3435762978947239
  batch 470 loss: 0.3436522429927866
  batch 471 loss: 0.3436443525388236
  batch 472 loss: 0.34358150328872566
LOSS train 0.34358150328872566 valid 0.2092401683330536
LOSS train 0.34358150328872566 valid 0.21189849078655243
LOSS train 0.34358150328872566 valid 0.2200164645910263
LOSS train 0.34358150328872566 valid 0.2092120684683323
LOSS train 0.34358150328872566 valid 0.21164627373218536
LOSS train 0.34358150328872566 valid 0.21537202348311743
LOSS train 0.34358150328872566 valid 0.2097274512052536
LOSS train 0.34358150328872566 valid 0.20895967446267605
LOSS train 0.34358150328872566 valid 0.20903872615761226
LOSS train 0.34358150328872566 valid 0.20777344107627868
LOSS train 0.34358150328872566 valid 0.20476976578885858
LOSS train 0.34358150328872566 valid 0.20655017842849097
LOSS train 0.34358150328872566 valid 0.2053728447510646
LOSS train 0.34358150328872566 valid 0.2034727931022644
LOSS train 0.34358150328872566 valid 0.20245711902777355
LOSS train 0.34358150328872566 valid 0.20451174676418304
LOSS train 0.34358150328872566 valid 0.20445657828274896
LOSS train 0.34358150328872566 valid 0.2044143701593081
LOSS train 0.34358150328872566 valid 0.20562990009784698
LOSS train 0.34358150328872566 valid 0.20556892976164817
LOSS train 0.34358150328872566 valid 0.20574927400975002
LOSS train 0.34358150328872566 valid 0.20451418581334027
LOSS train 0.34358150328872566 valid 0.20373305354429327
LOSS train 0.34358150328872566 valid 0.20396466491123041
LOSS train 0.34358150328872566 valid 0.2032107400894165
LOSS train 0.34358150328872566 valid 0.202523057277386
LOSS train 0.34358150328872566 valid 0.20268789871975226
LOSS train 0.34358150328872566 valid 0.20253487144197738
LOSS train 0.34358150328872566 valid 0.2018620952449996
LOSS train 0.34358150328872566 valid 0.20111789007981618
LOSS train 0.34358150328872566 valid 0.20103571972539347
LOSS train 0.34358150328872566 valid 0.2016607765108347
LOSS train 0.34358150328872566 valid 0.20092664568713217
LOSS train 0.34358150328872566 valid 0.20028876206454108
LOSS train 0.34358150328872566 valid 0.200745336498533
LOSS train 0.34358150328872566 valid 0.20122644470797646
LOSS train 0.34358150328872566 valid 0.20087143456613696
LOSS train 0.34358150328872566 valid 0.2005765010651789
LOSS train 0.34358150328872566 valid 0.2001289752049324
LOSS train 0.34358150328872566 valid 0.200558478012681
LOSS train 0.34358150328872566 valid 0.20038088865396453
LOSS train 0.34358150328872566 valid 0.20153384123529708
LOSS train 0.34358150328872566 valid 0.20158769675465518
LOSS train 0.34358150328872566 valid 0.20103175802664322
LOSS train 0.34358150328872566 valid 0.20050499041875203
LOSS train 0.34358150328872566 valid 0.20016387364138727
LOSS train 0.34358150328872566 valid 0.19982420256797304
LOSS train 0.34358150328872566 valid 0.20096203684806824
LOSS train 0.34358150328872566 valid 0.2005313190878654
LOSS train 0.34358150328872566 valid 0.20105737388134004
LOSS train 0.34358150328872566 valid 0.20058335772916383
LOSS train 0.34358150328872566 valid 0.20046987069340852
LOSS train 0.34358150328872566 valid 0.20101158770750155
LOSS train 0.34358150328872566 valid 0.20083101397311245
LOSS train 0.34358150328872566 valid 0.2009471278298985
LOSS train 0.34358150328872566 valid 0.20084505528211594
LOSS train 0.34358150328872566 valid 0.20039526072510502
LOSS train 0.34358150328872566 valid 0.20094243771043316
LOSS train 0.34358150328872566 valid 0.2009791779821202
LOSS train 0.34358150328872566 valid 0.200827057659626
LOSS train 0.34358150328872566 valid 0.20072110510263286
LOSS train 0.34358150328872566 valid 0.20044869641142507
LOSS train 0.34358150328872566 valid 0.20042779686905088
LOSS train 0.34358150328872566 valid 0.2002897998318076
LOSS train 0.34358150328872566 valid 0.19948076559947087
LOSS train 0.34358150328872566 valid 0.19935675552397064
LOSS train 0.34358150328872566 valid 0.19970263429542087
LOSS train 0.34358150328872566 valid 0.19913527851595597
LOSS train 0.34358150328872566 valid 0.19955402720665585
LOSS train 0.34358150328872566 valid 0.19995543722595488
LOSS train 0.34358150328872566 valid 0.2002404850553459
LOSS train 0.34358150328872566 valid 0.20055501887367833
LOSS train 0.34358150328872566 valid 0.20105951738684144
LOSS train 0.34358150328872566 valid 0.20114651080724355
LOSS train 0.34358150328872566 valid 0.20074613134066263
LOSS train 0.34358150328872566 valid 0.2008439469102182
LOSS train 0.34358150328872566 valid 0.20085737283353683
LOSS train 0.34358150328872566 valid 0.20073679223274574
LOSS train 0.34358150328872566 valid 0.20093359939659697
LOSS train 0.34358150328872566 valid 0.20071205962449312
LOSS train 0.34358150328872566 valid 0.20082903000307673
LOSS train 0.34358150328872566 valid 0.20096497401231672
LOSS train 0.34358150328872566 valid 0.2009744663913566
LOSS train 0.34358150328872566 valid 0.2010068664593356
LOSS train 0.34358150328872566 valid 0.2012291859177982
LOSS train 0.34358150328872566 valid 0.20119963066522464
LOSS train 0.34358150328872566 valid 0.20083428662398767
LOSS train 0.34358150328872566 valid 0.200771159238436
LOSS train 0.34358150328872566 valid 0.20123533335294616
LOSS train 0.34358150328872566 valid 0.20139342861043083
LOSS train 0.34358150328872566 valid 0.20148343928567655
LOSS train 0.34358150328872566 valid 0.2013535316547622
LOSS train 0.34358150328872566 valid 0.2012978502178705
LOSS train 0.34358150328872566 valid 0.20149810723167785
LOSS train 0.34358150328872566 valid 0.20164314759405036
LOSS train 0.34358150328872566 valid 0.20191384246572852
LOSS train 0.34358150328872566 valid 0.20201905594044126
LOSS train 0.34358150328872566 valid 0.2023425337915518
LOSS train 0.34358150328872566 valid 0.20241725775930616
LOSS train 0.34358150328872566 valid 0.20250095918774605
LOSS train 0.34358150328872566 valid 0.20244702445988608
LOSS train 0.34358150328872566 valid 0.20283861268384784
LOSS train 0.34358150328872566 valid 0.20268881205216194
LOSS train 0.34358150328872566 valid 0.2024310864508152
LOSS train 0.34358150328872566 valid 0.20257935027281443
LOSS train 0.34358150328872566 valid 0.2025836432036364
LOSS train 0.34358150328872566 valid 0.20241134509304973
LOSS train 0.34358150328872566 valid 0.2023742444537304
LOSS train 0.34358150328872566 valid 0.20196703322436832
LOSS train 0.34358150328872566 valid 0.20183431966738266
LOSS train 0.34358150328872566 valid 0.20197110643257965
LOSS train 0.34358150328872566 valid 0.202035625066076
LOSS train 0.34358150328872566 valid 0.20195892294951245
LOSS train 0.34358150328872566 valid 0.20191591834289985
LOSS train 0.34358150328872566 valid 0.202359629843546
LOSS train 0.34358150328872566 valid 0.20219653767758403
LOSS train 0.34358150328872566 valid 0.2025756979854698
LOSS train 0.34358150328872566 valid 0.20274631641173768
LOSS train 0.34358150328872566 valid 0.2025114147603011
LOSS train 0.34358150328872566 valid 0.20230000379184881
LOSS train 0.34358150328872566 valid 0.2023776680969995
LOSS train 0.34358150328872566 valid 0.2024759794356393
LOSS train 0.34358150328872566 valid 0.20243826460062972
LOSS train 0.34358150328872566 valid 0.20242671740631904
LOSS train 0.34358150328872566 valid 0.2024366592168808
LOSS train 0.34358150328872566 valid 0.20253139545047094
LOSS train 0.34358150328872566 valid 0.20258867306502784
LOSS train 0.34358150328872566 valid 0.20247995981480926
LOSS train 0.34358150328872566 valid 0.2023403421621914
LOSS train 0.34358150328872566 valid 0.20216163912644752
LOSS train 0.34358150328872566 valid 0.20206921849086995
LOSS train 0.34358150328872566 valid 0.20203930658824515
LOSS train 0.34358150328872566 valid 0.20197598288830063
LOSS train 0.34358150328872566 valid 0.20214146731504753
LOSS train 0.34358150328872566 valid 0.20213976347887957
LOSS train 0.34358150328872566 valid 0.2022492066025734
LOSS train 0.34358150328872566 valid 0.20216183440528646
LOSS train 0.34358150328872566 valid 0.20205260554085608
LOSS train 0.34358150328872566 valid 0.20195309378260332
LOSS train 0.34358150328872566 valid 0.2019802447940622
LOSS train 0.34358150328872566 valid 0.20202306145471885
LOSS train 0.34358150328872566 valid 0.20219086855649948
LOSS train 0.34358150328872566 valid 0.20220747162828911
LOSS train 0.34358150328872566 valid 0.20230678975996044
LOSS train 0.34358150328872566 valid 0.2021936680736213
LOSS train 0.34358150328872566 valid 0.20216529408138093
LOSS train 0.34358150328872566 valid 0.20216090804865572
LOSS train 0.34358150328872566 valid 0.20201507069774577
LOSS train 0.34358150328872566 valid 0.20204069660413984
LOSS train 0.34358150328872566 valid 0.20203355918327967
LOSS train 0.34358150328872566 valid 0.20190975444995804
LOSS train 0.34358150328872566 valid 0.20189881873758217
LOSS train 0.34358150328872566 valid 0.20167878873987136
LOSS train 0.34358150328872566 valid 0.2017658170941588
LOSS train 0.34358150328872566 valid 0.2017785273251995
LOSS train 0.34358150328872566 valid 0.20199126282181495
LOSS train 0.34358150328872566 valid 0.2018626923583875
LOSS train 0.34358150328872566 valid 0.2018411252883416
LOSS train 0.34358150328872566 valid 0.20201619231850845
LOSS train 0.34358150328872566 valid 0.20207218620926143
LOSS train 0.34358150328872566 valid 0.2021097189150982
LOSS train 0.34358150328872566 valid 0.2021497293938825
LOSS train 0.34358150328872566 valid 0.20192208675884762
LOSS train 0.34358150328872566 valid 0.2020044591186977
LOSS train 0.34358150328872566 valid 0.20204346748915586
LOSS train 0.34358150328872566 valid 0.2018710224922881
LOSS train 0.34358150328872566 valid 0.2019533026896551
LOSS train 0.34358150328872566 valid 0.20179482212378866
LOSS train 0.34358150328872566 valid 0.20159912082923234
LOSS train 0.34358150328872566 valid 0.2016029435922118
LOSS train 0.34358150328872566 valid 0.20149910232128457
LOSS train 0.34358150328872566 valid 0.2014939956540285
LOSS train 0.34358150328872566 valid 0.2014584872074899
LOSS train 0.34358150328872566 valid 0.20157244296252042
LOSS train 0.34358150328872566 valid 0.20148354308945793
LOSS train 0.34358150328872566 valid 0.2014163633469831
LOSS train 0.34358150328872566 valid 0.20138455590622573
LOSS train 0.34358150328872566 valid 0.2015118148554577
LOSS train 0.34358150328872566 valid 0.2015821053828607
LOSS train 0.34358150328872566 valid 0.20154091376397346
LOSS train 0.34358150328872566 valid 0.20148594183487128
LOSS train 0.34358150328872566 valid 0.20138078854306712
LOSS train 0.34358150328872566 valid 0.20140123237026195
LOSS train 0.34358150328872566 valid 0.20143642195540926
LOSS train 0.34358150328872566 valid 0.20148585599821967
LOSS train 0.34358150328872566 valid 0.20160252801192705
LOSS train 0.34358150328872566 valid 0.20151749141713515
LOSS train 0.34358150328872566 valid 0.20155517360631456
LOSS train 0.34358150328872566 valid 0.2014128194441871
LOSS train 0.34358150328872566 valid 0.201441396067017
LOSS train 0.34358150328872566 valid 0.20161502921456442
LOSS train 0.34358150328872566 valid 0.20165133592672646
LOSS train 0.34358150328872566 valid 0.20153298225118707
LOSS train 0.34358150328872566 valid 0.20151065574171617
LOSS train 0.34358150328872566 valid 0.2014394475099368
LOSS train 0.34358150328872566 valid 0.20171760851327253
LOSS train 0.34358150328872566 valid 0.20173295477622658
LOSS train 0.34358150328872566 valid 0.2018990007915882
LOSS train 0.34358150328872566 valid 0.20187122460885262
LOSS train 0.34358150328872566 valid 0.20191138938069345
LOSS train 0.34358150328872566 valid 0.20185164647612405
LOSS train 0.34358150328872566 valid 0.20191506620976005
LOSS train 0.34358150328872566 valid 0.2020546495180412
LOSS train 0.34358150328872566 valid 0.20198625961647315
LOSS train 0.34358150328872566 valid 0.20196079255604163
LOSS train 0.34358150328872566 valid 0.20195105454875428
LOSS train 0.34358150328872566 valid 0.20195372078729712
LOSS train 0.34358150328872566 valid 0.2017917281303268
LOSS train 0.34358150328872566 valid 0.20181333662220166
LOSS train 0.34358150328872566 valid 0.2018567938889776
LOSS train 0.34358150328872566 valid 0.20204914103469576
LOSS train 0.34358150328872566 valid 0.20198238999494966
LOSS train 0.34358150328872566 valid 0.20192987636221407
LOSS train 0.34358150328872566 valid 0.201936573625725
LOSS train 0.34358150328872566 valid 0.20172242466793505
LOSS train 0.34358150328872566 valid 0.2015848114258713
LOSS train 0.34358150328872566 valid 0.20141755367204342
LOSS train 0.34358150328872566 valid 0.20134228235537852
LOSS train 0.34358150328872566 valid 0.20141521701801857
LOSS train 0.34358150328872566 valid 0.20144229287450963
LOSS train 0.34358150328872566 valid 0.201394474749112
LOSS train 0.34358150328872566 valid 0.20149720587708927
LOSS train 0.34358150328872566 valid 0.20145315573354472
LOSS train 0.34358150328872566 valid 0.2013175896635013
LOSS train 0.34358150328872566 valid 0.20123275147544012
LOSS train 0.34358150328872566 valid 0.20114871028777773
LOSS train 0.34358150328872566 valid 0.20116258376495427
LOSS train 0.34358150328872566 valid 0.2010694984673408
LOSS train 0.34358150328872566 valid 0.2009921929591608
LOSS train 0.34358150328872566 valid 0.20106034376051116
LOSS train 0.34358150328872566 valid 0.20105366131443997
LOSS train 0.34358150328872566 valid 0.20114228848753304
LOSS train 0.34358150328872566 valid 0.20130782411333828
LOSS train 0.34358150328872566 valid 0.2012725075085958
LOSS train 0.34358150328872566 valid 0.20126730622129238
LOSS train 0.34358150328872566 valid 0.20118715929782996
LOSS train 0.34358150328872566 valid 0.20127216930882338
LOSS train 0.34358150328872566 valid 0.20134575170378724
LOSS train 0.34358150328872566 valid 0.20127975840947618
LOSS train 0.34358150328872566 valid 0.20101399136086304
LOSS train 0.34358150328872566 valid 0.20085712457346225
LOSS train 0.34358150328872566 valid 0.20089800083193898
LOSS train 0.34358150328872566 valid 0.20087956401055732
LOSS train 0.34358150328872566 valid 0.20087133494556927
LOSS train 0.34358150328872566 valid 0.2009710608696451
LOSS train 0.34358150328872566 valid 0.2010333836684382
LOSS train 0.34358150328872566 valid 0.20105848934968956
LOSS train 0.34358150328872566 valid 0.20107565453696635
LOSS train 0.34358150328872566 valid 0.20094427496314526
LOSS train 0.34358150328872566 valid 0.20111415022611617
LOSS train 0.34358150328872566 valid 0.20118563411482776
LOSS train 0.34358150328872566 valid 0.20120908361342218
LOSS train 0.34358150328872566 valid 0.20112244814280936
LOSS train 0.34358150328872566 valid 0.20120813809041901
LOSS train 0.34358150328872566 valid 0.2011720708772248
LOSS train 0.34358150328872566 valid 0.20111955935135484
LOSS train 0.34358150328872566 valid 0.2011462304494724
LOSS train 0.34358150328872566 valid 0.20122746077849885
LOSS train 0.34358150328872566 valid 0.20116840107330483
LOSS train 0.34358150328872566 valid 0.20101739099392524
LOSS train 0.34358150328872566 valid 0.2009951296437289
LOSS train 0.34358150328872566 valid 0.20113665525239843
LOSS train 0.34358150328872566 valid 0.20104209608451495
LOSS train 0.34358150328872566 valid 0.2010875033271132
LOSS train 0.34358150328872566 valid 0.2010317677034522
LOSS train 0.34358150328872566 valid 0.2010341783551345
LOSS train 0.34358150328872566 valid 0.20100974729668336
LOSS train 0.34358150328872566 valid 0.20103473955793166
LOSS train 0.34358150328872566 valid 0.2010898843241447
LOSS train 0.34358150328872566 valid 0.20100270217215574
LOSS train 0.34358150328872566 valid 0.200977836312843
LOSS train 0.34358150328872566 valid 0.2008224232122302
LOSS train 0.34358150328872566 valid 0.20073588072380302
LOSS train 0.34358150328872566 valid 0.20070599443721074
LOSS train 0.34358150328872566 valid 0.20078514554283836
LOSS train 0.34358150328872566 valid 0.2008102495169294
LOSS train 0.34358150328872566 valid 0.20084381415525498
LOSS train 0.34358150328872566 valid 0.2008441314637232
LOSS train 0.34358150328872566 valid 0.20082659437237674
LOSS train 0.34358150328872566 valid 0.20075404686587198
LOSS train 0.34358150328872566 valid 0.20069759999305753
LOSS train 0.34358150328872566 valid 0.2006851387679154
LOSS train 0.34358150328872566 valid 0.2007479960947913
LOSS train 0.34358150328872566 valid 0.20064740119055963
LOSS train 0.34358150328872566 valid 0.20070520540078482
LOSS train 0.34358150328872566 valid 0.200666831246206
LOSS train 0.34358150328872566 valid 0.20070771478402075
LOSS train 0.34358150328872566 valid 0.20069787858261
LOSS train 0.34358150328872566 valid 0.20065840826108794
LOSS train 0.34358150328872566 valid 0.2006228819489479
LOSS train 0.34358150328872566 valid 0.20059953051334395
LOSS train 0.34358150328872566 valid 0.2006008692902245
LOSS train 0.34358150328872566 valid 0.2006023296825715
LOSS train 0.34358150328872566 valid 0.20058909388018303
LOSS train 0.34358150328872566 valid 0.20060381752959752
LOSS train 0.34358150328872566 valid 0.20060950255877263
LOSS train 0.34358150328872566 valid 0.2005976793741939
LOSS train 0.34358150328872566 valid 0.20062363722780407
LOSS train 0.34358150328872566 valid 0.20064009471880553
LOSS train 0.34358150328872566 valid 0.2005998596549034
LOSS train 0.34358150328872566 valid 0.20057591746415807
LOSS train 0.34358150328872566 valid 0.2005777054571158
LOSS train 0.34358150328872566 valid 0.20051342942336997
LOSS train 0.34358150328872566 valid 0.20048827952460238
LOSS train 0.34358150328872566 valid 0.20049075494047072
LOSS train 0.34358150328872566 valid 0.20061850255610897
LOSS train 0.34358150328872566 valid 0.20056770343928074
LOSS train 0.34358150328872566 valid 0.20064817518963443
LOSS train 0.34358150328872566 valid 0.20069371723241405
LOSS train 0.34358150328872566 valid 0.20072035188636472
LOSS train 0.34358150328872566 valid 0.2007656540613849
LOSS train 0.34358150328872566 valid 0.20075106883469301
LOSS train 0.34358150328872566 valid 0.20084045165643905
LOSS train 0.34358150328872566 valid 0.2008339383040264
LOSS train 0.34358150328872566 valid 0.20081863398589786
LOSS train 0.34358150328872566 valid 0.20087823658427106
LOSS train 0.34358150328872566 valid 0.20082844323728363
LOSS train 0.34358150328872566 valid 0.20084953687663348
LOSS train 0.34358150328872566 valid 0.20087079203988317
LOSS train 0.34358150328872566 valid 0.20081664039753377
LOSS train 0.34358150328872566 valid 0.20084558998312907
LOSS train 0.34358150328872566 valid 0.20088240948523053
LOSS train 0.34358150328872566 valid 0.20080170999555028
LOSS train 0.34358150328872566 valid 0.20075944958277692
LOSS train 0.34358150328872566 valid 0.20062420414044307
LOSS train 0.34358150328872566 valid 0.20070975895491114
LOSS train 0.34358150328872566 valid 0.2006264511689498
LOSS train 0.34358150328872566 valid 0.2006826751024985
LOSS train 0.34358150328872566 valid 0.20063548623428518
LOSS train 0.34358150328872566 valid 0.20064547527017015
LOSS train 0.34358150328872566 valid 0.20053383910583947
LOSS train 0.34358150328872566 valid 0.2004982034215726
LOSS train 0.34358150328872566 valid 0.20056182266892614
LOSS train 0.34358150328872566 valid 0.20051092016482783
LOSS train 0.34358150328872566 valid 0.20047077705611044
LOSS train 0.34358150328872566 valid 0.20040671624952838
LOSS train 0.34358150328872566 valid 0.20055832176604682
LOSS train 0.34358150328872566 valid 0.2005022131567876
LOSS train 0.34358150328872566 valid 0.20045544844461402
LOSS train 0.34358150328872566 valid 0.20050496590488098
LOSS train 0.34358150328872566 valid 0.20048767363221065
LOSS train 0.34358150328872566 valid 0.20045605820347692
LOSS train 0.34358150328872566 valid 0.2003533084260479
LOSS train 0.34358150328872566 valid 0.2004066476988238
LOSS train 0.34358150328872566 valid 0.20049288285815198
LOSS train 0.34358150328872566 valid 0.20046676372344782
LOSS train 0.34358150328872566 valid 0.20042933085638096
LOSS train 0.34358150328872566 valid 0.20045580368103652
LOSS train 0.34358150328872566 valid 0.2004026525512466
LOSS train 0.34358150328872566 valid 0.20044239350727627
LOSS train 0.34358150328872566 valid 0.20047750954444593
LOSS train 0.34358150328872566 valid 0.20049632196737963
LOSS train 0.34358150328872566 valid 0.20057252881040655
LOSS train 0.34358150328872566 valid 0.2005358312678876
LOSS train 0.34358150328872566 valid 0.2004883909309414
LOSS train 0.34358150328872566 valid 0.20050223343325466
LOSS train 0.34358150328872566 valid 0.20054854757311633
LOSS train 0.34358150328872566 valid 0.20061455317382706
LOSS train 0.34358150328872566 valid 0.20061761589103422
LOSS train 0.34358150328872566 valid 0.2006919801235199
LOSS train 0.34358150328872566 valid 0.20064924108354668
LOSS train 0.34358150328872566 valid 0.2007465214360485
LOSS train 0.34358150328872566 valid 0.20080073284544564
LOSS train 0.34358150328872566 valid 0.20074872414653117
LOSS train 0.34358150328872566 valid 0.20080595294090167
LOSS train 0.34358150328872566 valid 0.20079565952058698
LOSS train 0.34358150328872566 valid 0.20080890772453122
LOSS train 0.34358150328872566 valid 0.20074938261962455
LOSS train 0.34358150328872566 valid 0.20078878391402846
EPOCH 3:
  batch 1 loss: 0.3814290761947632
  batch 2 loss: 0.37936335802078247
  batch 3 loss: 0.3627658585707347
  batch 4 loss: 0.3629966974258423
  batch 5 loss: 0.36570596098899844
  batch 6 loss: 0.35924666623274487
  batch 7 loss: 0.35457260268075125
  batch 8 loss: 0.35119933634996414
  batch 9 loss: 0.34787121415138245
  batch 10 loss: 0.3426429569721222
  batch 11 loss: 0.3423126448284496
  batch 12 loss: 0.3402119477589925
  batch 13 loss: 0.3385637952731206
  batch 14 loss: 0.3380938874823706
  batch 15 loss: 0.33877628048261005
  batch 16 loss: 0.3383421469479799
  batch 17 loss: 0.33795225444961996
  batch 18 loss: 0.33666462037298417
  batch 19 loss: 0.33601047647626775
  batch 20 loss: 0.3334591582417488
  batch 21 loss: 0.3369482656319936
  batch 22 loss: 0.3381131508133628
  batch 23 loss: 0.3359217501204947
  batch 24 loss: 0.3357520215213299
  batch 25 loss: 0.33624773859977725
  batch 26 loss: 0.33613551350740284
  batch 27 loss: 0.3364457995803268
  batch 28 loss: 0.3360835313796997
  batch 29 loss: 0.3352888730065576
  batch 30 loss: 0.33588143388430275
  batch 31 loss: 0.3357057754070528
  batch 32 loss: 0.3369501130655408
  batch 33 loss: 0.33841358441295044
  batch 34 loss: 0.33822623684125785
  batch 35 loss: 0.3393681083406721
  batch 36 loss: 0.3389231140414874
  batch 37 loss: 0.3385909747433018
  batch 38 loss: 0.3387833574884816
  batch 39 loss: 0.3395193937497261
  batch 40 loss: 0.3390668980777264
  batch 41 loss: 0.33915065483349127
  batch 42 loss: 0.33823639154434204
  batch 43 loss: 0.33758201987244363
  batch 44 loss: 0.33750570633194665
  batch 45 loss: 0.3365461203787062
  batch 46 loss: 0.3354138589423636
  batch 47 loss: 0.3352172374725342
  batch 48 loss: 0.33485363361736137
  batch 49 loss: 0.33517751888353
  batch 50 loss: 0.33543846309185027
  batch 51 loss: 0.33471402116850313
  batch 52 loss: 0.33452838315413547
  batch 53 loss: 0.33366776581080454
  batch 54 loss: 0.33322054478857255
  batch 55 loss: 0.33370337919755416
  batch 56 loss: 0.3340220611010279
  batch 57 loss: 0.33343543399844255
  batch 58 loss: 0.3328670915858499
  batch 59 loss: 0.3330108558727523
  batch 60 loss: 0.3328707421819369
  batch 61 loss: 0.33333382938728956
  batch 62 loss: 0.3338921199883184
  batch 63 loss: 0.3334654262141576
  batch 64 loss: 0.33305305568501353
  batch 65 loss: 0.33249494204154384
  batch 66 loss: 0.3319187904849197
  batch 67 loss: 0.3317433462214114
  batch 68 loss: 0.33174941381987405
  batch 69 loss: 0.33173793381538946
  batch 70 loss: 0.3310336036341531
  batch 71 loss: 0.33115132677722986
  batch 72 loss: 0.33092521213822895
  batch 73 loss: 0.33051472289921485
  batch 74 loss: 0.3300672628589579
  batch 75 loss: 0.3298070017496745
  batch 76 loss: 0.3298144869898495
  batch 77 loss: 0.3293447509988562
  batch 78 loss: 0.3294708121281404
  batch 79 loss: 0.3295481212531464
  batch 80 loss: 0.329427981749177
  batch 81 loss: 0.32901609348662103
  batch 82 loss: 0.32914232844259683
  batch 83 loss: 0.3286955245288022
  batch 84 loss: 0.3282865307160786
  batch 85 loss: 0.3285350904745214
  batch 86 loss: 0.3284960569337357
  batch 87 loss: 0.3285461640220949
  batch 88 loss: 0.3285525464876132
  batch 89 loss: 0.32836935426412006
  batch 90 loss: 0.3287739846441481
  batch 91 loss: 0.32865842620095054
  batch 92 loss: 0.3284228845782902
  batch 93 loss: 0.32826170357324747
  batch 94 loss: 0.3284290138077229
  batch 95 loss: 0.32840816378593446
  batch 96 loss: 0.32831295641760033
  batch 97 loss: 0.32888584714574914
  batch 98 loss: 0.3287350973304437
  batch 99 loss: 0.32861820104146244
  batch 100 loss: 0.32850818991661074
  batch 101 loss: 0.3281411655468516
  batch 102 loss: 0.3282490100930719
  batch 103 loss: 0.3279473335997572
  batch 104 loss: 0.32827235987553227
  batch 105 loss: 0.3281967242558797
  batch 106 loss: 0.32845741904006814
  batch 107 loss: 0.32834328474285446
  batch 108 loss: 0.32795795963870156
  batch 109 loss: 0.32824019992023434
  batch 110 loss: 0.3284992469982667
  batch 111 loss: 0.3285060463187931
  batch 112 loss: 0.32839037637625423
  batch 113 loss: 0.32804005388664986
  batch 114 loss: 0.32794072596650375
  batch 115 loss: 0.32767580877179686
  batch 116 loss: 0.32769435335849895
  batch 117 loss: 0.3277848897836147
  batch 118 loss: 0.3279304615521835
  batch 119 loss: 0.3282282252271636
  batch 120 loss: 0.3281263321638107
  batch 121 loss: 0.3282179396507169
  batch 122 loss: 0.328131050115726
  batch 123 loss: 0.32801107177889444
  batch 124 loss: 0.32785442086958116
  batch 125 loss: 0.3274523615837097
  batch 126 loss: 0.3276088261415088
  batch 127 loss: 0.3275606627539387
  batch 128 loss: 0.3273802387993783
  batch 129 loss: 0.32736134944960127
  batch 130 loss: 0.3272825493262364
  batch 131 loss: 0.3270140228835681
  batch 132 loss: 0.3270442822214329
  batch 133 loss: 0.32677335757061954
  batch 134 loss: 0.3265240314736295
  batch 135 loss: 0.32649581454418325
  batch 136 loss: 0.3266191291896736
  batch 137 loss: 0.32684873164135175
  batch 138 loss: 0.3270254124333893
  batch 139 loss: 0.3270431579016953
  batch 140 loss: 0.32726575136184693
  batch 141 loss: 0.327127864808901
  batch 142 loss: 0.3271686629930013
  batch 143 loss: 0.3274526664843926
  batch 144 loss: 0.3273581208454238
  batch 145 loss: 0.327466253165541
  batch 146 loss: 0.32741843470155374
  batch 147 loss: 0.3274173910925988
  batch 148 loss: 0.3275258782747629
  batch 149 loss: 0.3271420428016842
  batch 150 loss: 0.3273356690009435
  batch 151 loss: 0.32709887702733476
  batch 152 loss: 0.32690257873190076
  batch 153 loss: 0.32716745761484883
  batch 154 loss: 0.32720442445247205
  batch 155 loss: 0.3272495004438585
  batch 156 loss: 0.3270935935851855
  batch 157 loss: 0.327053304880288
  batch 158 loss: 0.32720253663727
  batch 159 loss: 0.32735160124376883
  batch 160 loss: 0.3271021321415901
  batch 161 loss: 0.3268235557197784
  batch 162 loss: 0.32667075557473263
  batch 163 loss: 0.32643345565152315
  batch 164 loss: 0.3265702097517688
  batch 165 loss: 0.3267312528509082
  batch 166 loss: 0.32683548798044043
  batch 167 loss: 0.3266447043704416
  batch 168 loss: 0.32636819318646476
  batch 169 loss: 0.3264688278091024
  batch 170 loss: 0.32650584105183095
  batch 171 loss: 0.3266487457947424
  batch 172 loss: 0.32662545785654423
  batch 173 loss: 0.32662287802365475
  batch 174 loss: 0.3266342681372303
  batch 175 loss: 0.32659394366400585
  batch 176 loss: 0.32674174823544244
  batch 177 loss: 0.3269502143401884
  batch 178 loss: 0.3265464319271988
  batch 179 loss: 0.3266483964866766
  batch 180 loss: 0.3266377068228192
  batch 181 loss: 0.3264193518385703
  batch 182 loss: 0.32632733472100983
  batch 183 loss: 0.3261494149601525
  batch 184 loss: 0.32602226539798407
  batch 185 loss: 0.32585935705416913
  batch 186 loss: 0.32587226968939587
  batch 187 loss: 0.3260583337296777
  batch 188 loss: 0.3262062207498449
  batch 189 loss: 0.32626582579637964
  batch 190 loss: 0.3262264788150787
  batch 191 loss: 0.326342480188889
  batch 192 loss: 0.3261355801175038
  batch 193 loss: 0.3260721513335569
  batch 194 loss: 0.3260209749347156
  batch 195 loss: 0.3262444020845951
  batch 196 loss: 0.3261503367399683
  batch 197 loss: 0.3263046213515519
  batch 198 loss: 0.32639135691252624
  batch 199 loss: 0.3263192530253425
  batch 200 loss: 0.3262223896384239
  batch 201 loss: 0.3261387174698844
  batch 202 loss: 0.32597760339774706
  batch 203 loss: 0.32593726304364323
  batch 204 loss: 0.3259447862704595
  batch 205 loss: 0.3257985309856694
  batch 206 loss: 0.32578222554864233
  batch 207 loss: 0.32584249800530035
  batch 208 loss: 0.32560611774141973
  batch 209 loss: 0.32561497554254304
  batch 210 loss: 0.3256379300639743
  batch 211 loss: 0.32550948265039525
  batch 212 loss: 0.3255612114971539
  batch 213 loss: 0.3253597140032361
  batch 214 loss: 0.32529934391240095
  batch 215 loss: 0.3253696535908899
  batch 216 loss: 0.32551577121571257
  batch 217 loss: 0.32524919688426956
  batch 218 loss: 0.32508144438813585
  batch 219 loss: 0.3248205371371143
  batch 220 loss: 0.32484852143309334
  batch 221 loss: 0.3248174948390253
  batch 222 loss: 0.32484468817710876
  batch 223 loss: 0.32473804718175814
  batch 224 loss: 0.32480302386518034
  batch 225 loss: 0.3249285309844547
  batch 226 loss: 0.3249531655976203
  batch 227 loss: 0.3247861757152406
  batch 228 loss: 0.3247897069443736
  batch 229 loss: 0.3247926630828058
  batch 230 loss: 0.32475431094998897
  batch 231 loss: 0.3247642984121909
  batch 232 loss: 0.324584718536714
  batch 233 loss: 0.32454975301104044
  batch 234 loss: 0.3244741797192484
  batch 235 loss: 0.3242689388863584
  batch 236 loss: 0.32417901428574225
  batch 237 loss: 0.32427125251242883
  batch 238 loss: 0.3241248507710064
  batch 239 loss: 0.32409773661002955
  batch 240 loss: 0.3241907595346371
  batch 241 loss: 0.32406210206850933
  batch 242 loss: 0.32404892762337834
  batch 243 loss: 0.32408004974632104
  batch 244 loss: 0.3240917566858354
  batch 245 loss: 0.32414909917481094
  batch 246 loss: 0.3241646556350274
  batch 247 loss: 0.3241218449133128
  batch 248 loss: 0.32421048538338754
  batch 249 loss: 0.3241241468961938
  batch 250 loss: 0.32400992584228516
  batch 251 loss: 0.32384739786505223
  batch 252 loss: 0.3237143534989584
  batch 253 loss: 0.3236753851057512
  batch 254 loss: 0.3236564463517797
  batch 255 loss: 0.3234267798124575
  batch 256 loss: 0.3233710144413635
  batch 257 loss: 0.3234459600096083
  batch 258 loss: 0.32328197175218154
  batch 259 loss: 0.32306704949228
  batch 260 loss: 0.32294252377289995
  batch 261 loss: 0.32305388694978765
  batch 262 loss: 0.3229786372594251
  batch 263 loss: 0.32287135187663957
  batch 264 loss: 0.32306306210882735
  batch 265 loss: 0.32306171880578094
  batch 266 loss: 0.323098534704151
  batch 267 loss: 0.32285752881332286
  batch 268 loss: 0.32279905954848476
  batch 269 loss: 0.3227839566296361
  batch 270 loss: 0.32279311727594445
  batch 271 loss: 0.3228343559147247
  batch 272 loss: 0.32274561342509356
  batch 273 loss: 0.32269477669572655
  batch 274 loss: 0.32253964310579925
  batch 275 loss: 0.3225485133041035
  batch 276 loss: 0.32256009742833563
  batch 277 loss: 0.3225508651793649
  batch 278 loss: 0.32245447138230576
  batch 279 loss: 0.3224632104451511
  batch 280 loss: 0.3223571867815086
  batch 281 loss: 0.3223102516975267
  batch 282 loss: 0.3222693089686387
  batch 283 loss: 0.3222202331143639
  batch 284 loss: 0.3222126787607099
  batch 285 loss: 0.3221374910128744
  batch 286 loss: 0.32215258656265017
  batch 287 loss: 0.32207038894762974
  batch 288 loss: 0.32206476262460154
  batch 289 loss: 0.32206117993407596
  batch 290 loss: 0.3220274731003005
  batch 291 loss: 0.32198141192652513
  batch 292 loss: 0.32195962598062544
  batch 293 loss: 0.3220283504028776
  batch 294 loss: 0.32195697410577007
  batch 295 loss: 0.3219386513960564
  batch 296 loss: 0.3221193118071234
  batch 297 loss: 0.3221330798234201
  batch 298 loss: 0.3220702731769357
  batch 299 loss: 0.32214954007030727
  batch 300 loss: 0.32212724486986793
  batch 301 loss: 0.3221690968619629
  batch 302 loss: 0.3222265929378421
  batch 303 loss: 0.3221736468694391
  batch 304 loss: 0.32220896558934137
  batch 305 loss: 0.32213224631841064
  batch 306 loss: 0.3222476306304433
  batch 307 loss: 0.32221196062790064
  batch 308 loss: 0.32221623326276805
  batch 309 loss: 0.3221108246003926
  batch 310 loss: 0.32212844740959906
  batch 311 loss: 0.3219369631679878
  batch 312 loss: 0.32186710853607226
  batch 313 loss: 0.3218161279020218
  batch 314 loss: 0.3218596229317841
  batch 315 loss: 0.3218036178558592
  batch 316 loss: 0.3217597048305258
  batch 317 loss: 0.32190562977399734
  batch 318 loss: 0.3217462565539018
  batch 319 loss: 0.3215517777260568
  batch 320 loss: 0.32168072341009973
  batch 321 loss: 0.3216403644775676
  batch 322 loss: 0.32150200435093473
  batch 323 loss: 0.3215243678897527
  batch 324 loss: 0.32150269980415885
  batch 325 loss: 0.3215408634222471
  batch 326 loss: 0.3216605887639742
  batch 327 loss: 0.3216049611933005
  batch 328 loss: 0.3215700325260802
  batch 329 loss: 0.32152552472421825
  batch 330 loss: 0.3215500078418038
  batch 331 loss: 0.32146977919106035
  batch 332 loss: 0.32156836914728926
  batch 333 loss: 0.32150564510543067
  batch 334 loss: 0.32169768919131
  batch 335 loss: 0.3216929397476253
  batch 336 loss: 0.3216962680397999
  batch 337 loss: 0.32166567480175007
  batch 338 loss: 0.3215888860310323
  batch 339 loss: 0.3215177528626096
  batch 340 loss: 0.32146297158563836
  batch 341 loss: 0.32138657491228095
  batch 342 loss: 0.32137733541036906
  batch 343 loss: 0.3214649893452058
  batch 344 loss: 0.32148941753562105
  batch 345 loss: 0.32153790005739186
  batch 346 loss: 0.321477397587258
  batch 347 loss: 0.3213406562461633
  batch 348 loss: 0.3212914252760767
  batch 349 loss: 0.32134614479234364
  batch 350 loss: 0.3213403000150408
  batch 351 loss: 0.32135082369516377
  batch 352 loss: 0.3212858397005634
  batch 353 loss: 0.32128611713563415
  batch 354 loss: 0.3213082418098288
  batch 355 loss: 0.32124958810671955
  batch 356 loss: 0.32126036504011474
  batch 357 loss: 0.3210318571880084
  batch 358 loss: 0.3208622153244871
  batch 359 loss: 0.3208278689875908
  batch 360 loss: 0.32072631013062264
  batch 361 loss: 0.32068710660670274
  batch 362 loss: 0.32060996768224304
  batch 363 loss: 0.32065551307247364
  batch 364 loss: 0.32056069161210743
  batch 365 loss: 0.32061457348196476
  batch 366 loss: 0.32065720595622976
  batch 367 loss: 0.32071040932099243
  batch 368 loss: 0.320625517760282
  batch 369 loss: 0.32064907612193244
  batch 370 loss: 0.32077792560732044
  batch 371 loss: 0.32073518722205147
  batch 372 loss: 0.32078811181809314
  batch 373 loss: 0.3208977567286658
  batch 374 loss: 0.3208417144210581
  batch 375 loss: 0.32088913504282635
  batch 376 loss: 0.32095867141764217
  batch 377 loss: 0.3209439167451479
  batch 378 loss: 0.32085374447088394
  batch 379 loss: 0.32087950750517025
  batch 380 loss: 0.32081095607657184
  batch 381 loss: 0.3207613722545894
  batch 382 loss: 0.32069924941861816
  batch 383 loss: 0.3206346261439062
  batch 384 loss: 0.32065707685736317
  batch 385 loss: 0.3206102760581227
  batch 386 loss: 0.32060809279044056
  batch 387 loss: 0.32058613452800483
  batch 388 loss: 0.32058588169591945
  batch 389 loss: 0.3205732678692874
  batch 390 loss: 0.3205821919135558
  batch 391 loss: 0.32042549546722254
  batch 392 loss: 0.3204938146684851
  batch 393 loss: 0.32053455164414324
  batch 394 loss: 0.32057402430452064
  batch 395 loss: 0.32057611263251
  batch 396 loss: 0.32061336061569173
  batch 397 loss: 0.32069092921405956
  batch 398 loss: 0.3206431793058338
  batch 399 loss: 0.32061619171522615
  batch 400 loss: 0.32070145465433597
  batch 401 loss: 0.32062239019650773
  batch 402 loss: 0.3205758522250759
  batch 403 loss: 0.3204609095014946
  batch 404 loss: 0.3203854054065034
  batch 405 loss: 0.32043387242305427
  batch 406 loss: 0.3203727409904226
  batch 407 loss: 0.3204166878939261
  batch 408 loss: 0.3203577787122306
  batch 409 loss: 0.3203234831394951
  batch 410 loss: 0.32029539840977367
  batch 411 loss: 0.32028829326304786
  batch 412 loss: 0.32040380896295156
  batch 413 loss: 0.3203822281256715
  batch 414 loss: 0.3203338352234467
  batch 415 loss: 0.32030484073133353
  batch 416 loss: 0.3202460903960925
  batch 417 loss: 0.3201121886571248
  batch 418 loss: 0.32008369088743294
  batch 419 loss: 0.32006455826304125
  batch 420 loss: 0.32004081145638513
  batch 421 loss: 0.32005011893791147
  batch 422 loss: 0.3200275449532468
  batch 423 loss: 0.3199690400708652
  batch 424 loss: 0.3200888829130047
  batch 425 loss: 0.31994277617510625
  batch 426 loss: 0.31979141758641166
  batch 427 loss: 0.3198026196738875
  batch 428 loss: 0.3197405876539578
  batch 429 loss: 0.3196822100585991
  batch 430 loss: 0.3197382286537525
  batch 431 loss: 0.3196765373173557
  batch 432 loss: 0.31960898610176863
  batch 433 loss: 0.3195441780960312
  batch 434 loss: 0.31943768434535524
  batch 435 loss: 0.3195144235402688
  batch 436 loss: 0.3195480972255042
  batch 437 loss: 0.3194653485950671
  batch 438 loss: 0.31938006140325714
  batch 439 loss: 0.31931327470071225
  batch 440 loss: 0.31942001059651376
  batch 441 loss: 0.3194526552883676
  batch 442 loss: 0.3194251607985518
  batch 443 loss: 0.31935606018953344
  batch 444 loss: 0.3194162377768809
  batch 445 loss: 0.31938309080145333
  batch 446 loss: 0.3194419724257003
  batch 447 loss: 0.3194414853916339
  batch 448 loss: 0.319488377909043
  batch 449 loss: 0.3194835272955735
  batch 450 loss: 0.3194372724162208
  batch 451 loss: 0.31937313436668885
  batch 452 loss: 0.3192623599836257
  batch 453 loss: 0.31923297547609863
  batch 454 loss: 0.31919758520725017
  batch 455 loss: 0.3191126142884349
  batch 456 loss: 0.3190965486461656
  batch 457 loss: 0.31909656524658203
  batch 458 loss: 0.31900755416878446
  batch 459 loss: 0.3189715197013614
  batch 460 loss: 0.3189557781038077
  batch 461 loss: 0.3189827662098692
  batch 462 loss: 0.3190201751210473
  batch 463 loss: 0.31904030998376226
  batch 464 loss: 0.31897156358022116
  batch 465 loss: 0.31898059255333355
  batch 466 loss: 0.3189134989173627
  batch 467 loss: 0.31897547834690426
  batch 468 loss: 0.31893712339492947
  batch 469 loss: 0.31885394892458724
  batch 470 loss: 0.318980824566902
  batch 471 loss: 0.318955892273828
  batch 472 loss: 0.3188992450171608
LOSS train 0.3188992450171608 valid 0.19129903614521027
LOSS train 0.3188992450171608 valid 0.19447433948516846
LOSS train 0.3188992450171608 valid 0.2073462257782618
LOSS train 0.3188992450171608 valid 0.19806643202900887
LOSS train 0.3188992450171608 valid 0.20167070925235747
LOSS train 0.3188992450171608 valid 0.20463197181622186
LOSS train 0.3188992450171608 valid 0.1993452629872731
LOSS train 0.3188992450171608 valid 0.19831972010433674
LOSS train 0.3188992450171608 valid 0.19835449755191803
LOSS train 0.3188992450171608 valid 0.1974669173359871
LOSS train 0.3188992450171608 valid 0.19521987031806598
LOSS train 0.3188992450171608 valid 0.1967846378684044
LOSS train 0.3188992450171608 valid 0.19644786761357233
LOSS train 0.3188992450171608 valid 0.19448261708021164
LOSS train 0.3188992450171608 valid 0.193320436278979
LOSS train 0.3188992450171608 valid 0.1959705464541912
LOSS train 0.3188992450171608 valid 0.19561515573193045
LOSS train 0.3188992450171608 valid 0.19532577941815057
LOSS train 0.3188992450171608 valid 0.19657663608852186
LOSS train 0.3188992450171608 valid 0.19647027403116227
LOSS train 0.3188992450171608 valid 0.19689774442286717
LOSS train 0.3188992450171608 valid 0.19574394009330057
LOSS train 0.3188992450171608 valid 0.19490512816802316
LOSS train 0.3188992450171608 valid 0.19511859429379305
LOSS train 0.3188992450171608 valid 0.19431836426258087
LOSS train 0.3188992450171608 valid 0.1937613206414076
LOSS train 0.3188992450171608 valid 0.1940139659025051
LOSS train 0.3188992450171608 valid 0.19390561218772615
LOSS train 0.3188992450171608 valid 0.19306610576037703
LOSS train 0.3188992450171608 valid 0.19230988174676894
LOSS train 0.3188992450171608 valid 0.1921859186503195
LOSS train 0.3188992450171608 valid 0.1927586025558412
LOSS train 0.3188992450171608 valid 0.19203229067903577
LOSS train 0.3188992450171608 valid 0.1914930738070432
LOSS train 0.3188992450171608 valid 0.19201539073671614
LOSS train 0.3188992450171608 valid 0.19256047076649135
LOSS train 0.3188992450171608 valid 0.19192802624122515
LOSS train 0.3188992450171608 valid 0.19161663047577204
LOSS train 0.3188992450171608 valid 0.19125825204910377
LOSS train 0.3188992450171608 valid 0.19186240546405314
LOSS train 0.3188992450171608 valid 0.19157546431553074
LOSS train 0.3188992450171608 valid 0.19265582306044443
LOSS train 0.3188992450171608 valid 0.19275948717150576
LOSS train 0.3188992450171608 valid 0.1920049126175317
LOSS train 0.3188992450171608 valid 0.19161755475733017
LOSS train 0.3188992450171608 valid 0.19119439014922018
LOSS train 0.3188992450171608 valid 0.1908421072554081
LOSS train 0.3188992450171608 valid 0.19211030999819437
LOSS train 0.3188992450171608 valid 0.19173646794289959
LOSS train 0.3188992450171608 valid 0.19231493443250655
LOSS train 0.3188992450171608 valid 0.19188938684323253
LOSS train 0.3188992450171608 valid 0.19175937141363436
LOSS train 0.3188992450171608 valid 0.1924526238778852
LOSS train 0.3188992450171608 valid 0.19228377358780968
LOSS train 0.3188992450171608 valid 0.19251931363886052
LOSS train 0.3188992450171608 valid 0.19234327705843107
LOSS train 0.3188992450171608 valid 0.19182220336637998
LOSS train 0.3188992450171608 valid 0.1923443004488945
LOSS train 0.3188992450171608 valid 0.1924401892948959
LOSS train 0.3188992450171608 valid 0.19240377818544704
LOSS train 0.3188992450171608 valid 0.19236061685397976
LOSS train 0.3188992450171608 valid 0.19213213675445126
LOSS train 0.3188992450171608 valid 0.192085357175933
LOSS train 0.3188992450171608 valid 0.1920126595068723
LOSS train 0.3188992450171608 valid 0.19122882989736703
LOSS train 0.3188992450171608 valid 0.1912576366554607
LOSS train 0.3188992450171608 valid 0.19170062132735752
LOSS train 0.3188992450171608 valid 0.1911541317753932
LOSS train 0.3188992450171608 valid 0.1915513579396234
LOSS train 0.3188992450171608 valid 0.1919767922588757
LOSS train 0.3188992450171608 valid 0.19217896881237836
LOSS train 0.3188992450171608 valid 0.19248254348834357
LOSS train 0.3188992450171608 valid 0.19290508778944407
LOSS train 0.3188992450171608 valid 0.19297785916038462
LOSS train 0.3188992450171608 valid 0.19256499111652375
LOSS train 0.3188992450171608 valid 0.19271545288594147
LOSS train 0.3188992450171608 valid 0.19273571012082039
LOSS train 0.3188992450171608 valid 0.1926386159582016
LOSS train 0.3188992450171608 valid 0.19278281361241884
LOSS train 0.3188992450171608 valid 0.19254304934293032
LOSS train 0.3188992450171608 valid 0.19266530172324475
LOSS train 0.3188992450171608 valid 0.19281536467918536
LOSS train 0.3188992450171608 valid 0.1927881118762924
LOSS train 0.3188992450171608 valid 0.19274967660506567
LOSS train 0.3188992450171608 valid 0.19296564687700832
LOSS train 0.3188992450171608 valid 0.19297408884347872
LOSS train 0.3188992450171608 valid 0.19252990888453078
LOSS train 0.3188992450171608 valid 0.19257448190315204
LOSS train 0.3188992450171608 valid 0.19293471233228618
LOSS train 0.3188992450171608 valid 0.19303687148623996
LOSS train 0.3188992450171608 valid 0.19313591161927024
LOSS train 0.3188992450171608 valid 0.19299426626252092
LOSS train 0.3188992450171608 valid 0.19296479433454494
LOSS train 0.3188992450171608 valid 0.1931396388310067
LOSS train 0.3188992450171608 valid 0.19329345069433512
LOSS train 0.3188992450171608 valid 0.19356088681767383
LOSS train 0.3188992450171608 valid 0.19364531814437552
LOSS train 0.3188992450171608 valid 0.1939299462401137
LOSS train 0.3188992450171608 valid 0.19405142481278892
LOSS train 0.3188992450171608 valid 0.19415480881929398
LOSS train 0.3188992450171608 valid 0.19408750061941618
LOSS train 0.3188992450171608 valid 0.194418542963617
LOSS train 0.3188992450171608 valid 0.1942472977256312
LOSS train 0.3188992450171608 valid 0.19403358950064734
LOSS train 0.3188992450171608 valid 0.19427190195946467
LOSS train 0.3188992450171608 valid 0.19431632644725297
LOSS train 0.3188992450171608 valid 0.19411107424263643
LOSS train 0.3188992450171608 valid 0.1940831900746734
LOSS train 0.3188992450171608 valid 0.19364795088768005
LOSS train 0.3188992450171608 valid 0.19349421940066597
LOSS train 0.3188992450171608 valid 0.19361413518587747
LOSS train 0.3188992450171608 valid 0.19362880435905286
LOSS train 0.3188992450171608 valid 0.1935410688144971
LOSS train 0.3188992450171608 valid 0.19352335665832487
LOSS train 0.3188992450171608 valid 0.19395352964815887
LOSS train 0.3188992450171608 valid 0.19381714753549675
LOSS train 0.3188992450171608 valid 0.19415602075238514
LOSS train 0.3188992450171608 valid 0.19429843569711103
LOSS train 0.3188992450171608 valid 0.19405908256518742
LOSS train 0.3188992450171608 valid 0.19383632702132067
LOSS train 0.3188992450171608 valid 0.1938827322287993
LOSS train 0.3188992450171608 valid 0.19398735341478568
LOSS train 0.3188992450171608 valid 0.19395139716505036
LOSS train 0.3188992450171608 valid 0.19395545689809707
LOSS train 0.3188992450171608 valid 0.1940082588195801
LOSS train 0.3188992450171608 valid 0.1940783065935922
LOSS train 0.3188992450171608 valid 0.19411255888582216
LOSS train 0.3188992450171608 valid 0.19402523164171726
LOSS train 0.3188992450171608 valid 0.19384473134842953
LOSS train 0.3188992450171608 valid 0.19364917473151133
LOSS train 0.3188992450171608 valid 0.19357250824229408
LOSS train 0.3188992450171608 valid 0.19355353848500687
LOSS train 0.3188992450171608 valid 0.19344730977725266
LOSS train 0.3188992450171608 valid 0.19355995508272256
LOSS train 0.3188992450171608 valid 0.19353952551329578
LOSS train 0.3188992450171608 valid 0.1937364889856647
LOSS train 0.3188992450171608 valid 0.19369549994921162
LOSS train 0.3188992450171608 valid 0.19358604304168536
LOSS train 0.3188992450171608 valid 0.19343613302536147
LOSS train 0.3188992450171608 valid 0.19352641435606138
LOSS train 0.3188992450171608 valid 0.19358237605568365
LOSS train 0.3188992450171608 valid 0.19374098586784283
LOSS train 0.3188992450171608 valid 0.19380593518693964
LOSS train 0.3188992450171608 valid 0.19391094924261174
LOSS train 0.3188992450171608 valid 0.19379861159571285
LOSS train 0.3188992450171608 valid 0.19378315963565487
LOSS train 0.3188992450171608 valid 0.19379064977979985
LOSS train 0.3188992450171608 valid 0.19364302112041293
LOSS train 0.3188992450171608 valid 0.19372705435192825
LOSS train 0.3188992450171608 valid 0.19368819842735926
LOSS train 0.3188992450171608 valid 0.19359420595184856
LOSS train 0.3188992450171608 valid 0.1935723316120474
LOSS train 0.3188992450171608 valid 0.19331885445741268
LOSS train 0.3188992450171608 valid 0.19337738634316953
LOSS train 0.3188992450171608 valid 0.19342465448764062
LOSS train 0.3188992450171608 valid 0.19361781911590162
LOSS train 0.3188992450171608 valid 0.193570833392204
LOSS train 0.3188992450171608 valid 0.1935907192056692
LOSS train 0.3188992450171608 valid 0.1937430019273698
LOSS train 0.3188992450171608 valid 0.19378055967390537
LOSS train 0.3188992450171608 valid 0.19385493023795372
LOSS train 0.3188992450171608 valid 0.19390582532426456
LOSS train 0.3188992450171608 valid 0.19368465729286335
LOSS train 0.3188992450171608 valid 0.19373665459272338
LOSS train 0.3188992450171608 valid 0.19377573430538178
LOSS train 0.3188992450171608 valid 0.19359058805977006
LOSS train 0.3188992450171608 valid 0.19368466833037531
LOSS train 0.3188992450171608 valid 0.1935198243175234
LOSS train 0.3188992450171608 valid 0.19334611940313373
LOSS train 0.3188992450171608 valid 0.1933544098454363
LOSS train 0.3188992450171608 valid 0.19320716913680583
LOSS train 0.3188992450171608 valid 0.19320667742989783
LOSS train 0.3188992450171608 valid 0.19317917663582487
LOSS train 0.3188992450171608 valid 0.1933024893889482
LOSS train 0.3188992450171608 valid 0.1931988626718521
LOSS train 0.3188992450171608 valid 0.19314410334283655
LOSS train 0.3188992450171608 valid 0.1930770191264018
LOSS train 0.3188992450171608 valid 0.1932216707742616
LOSS train 0.3188992450171608 valid 0.19331218302249908
LOSS train 0.3188992450171608 valid 0.19327786184019513
LOSS train 0.3188992450171608 valid 0.19322418063385052
LOSS train 0.3188992450171608 valid 0.19312971036185275
LOSS train 0.3188992450171608 valid 0.19314931341207744
LOSS train 0.3188992450171608 valid 0.19312923919895422
LOSS train 0.3188992450171608 valid 0.1931627114076872
LOSS train 0.3188992450171608 valid 0.19329331927402046
LOSS train 0.3188992450171608 valid 0.1931880546763619
LOSS train 0.3188992450171608 valid 0.19320679194432624
LOSS train 0.3188992450171608 valid 0.19308481408805445
LOSS train 0.3188992450171608 valid 0.19306634115545374
LOSS train 0.3188992450171608 valid 0.19322608871609753
LOSS train 0.3188992450171608 valid 0.1932853073036919
LOSS train 0.3188992450171608 valid 0.19320300630646048
LOSS train 0.3188992450171608 valid 0.19317303283005646
LOSS train 0.3188992450171608 valid 0.1930750100276409
LOSS train 0.3188992450171608 valid 0.1933376132985767
LOSS train 0.3188992450171608 valid 0.1932997391913748
LOSS train 0.3188992450171608 valid 0.19343205142502834
LOSS train 0.3188992450171608 valid 0.1934051907541764
LOSS train 0.3188992450171608 valid 0.19341004095971584
LOSS train 0.3188992450171608 valid 0.19336027277642814
LOSS train 0.3188992450171608 valid 0.1934778461656948
LOSS train 0.3188992450171608 valid 0.1936193590093716
LOSS train 0.3188992450171608 valid 0.1935477188843138
LOSS train 0.3188992450171608 valid 0.1935018711700672
LOSS train 0.3188992450171608 valid 0.19353402644685172
LOSS train 0.3188992450171608 valid 0.193504446082645
LOSS train 0.3188992450171608 valid 0.19331319780590442
LOSS train 0.3188992450171608 valid 0.19334764479164873
LOSS train 0.3188992450171608 valid 0.1933934646702948
LOSS train 0.3188992450171608 valid 0.19358966453662982
LOSS train 0.3188992450171608 valid 0.19353403600881686
LOSS train 0.3188992450171608 valid 0.19350584711827024
LOSS train 0.3188992450171608 valid 0.19348841090904217
LOSS train 0.3188992450171608 valid 0.19325363733047662
LOSS train 0.3188992450171608 valid 0.19309041531825508
LOSS train 0.3188992450171608 valid 0.19291480194588412
LOSS train 0.3188992450171608 valid 0.19284315195378907
LOSS train 0.3188992450171608 valid 0.19290120300909155
LOSS train 0.3188992450171608 valid 0.19289121959697117
LOSS train 0.3188992450171608 valid 0.19283429064632002
LOSS train 0.3188992450171608 valid 0.1929310485049411
LOSS train 0.3188992450171608 valid 0.19288804739580026
LOSS train 0.3188992450171608 valid 0.19274968088471464
LOSS train 0.3188992450171608 valid 0.19264899492263793
LOSS train 0.3188992450171608 valid 0.1925673373373209
LOSS train 0.3188992450171608 valid 0.19255942047167454
LOSS train 0.3188992450171608 valid 0.19249624477928146
LOSS train 0.3188992450171608 valid 0.19242091249170262
LOSS train 0.3188992450171608 valid 0.19247655991626822
LOSS train 0.3188992450171608 valid 0.19249493941600188
LOSS train 0.3188992450171608 valid 0.19260541156962
LOSS train 0.3188992450171608 valid 0.19280776355911222
LOSS train 0.3188992450171608 valid 0.1927517537250478
LOSS train 0.3188992450171608 valid 0.1927263084244221
LOSS train 0.3188992450171608 valid 0.1926253034528029
LOSS train 0.3188992450171608 valid 0.19269417509247985
LOSS train 0.3188992450171608 valid 0.19273429420314916
LOSS train 0.3188992450171608 valid 0.19267809684057116
LOSS train 0.3188992450171608 valid 0.19240875424196321
LOSS train 0.3188992450171608 valid 0.19224783932766973
LOSS train 0.3188992450171608 valid 0.192315058828878
LOSS train 0.3188992450171608 valid 0.19231516582730376
LOSS train 0.3188992450171608 valid 0.1923183135566164
LOSS train 0.3188992450171608 valid 0.19238995496107608
LOSS train 0.3188992450171608 valid 0.19244879829447445
LOSS train 0.3188992450171608 valid 0.19250681676604003
LOSS train 0.3188992450171608 valid 0.19250443812099197
LOSS train 0.3188992450171608 valid 0.1923793232345198
LOSS train 0.3188992450171608 valid 0.1925458623766899
LOSS train 0.3188992450171608 valid 0.1926259361652739
LOSS train 0.3188992450171608 valid 0.19265122593395292
LOSS train 0.3188992450171608 valid 0.19255284511524698
LOSS train 0.3188992450171608 valid 0.19263964322373622
LOSS train 0.3188992450171608 valid 0.19261424243450165
LOSS train 0.3188992450171608 valid 0.19254940893733874
LOSS train 0.3188992450171608 valid 0.19256967984516796
LOSS train 0.3188992450171608 valid 0.19263976332984228
LOSS train 0.3188992450171608 valid 0.19258687768893812
LOSS train 0.3188992450171608 valid 0.19244758893664066
LOSS train 0.3188992450171608 valid 0.19243373342172396
LOSS train 0.3188992450171608 valid 0.19260449059136953
LOSS train 0.3188992450171608 valid 0.19251665994241662
LOSS train 0.3188992450171608 valid 0.19256693994005522
LOSS train 0.3188992450171608 valid 0.1925182746828727
LOSS train 0.3188992450171608 valid 0.19250932436688503
LOSS train 0.3188992450171608 valid 0.19248852959732884
LOSS train 0.3188992450171608 valid 0.19252305522338667
LOSS train 0.3188992450171608 valid 0.19259616250663886
LOSS train 0.3188992450171608 valid 0.19249671912855573
LOSS train 0.3188992450171608 valid 0.19244319359974668
LOSS train 0.3188992450171608 valid 0.1923078772995402
LOSS train 0.3188992450171608 valid 0.19222998116915915
LOSS train 0.3188992450171608 valid 0.1922018676452393
LOSS train 0.3188992450171608 valid 0.1922827300158414
LOSS train 0.3188992450171608 valid 0.19232141404696132
LOSS train 0.3188992450171608 valid 0.19235622576212624
LOSS train 0.3188992450171608 valid 0.19238024060245898
LOSS train 0.3188992450171608 valid 0.19237424182208207
LOSS train 0.3188992450171608 valid 0.1922875781676599
LOSS train 0.3188992450171608 valid 0.19222840815252257
LOSS train 0.3188992450171608 valid 0.19220169163342063
LOSS train 0.3188992450171608 valid 0.19226333104047674
LOSS train 0.3188992450171608 valid 0.1921473616655444
LOSS train 0.3188992450171608 valid 0.19220104175701477
LOSS train 0.3188992450171608 valid 0.19218060244005042
LOSS train 0.3188992450171608 valid 0.1922026629128107
LOSS train 0.3188992450171608 valid 0.19221643782738182
LOSS train 0.3188992450171608 valid 0.19220616923690254
LOSS train 0.3188992450171608 valid 0.192161037541669
LOSS train 0.3188992450171608 valid 0.19214674881643445
LOSS train 0.3188992450171608 valid 0.19212979159943044
LOSS train 0.3188992450171608 valid 0.19212218264670908
LOSS train 0.3188992450171608 valid 0.19210792013577052
LOSS train 0.3188992450171608 valid 0.19211472313282854
LOSS train 0.3188992450171608 valid 0.1921267568863727
LOSS train 0.3188992450171608 valid 0.19211891022595493
LOSS train 0.3188992450171608 valid 0.1921540402226
LOSS train 0.3188992450171608 valid 0.19214286931781066
LOSS train 0.3188992450171608 valid 0.19210041756431262
LOSS train 0.3188992450171608 valid 0.19208611483589755
LOSS train 0.3188992450171608 valid 0.19209419625089658
LOSS train 0.3188992450171608 valid 0.19203193623437345
LOSS train 0.3188992450171608 valid 0.1920132632122228
LOSS train 0.3188992450171608 valid 0.1920120430774376
LOSS train 0.3188992450171608 valid 0.19215441597443
LOSS train 0.3188992450171608 valid 0.19211165145864703
LOSS train 0.3188992450171608 valid 0.19220381646187273
LOSS train 0.3188992450171608 valid 0.19225823088371252
LOSS train 0.3188992450171608 valid 0.1922806155297064
LOSS train 0.3188992450171608 valid 0.19232819749228058
LOSS train 0.3188992450171608 valid 0.1923281047015618
LOSS train 0.3188992450171608 valid 0.1924225670841936
LOSS train 0.3188992450171608 valid 0.19243835415809776
LOSS train 0.3188992450171608 valid 0.192464098949281
LOSS train 0.3188992450171608 valid 0.19254385015066666
LOSS train 0.3188992450171608 valid 0.19251859587637785
LOSS train 0.3188992450171608 valid 0.19254771132701598
LOSS train 0.3188992450171608 valid 0.19256984897915472
LOSS train 0.3188992450171608 valid 0.19249720023944975
LOSS train 0.3188992450171608 valid 0.19252681221546042
LOSS train 0.3188992450171608 valid 0.19255598097669413
LOSS train 0.3188992450171608 valid 0.19246576163606377
LOSS train 0.3188992450171608 valid 0.19241955687786327
LOSS train 0.3188992450171608 valid 0.192281972857622
LOSS train 0.3188992450171608 valid 0.19237854368299062
LOSS train 0.3188992450171608 valid 0.1922950204177005
LOSS train 0.3188992450171608 valid 0.19234389829926374
LOSS train 0.3188992450171608 valid 0.19227861656062872
LOSS train 0.3188992450171608 valid 0.19227758578278803
LOSS train 0.3188992450171608 valid 0.1921788343760182
LOSS train 0.3188992450171608 valid 0.19214564494519348
LOSS train 0.3188992450171608 valid 0.1922198858884004
LOSS train 0.3188992450171608 valid 0.1921905396048894
LOSS train 0.3188992450171608 valid 0.1921566122503423
LOSS train 0.3188992450171608 valid 0.1921062727591821
LOSS train 0.3188992450171608 valid 0.1922716624393661
LOSS train 0.3188992450171608 valid 0.1922140168632276
LOSS train 0.3188992450171608 valid 0.1921630238009765
LOSS train 0.3188992450171608 valid 0.1922152158968589
LOSS train 0.3188992450171608 valid 0.19219987465838884
LOSS train 0.3188992450171608 valid 0.1921854289303049
LOSS train 0.3188992450171608 valid 0.19208793528921056
LOSS train 0.3188992450171608 valid 0.19211728229772213
LOSS train 0.3188992450171608 valid 0.19220010158808334
LOSS train 0.3188992450171608 valid 0.1921855570436213
LOSS train 0.3188992450171608 valid 0.19214154414897006
LOSS train 0.3188992450171608 valid 0.19217365980148315
LOSS train 0.3188992450171608 valid 0.1921178128965263
LOSS train 0.3188992450171608 valid 0.19215417981147767
LOSS train 0.3188992450171608 valid 0.19219419988471897
LOSS train 0.3188992450171608 valid 0.1922151232984933
LOSS train 0.3188992450171608 valid 0.1922895053966187
LOSS train 0.3188992450171608 valid 0.19224803726383521
LOSS train 0.3188992450171608 valid 0.19217131448463654
LOSS train 0.3188992450171608 valid 0.1921734938795647
LOSS train 0.3188992450171608 valid 0.19225116036519282
LOSS train 0.3188992450171608 valid 0.19229498655436425
LOSS train 0.3188992450171608 valid 0.1923022605167458
LOSS train 0.3188992450171608 valid 0.19237620880206427
LOSS train 0.3188992450171608 valid 0.19232750595276377
LOSS train 0.3188992450171608 valid 0.19243674387753998
LOSS train 0.3188992450171608 valid 0.19248866218016494
LOSS train 0.3188992450171608 valid 0.19244775036861608
LOSS train 0.3188992450171608 valid 0.19249657090396097
LOSS train 0.3188992450171608 valid 0.192496539302211
LOSS train 0.3188992450171608 valid 0.19250887388754281
LOSS train 0.3188992450171608 valid 0.19245503170658712
LOSS train 0.3188992450171608 valid 0.19247882003545114
EPOCH 4:
  batch 1 loss: 0.36254894733428955
  batch 2 loss: 0.36341550946235657
  batch 3 loss: 0.3428727090358734
  batch 4 loss: 0.34655556082725525
  batch 5 loss: 0.3459915637969971
  batch 6 loss: 0.33885247508684796
  batch 7 loss: 0.33660868235996794
  batch 8 loss: 0.33267783373594284
  batch 9 loss: 0.32983463671472335
  batch 10 loss: 0.3252716094255447
  batch 11 loss: 0.32488021796399896
  batch 12 loss: 0.32267151524623233
  batch 13 loss: 0.3207542093900534
  batch 14 loss: 0.3204358411686761
  batch 15 loss: 0.32230253020922345
  batch 16 loss: 0.3222890291363001
  batch 17 loss: 0.3225401569815243
  batch 18 loss: 0.3208833932876587
  batch 19 loss: 0.31953718003473786
  batch 20 loss: 0.3169910177588463
  batch 21 loss: 0.31987661832854863
  batch 22 loss: 0.32106576047160407
  batch 23 loss: 0.3185010837472003
  batch 24 loss: 0.31818420191605884
  batch 25 loss: 0.3185667324066162
  batch 26 loss: 0.3184803850375689
  batch 27 loss: 0.3192374562775647
  batch 28 loss: 0.31892155217272894
  batch 29 loss: 0.31848400728455906
  batch 30 loss: 0.3194494903087616
  batch 31 loss: 0.3192510989404494
  batch 32 loss: 0.32011401653289795
  batch 33 loss: 0.3213999605540073
  batch 34 loss: 0.32100920905085173
  batch 35 loss: 0.32243550760405404
  batch 36 loss: 0.3219491657283571
  batch 37 loss: 0.3215057592134218
  batch 38 loss: 0.3216599978898701
  batch 39 loss: 0.3222881234609164
  batch 40 loss: 0.3218385435640812
  batch 41 loss: 0.32159220736201216
  batch 42 loss: 0.32063115423633937
  batch 43 loss: 0.3199590506941773
  batch 44 loss: 0.31983765079216525
  batch 45 loss: 0.3191519604788886
  batch 46 loss: 0.31761158257722855
  batch 47 loss: 0.31720863155862117
  batch 48 loss: 0.31713662141313154
  batch 49 loss: 0.31761830953919157
  batch 50 loss: 0.31746054619550707
  batch 51 loss: 0.316921644058882
  batch 52 loss: 0.31675373447629124
  batch 53 loss: 0.3157349810847696
  batch 54 loss: 0.31530298485800073
  batch 55 loss: 0.3159520596265793
  batch 56 loss: 0.31627766814615044
  batch 57 loss: 0.31558185214536233
  batch 58 loss: 0.31506229860001594
  batch 59 loss: 0.3151222288103427
  batch 60 loss: 0.3151720258096854
  batch 61 loss: 0.31556390372456095
  batch 62 loss: 0.3159684127857608
  batch 63 loss: 0.3155705098594938
  batch 64 loss: 0.31519402633421123
  batch 65 loss: 0.31476912658948164
  batch 66 loss: 0.3144718194098184
  batch 67 loss: 0.3142699730485233
  batch 68 loss: 0.3141818850794259
  batch 69 loss: 0.3140490212734195
  batch 70 loss: 0.3135213751878057
  batch 71 loss: 0.313519869681815
  batch 72 loss: 0.3131305213189787
  batch 73 loss: 0.3127268816102041
  batch 74 loss: 0.3122567182054391
  batch 75 loss: 0.3121019019683202
  batch 76 loss: 0.31216144110811384
  batch 77 loss: 0.31154086198899655
  batch 78 loss: 0.31169114949611515
  batch 79 loss: 0.31192859875250467
  batch 80 loss: 0.31168973352760077
  batch 81 loss: 0.31135558438153915
  batch 82 loss: 0.3113266562179821
  batch 83 loss: 0.31099233372383805
  batch 84 loss: 0.31066709313364255
  batch 85 loss: 0.31087287156020893
  batch 86 loss: 0.3108364231018133
  batch 87 loss: 0.3106676845372408
  batch 88 loss: 0.31064716032282874
  batch 89 loss: 0.3102579607387607
  batch 90 loss: 0.31072894583145777
  batch 91 loss: 0.3106969305119672
  batch 92 loss: 0.31052521988749504
  batch 93 loss: 0.3104600824656025
  batch 94 loss: 0.31045838001560655
  batch 95 loss: 0.3104840929570951
  batch 96 loss: 0.310417572191606
  batch 97 loss: 0.31101611917166366
  batch 98 loss: 0.3109011274515366
  batch 99 loss: 0.3106859400115832
  batch 100 loss: 0.31056720986962316
  batch 101 loss: 0.31024506555335357
  batch 102 loss: 0.3103449281816389
  batch 103 loss: 0.3100129422342893
  batch 104 loss: 0.3103030677884817
  batch 105 loss: 0.31011796920072465
  batch 106 loss: 0.31040810118868667
  batch 107 loss: 0.31034083831533094
  batch 108 loss: 0.3100600710345639
  batch 109 loss: 0.3103637661135525
  batch 110 loss: 0.31047539751638065
  batch 111 loss: 0.3105304304275427
  batch 112 loss: 0.31037991693509476
  batch 113 loss: 0.31003482771658264
  batch 114 loss: 0.3097917062410137
  batch 115 loss: 0.3094801302837289
  batch 116 loss: 0.3095105551183224
  batch 117 loss: 0.30956860268727326
  batch 118 loss: 0.3097221160591659
  batch 119 loss: 0.3100191093042117
  batch 120 loss: 0.30987030553321043
  batch 121 loss: 0.3098962300818814
  batch 122 loss: 0.30984358404014933
  batch 123 loss: 0.30977774187316737
  batch 124 loss: 0.309651221839651
  batch 125 loss: 0.30935485208034513
  batch 126 loss: 0.3095033418328043
  batch 127 loss: 0.30948453707488505
  batch 128 loss: 0.30921230011153966
  batch 129 loss: 0.3092234008765036
  batch 130 loss: 0.30913831511369116
  batch 131 loss: 0.3088253626159129
  batch 132 loss: 0.3087673944731553
  batch 133 loss: 0.308524760200565
  batch 134 loss: 0.30824638091361345
  batch 135 loss: 0.30821961550800886
  batch 136 loss: 0.3082874937749961
  batch 137 loss: 0.3085293901445222
  batch 138 loss: 0.3086857442622599
  batch 139 loss: 0.30864964082515495
  batch 140 loss: 0.30895732886024885
  batch 141 loss: 0.3088155009433733
  batch 142 loss: 0.30878950686941686
  batch 143 loss: 0.30898536616688843
  batch 144 loss: 0.30888336927940446
  batch 145 loss: 0.3088535143383618
  batch 146 loss: 0.30882876366376877
  batch 147 loss: 0.3088603716115562
  batch 148 loss: 0.3090051395466199
  batch 149 loss: 0.3086171779256539
  batch 150 loss: 0.3087100598216057
  batch 151 loss: 0.30844581985710473
  batch 152 loss: 0.30839126861017
  batch 153 loss: 0.30868088936104493
  batch 154 loss: 0.30866660826004944
  batch 155 loss: 0.3085926047255916
  batch 156 loss: 0.3084253440491664
  batch 157 loss: 0.3083203126480625
  batch 158 loss: 0.3084947482883176
  batch 159 loss: 0.30858856955039426
  batch 160 loss: 0.3083882241509855
  batch 161 loss: 0.3080768338086442
  batch 162 loss: 0.30806228435701793
  batch 163 loss: 0.3078080550110413
  batch 164 loss: 0.3079237237390948
  batch 165 loss: 0.30799743415731373
  batch 166 loss: 0.3080097834568426
  batch 167 loss: 0.307855860648041
  batch 168 loss: 0.3075582848950511
  batch 169 loss: 0.30770130896356684
  batch 170 loss: 0.3076544347054818
  batch 171 loss: 0.30774826656656656
  batch 172 loss: 0.30782039454856586
  batch 173 loss: 0.3079404189235213
  batch 174 loss: 0.30809415697023784
  batch 175 loss: 0.30803152910300663
  batch 176 loss: 0.308303903026337
  batch 177 loss: 0.30858509225697167
  batch 178 loss: 0.30826635700598193
  batch 179 loss: 0.3083930575814327
  batch 180 loss: 0.3083895112905237
  batch 181 loss: 0.3083069562418026
  batch 182 loss: 0.3082007129605
  batch 183 loss: 0.3080088974185329
  batch 184 loss: 0.30786157763846544
  batch 185 loss: 0.3076950471948933
  batch 186 loss: 0.30773957930905843
  batch 187 loss: 0.30794110408122527
  batch 188 loss: 0.3079516232172225
  batch 189 loss: 0.307960021196219
  batch 190 loss: 0.30788866728544234
  batch 191 loss: 0.3080189593955484
  batch 192 loss: 0.30781993789908785
  batch 193 loss: 0.3077797079796618
  batch 194 loss: 0.3076774085459021
  batch 195 loss: 0.30793341191915363
  batch 196 loss: 0.3078016943621392
  batch 197 loss: 0.30814208861837533
  batch 198 loss: 0.3083146197175739
  batch 199 loss: 0.3082858555430743
  batch 200 loss: 0.308213227763772
  batch 201 loss: 0.3081764652213054
  batch 202 loss: 0.30815686514176943
  batch 203 loss: 0.3081042613155149
  batch 204 loss: 0.30814602043406636
  batch 205 loss: 0.3080396494487437
  batch 206 loss: 0.3079844457865919
  batch 207 loss: 0.3080431523098462
  batch 208 loss: 0.30787839804990935
  batch 209 loss: 0.3078873936782043
  batch 210 loss: 0.3079690844530151
  batch 211 loss: 0.30791237759646645
  batch 212 loss: 0.3079363315032338
  batch 213 loss: 0.3077751110557099
  batch 214 loss: 0.30781825255846307
  batch 215 loss: 0.30786232539387637
  batch 216 loss: 0.3079447913914919
  batch 217 loss: 0.30774096389245326
  batch 218 loss: 0.3075646515423005
  batch 219 loss: 0.3073711644977195
  batch 220 loss: 0.3074127403849905
  batch 221 loss: 0.30735914881143095
  batch 222 loss: 0.30730259599717885
  batch 223 loss: 0.30717190545503337
  batch 224 loss: 0.3072491022758186
  batch 225 loss: 0.30735960728592343
  batch 226 loss: 0.30734799373730093
  batch 227 loss: 0.30722091960749437
  batch 228 loss: 0.3072916935279704
  batch 229 loss: 0.30729385787482866
  batch 230 loss: 0.307296702719253
  batch 231 loss: 0.307372085911371
  batch 232 loss: 0.30725062310952567
  batch 233 loss: 0.3073293557110774
  batch 234 loss: 0.3072750518082554
  batch 235 loss: 0.3071009243422366
  batch 236 loss: 0.30702507439053667
  batch 237 loss: 0.3071631920614323
  batch 238 loss: 0.3070374935740182
  batch 239 loss: 0.3070233095764615
  batch 240 loss: 0.3070683507248759
  batch 241 loss: 0.30694670517662254
  batch 242 loss: 0.30690598235396316
  batch 243 loss: 0.3069204599155811
  batch 244 loss: 0.3069464828757966
  batch 245 loss: 0.30697549806565655
  batch 246 loss: 0.3070012880413513
  batch 247 loss: 0.30698743420332547
  batch 248 loss: 0.30711679106518147
  batch 249 loss: 0.30699226206325625
  batch 250 loss: 0.30687567621469497
  batch 251 loss: 0.3067268056579795
  batch 252 loss: 0.30652059904403156
  batch 253 loss: 0.30651639849536505
  batch 254 loss: 0.30656714496884757
  batch 255 loss: 0.30634751699718776
  batch 256 loss: 0.30633731320267543
  batch 257 loss: 0.30640383331460247
  batch 258 loss: 0.3062101508411326
  batch 259 loss: 0.3060997931078134
  batch 260 loss: 0.3060494447556826
  batch 261 loss: 0.3062154503960262
  batch 262 loss: 0.30613042101377747
  batch 263 loss: 0.306014155763637
  batch 264 loss: 0.3061320257344932
  batch 265 loss: 0.30614370911751154
  batch 266 loss: 0.30614797440462543
  batch 267 loss: 0.3059706272145782
  batch 268 loss: 0.30593978708137326
  batch 269 loss: 0.30592248414307277
  batch 270 loss: 0.3059966389227797
  batch 271 loss: 0.30605352491250337
  batch 272 loss: 0.30597589827854843
  batch 273 loss: 0.3059514879416197
  batch 274 loss: 0.30580538397070267
  batch 275 loss: 0.3058409830115058
  batch 276 loss: 0.30585977537692455
  batch 277 loss: 0.305854596654861
  batch 278 loss: 0.3057321096174151
  batch 279 loss: 0.30568345296981086
  batch 280 loss: 0.30557650524590696
  batch 281 loss: 0.3055561904487237
  batch 282 loss: 0.305550162938047
  batch 283 loss: 0.3054964714568411
  batch 284 loss: 0.30546726637952765
  batch 285 loss: 0.3053823642040554
  batch 286 loss: 0.30548665964311655
  batch 287 loss: 0.3054699110133307
  batch 288 loss: 0.30550195799312657
  batch 289 loss: 0.30553414247233973
  batch 290 loss: 0.3054746651957775
  batch 291 loss: 0.30541852551842064
  batch 292 loss: 0.3054454173025203
  batch 293 loss: 0.30553832044983886
  batch 294 loss: 0.3054850788242152
  batch 295 loss: 0.30548527346829235
  batch 296 loss: 0.3057563475559692
  batch 297 loss: 0.3057913344215464
  batch 298 loss: 0.3057023109305625
  batch 299 loss: 0.30579601447518456
  batch 300 loss: 0.30582890475789704
  batch 301 loss: 0.3058487398184811
  batch 302 loss: 0.30589214702513046
  batch 303 loss: 0.3058429413010972
  batch 304 loss: 0.3059233343228698
  batch 305 loss: 0.3058926539831474
  batch 306 loss: 0.30605505557816015
  batch 307 loss: 0.30598235727327266
  batch 308 loss: 0.30605233668700443
  batch 309 loss: 0.3059859565643045
  batch 310 loss: 0.3060320898890495
  batch 311 loss: 0.30590525360544396
  batch 312 loss: 0.3058617544384339
  batch 313 loss: 0.30577533965864884
  batch 314 loss: 0.3058087305183623
  batch 315 loss: 0.30576056918454547
  batch 316 loss: 0.3056888359162626
  batch 317 loss: 0.3058048223941484
  batch 318 loss: 0.30566362516497664
  batch 319 loss: 0.30549052250235803
  batch 320 loss: 0.3056296689901501
  batch 321 loss: 0.3055613917817951
  batch 322 loss: 0.30538981037665597
  batch 323 loss: 0.30541972299293835
  batch 324 loss: 0.3053733389594673
  batch 325 loss: 0.3054230613433398
  batch 326 loss: 0.3056056532848832
  batch 327 loss: 0.3055434521823119
  batch 328 loss: 0.30554933014621094
  batch 329 loss: 0.30553953358648756
  batch 330 loss: 0.305530054054477
  batch 331 loss: 0.30546367929241086
  batch 332 loss: 0.30554083676402827
  batch 333 loss: 0.3054584041461572
  batch 334 loss: 0.305639929801761
  batch 335 loss: 0.3056049351816747
  batch 336 loss: 0.3056051983453688
  batch 337 loss: 0.30558950721510086
  batch 338 loss: 0.30549272114532233
  batch 339 loss: 0.30545793385808095
  batch 340 loss: 0.30535559483310754
  batch 341 loss: 0.30528848904493616
  batch 342 loss: 0.3053371281913149
  batch 343 loss: 0.3054196979231459
  batch 344 loss: 0.3054662772649249
  batch 345 loss: 0.30550038153710574
  batch 346 loss: 0.30546646840827313
  batch 347 loss: 0.3053495048703653
  batch 348 loss: 0.3052764172269695
  batch 349 loss: 0.30539567495314646
  batch 350 loss: 0.3053593635984829
  batch 351 loss: 0.3053226038346603
  batch 352 loss: 0.30525724102996965
  batch 353 loss: 0.3052398362764218
  batch 354 loss: 0.3052439152099992
  batch 355 loss: 0.3051493985971934
  batch 356 loss: 0.3051274779221315
  batch 357 loss: 0.304933543883118
  batch 358 loss: 0.30475367393407077
  batch 359 loss: 0.3047317579015052
  batch 360 loss: 0.3046394900729259
  batch 361 loss: 0.3046216066111488
  batch 362 loss: 0.30453073826119387
  batch 363 loss: 0.30453815518496125
  batch 364 loss: 0.3044357551568812
  batch 365 loss: 0.3044419787926217
  batch 366 loss: 0.30449307302796774
  batch 367 loss: 0.30456478260356007
  batch 368 loss: 0.30446237775132706
  batch 369 loss: 0.30449983029346156
  batch 370 loss: 0.3046715143161851
  batch 371 loss: 0.3046312187158194
  batch 372 loss: 0.3047341835354605
  batch 373 loss: 0.30488113723395655
  batch 374 loss: 0.3048643023174077
  batch 375 loss: 0.3049121073484421
  batch 376 loss: 0.30500237647681794
  batch 377 loss: 0.304998952095009
  batch 378 loss: 0.3049282989410496
  batch 379 loss: 0.3049730599162446
  batch 380 loss: 0.3049133701151923
  batch 381 loss: 0.3048700333971364
  batch 382 loss: 0.30480683337487474
  batch 383 loss: 0.30476713978123726
  batch 384 loss: 0.30476538095778477
  batch 385 loss: 0.30470154444118597
  batch 386 loss: 0.30470029164317974
  batch 387 loss: 0.30469403722027477
  batch 388 loss: 0.30467818135914115
  batch 389 loss: 0.3046540445947402
  batch 390 loss: 0.3046813096373509
  batch 391 loss: 0.30454749520629876
  batch 392 loss: 0.30467716380193527
  batch 393 loss: 0.30472608001632545
  batch 394 loss: 0.304713913215901
  batch 395 loss: 0.3047478166939337
  batch 396 loss: 0.3048052214688123
  batch 397 loss: 0.3048460919325538
  batch 398 loss: 0.3048206449827956
  batch 399 loss: 0.3047891293178525
  batch 400 loss: 0.30488127391785386
  batch 401 loss: 0.3048261426705077
  batch 402 loss: 0.3048029921170491
  batch 403 loss: 0.3047282495732343
  batch 404 loss: 0.30467272251106725
  batch 405 loss: 0.30470374399497185
  batch 406 loss: 0.304617536662541
  batch 407 loss: 0.30461357775572184
  batch 408 loss: 0.3045323858147158
  batch 409 loss: 0.3044878354063827
  batch 410 loss: 0.3044280974966724
  batch 411 loss: 0.304424205364392
  batch 412 loss: 0.30450798249215755
  batch 413 loss: 0.3044634222912153
  batch 414 loss: 0.30442437943485046
  batch 415 loss: 0.30436102607882165
  batch 416 loss: 0.30438444308506757
  batch 417 loss: 0.3042807016941569
  batch 418 loss: 0.3042662427827502
  batch 419 loss: 0.304244054352469
  batch 420 loss: 0.3042567699792839
  batch 421 loss: 0.304297172789619
  batch 422 loss: 0.3042561593002053
  batch 423 loss: 0.30420715585527125
  batch 424 loss: 0.304283705346708
  batch 425 loss: 0.30416758134084587
  batch 426 loss: 0.304030255182808
  batch 427 loss: 0.3040445706604236
  batch 428 loss: 0.30401422730951666
  batch 429 loss: 0.3039323507627963
  batch 430 loss: 0.30393495199292203
  batch 431 loss: 0.3039301539795028
  batch 432 loss: 0.3039131359093719
  batch 433 loss: 0.303825549110254
  batch 434 loss: 0.3037574508223116
  batch 435 loss: 0.3038601212117864
  batch 436 loss: 0.3038950553591098
  batch 437 loss: 0.30380991510450156
  batch 438 loss: 0.30373906429227626
  batch 439 loss: 0.30365629714822445
  batch 440 loss: 0.3037351195107807
  batch 441 loss: 0.3037819321734024
  batch 442 loss: 0.30380471812653864
  batch 443 loss: 0.3037373127571347
  batch 444 loss: 0.3038381350872753
  batch 445 loss: 0.3038443761595179
  batch 446 loss: 0.30386812824572146
  batch 447 loss: 0.303897176412928
  batch 448 loss: 0.30395449365356136
  batch 449 loss: 0.3039080174038299
  batch 450 loss: 0.3038697426186667
  batch 451 loss: 0.3038605145630974
  batch 452 loss: 0.30376988649368286
  batch 453 loss: 0.30372746680219703
  batch 454 loss: 0.303714011012195
  batch 455 loss: 0.30362199564556497
  batch 456 loss: 0.30361943031873617
  batch 457 loss: 0.303631299368923
  batch 458 loss: 0.30355210394057647
  batch 459 loss: 0.3035027105564126
  batch 460 loss: 0.3034980692293333
  batch 461 loss: 0.30353527213901343
  batch 462 loss: 0.30357982431139263
  batch 463 loss: 0.3036030198791372
  batch 464 loss: 0.3035211396756871
  batch 465 loss: 0.30352123174616086
  batch 466 loss: 0.30347708603087414
  batch 467 loss: 0.30350593153618644
  batch 468 loss: 0.3034294651996376
  batch 469 loss: 0.3033747354677237
  batch 470 loss: 0.3035281894054819
  batch 471 loss: 0.30352418051909996
  batch 472 loss: 0.30349814102558764
LOSS train 0.30349814102558764 valid 0.1895095258951187
LOSS train 0.30349814102558764 valid 0.18846723437309265
LOSS train 0.30349814102558764 valid 0.20421006282170615
LOSS train 0.30349814102558764 valid 0.1963249184191227
LOSS train 0.30349814102558764 valid 0.20077323615550996
LOSS train 0.30349814102558764 valid 0.20349665731191635
LOSS train 0.30349814102558764 valid 0.19799195230007172
LOSS train 0.30349814102558764 valid 0.1974026821553707
LOSS train 0.30349814102558764 valid 0.198074695136812
LOSS train 0.30349814102558764 valid 0.19661699682474137
LOSS train 0.30349814102558764 valid 0.19439619915051895
LOSS train 0.30349814102558764 valid 0.1962392802039782
LOSS train 0.30349814102558764 valid 0.19561487092421606
LOSS train 0.30349814102558764 valid 0.19353465097291128
LOSS train 0.30349814102558764 valid 0.19247413873672486
LOSS train 0.30349814102558764 valid 0.19491161312907934
LOSS train 0.30349814102558764 valid 0.1939412574557697
LOSS train 0.30349814102558764 valid 0.1934863320655293
LOSS train 0.30349814102558764 valid 0.1945576110952779
LOSS train 0.30349814102558764 valid 0.19443410262465477
LOSS train 0.30349814102558764 valid 0.19498228246257418
LOSS train 0.30349814102558764 valid 0.1938874660567804
LOSS train 0.30349814102558764 valid 0.19292110463847284
LOSS train 0.30349814102558764 valid 0.19294715051849684
LOSS train 0.30349814102558764 valid 0.19267261624336243
LOSS train 0.30349814102558764 valid 0.19223060401586387
LOSS train 0.30349814102558764 valid 0.19265589338761788
LOSS train 0.30349814102558764 valid 0.19258988755089895
LOSS train 0.30349814102558764 valid 0.19162486853270694
LOSS train 0.30349814102558764 valid 0.190803687274456
LOSS train 0.30349814102558764 valid 0.19054492875452964
LOSS train 0.30349814102558764 valid 0.1910692360252142
LOSS train 0.30349814102558764 valid 0.19018748718680759
LOSS train 0.30349814102558764 valid 0.18977703899145126
LOSS train 0.30349814102558764 valid 0.19048486862863814
LOSS train 0.30349814102558764 valid 0.19096409322486985
LOSS train 0.30349814102558764 valid 0.19021962784432075
LOSS train 0.30349814102558764 valid 0.18979956131232412
LOSS train 0.30349814102558764 valid 0.18943673486892992
LOSS train 0.30349814102558764 valid 0.19000015929341316
LOSS train 0.30349814102558764 valid 0.18981438911542658
LOSS train 0.30349814102558764 valid 0.19101395103193464
LOSS train 0.30349814102558764 valid 0.19110114110070606
LOSS train 0.30349814102558764 valid 0.19020127268000084
LOSS train 0.30349814102558764 valid 0.18996160858207278
LOSS train 0.30349814102558764 valid 0.18940506292426068
LOSS train 0.30349814102558764 valid 0.18908816068730455
LOSS train 0.30349814102558764 valid 0.19066204192737737
LOSS train 0.30349814102558764 valid 0.19027405277806886
LOSS train 0.30349814102558764 valid 0.19083432286977767
LOSS train 0.30349814102558764 valid 0.19057313864137612
LOSS train 0.30349814102558764 valid 0.19045629094426447
LOSS train 0.30349814102558764 valid 0.1912715901743691
LOSS train 0.30349814102558764 valid 0.1910508683434239
LOSS train 0.30349814102558764 valid 0.19117006198926406
LOSS train 0.30349814102558764 valid 0.1910492695335831
LOSS train 0.30349814102558764 valid 0.19059590603175916
LOSS train 0.30349814102558764 valid 0.19128552901333776
LOSS train 0.30349814102558764 valid 0.19125662466226998
LOSS train 0.30349814102558764 valid 0.19125531340638796
LOSS train 0.30349814102558764 valid 0.19127569994965538
LOSS train 0.30349814102558764 valid 0.1909279631030175
LOSS train 0.30349814102558764 valid 0.19084749382639687
LOSS train 0.30349814102558764 valid 0.19078865414485335
LOSS train 0.30349814102558764 valid 0.18995994604550875
LOSS train 0.30349814102558764 valid 0.18992672770312338
LOSS train 0.30349814102558764 valid 0.19052082211224
LOSS train 0.30349814102558764 valid 0.18993680705042446
LOSS train 0.30349814102558764 valid 0.19034866535145303
LOSS train 0.30349814102558764 valid 0.19078596362045833
LOSS train 0.30349814102558764 valid 0.19101379853738865
LOSS train 0.30349814102558764 valid 0.19129741026295555
LOSS train 0.30349814102558764 valid 0.19164725893164333
LOSS train 0.30349814102558764 valid 0.1916887528590254
LOSS train 0.30349814102558764 valid 0.1912821408112844
LOSS train 0.30349814102558764 valid 0.19151855887551056
LOSS train 0.30349814102558764 valid 0.19157091228218823
LOSS train 0.30349814102558764 valid 0.19140467467980507
LOSS train 0.30349814102558764 valid 0.1914915084084378
LOSS train 0.30349814102558764 valid 0.19120626710355282
LOSS train 0.30349814102558764 valid 0.19125346545084024
LOSS train 0.30349814102558764 valid 0.19133167913774166
LOSS train 0.30349814102558764 valid 0.19130916606230908
LOSS train 0.30349814102558764 valid 0.1912605565573488
LOSS train 0.30349814102558764 valid 0.19153044328970067
LOSS train 0.30349814102558764 valid 0.19161166527936624
LOSS train 0.30349814102558764 valid 0.19110462514833473
LOSS train 0.30349814102558764 valid 0.19115747494453733
LOSS train 0.30349814102558764 valid 0.19145773568849886
LOSS train 0.30349814102558764 valid 0.19151622305313745
LOSS train 0.30349814102558764 valid 0.19159046552338443
LOSS train 0.30349814102558764 valid 0.19141636529694434
LOSS train 0.30349814102558764 valid 0.19144977292706888
LOSS train 0.30349814102558764 valid 0.19163347653886106
LOSS train 0.30349814102558764 valid 0.19177465878034894
LOSS train 0.30349814102558764 valid 0.19211875119556984
LOSS train 0.30349814102558764 valid 0.1921829247904807
LOSS train 0.30349814102558764 valid 0.19246958849989637
LOSS train 0.30349814102558764 valid 0.1925875943417501
LOSS train 0.30349814102558764 valid 0.19264351665973664
LOSS train 0.30349814102558764 valid 0.19263776517150424
LOSS train 0.30349814102558764 valid 0.19305000965501748
LOSS train 0.30349814102558764 valid 0.19282363774707018
LOSS train 0.30349814102558764 valid 0.1926065803720401
LOSS train 0.30349814102558764 valid 0.19289834797382355
LOSS train 0.30349814102558764 valid 0.19290396753909453
LOSS train 0.30349814102558764 valid 0.1926793204011204
LOSS train 0.30349814102558764 valid 0.1926554605640747
LOSS train 0.30349814102558764 valid 0.19214588403701782
LOSS train 0.30349814102558764 valid 0.19192277816208927
LOSS train 0.30349814102558764 valid 0.19201073829118195
LOSS train 0.30349814102558764 valid 0.19204772263765335
LOSS train 0.30349814102558764 valid 0.1919743524188489
LOSS train 0.30349814102558764 valid 0.19189742285954325
LOSS train 0.30349814102558764 valid 0.19231894340204156
LOSS train 0.30349814102558764 valid 0.19220810858853932
LOSS train 0.30349814102558764 valid 0.1924747327963511
LOSS train 0.30349814102558764 valid 0.19263261657650188
LOSS train 0.30349814102558764 valid 0.19240683429882305
LOSS train 0.30349814102558764 valid 0.19220068231225013
LOSS train 0.30349814102558764 valid 0.19227579802521005
LOSS train 0.30349814102558764 valid 0.19240681240793134
LOSS train 0.30349814102558764 valid 0.1923623894288288
LOSS train 0.30349814102558764 valid 0.19233079338746686
LOSS train 0.30349814102558764 valid 0.1924051364660263
LOSS train 0.30349814102558764 valid 0.1924436255579903
LOSS train 0.30349814102558764 valid 0.19243126321495987
LOSS train 0.30349814102558764 valid 0.19234625122044235
LOSS train 0.30349814102558764 valid 0.19213553581588952
LOSS train 0.30349814102558764 valid 0.19188368561176153
LOSS train 0.30349814102558764 valid 0.19183210914826576
LOSS train 0.30349814102558764 valid 0.19179765597888918
LOSS train 0.30349814102558764 valid 0.19167599261255192
LOSS train 0.30349814102558764 valid 0.19179924143784083
LOSS train 0.30349814102558764 valid 0.19177113616908037
LOSS train 0.30349814102558764 valid 0.19190335503834136
LOSS train 0.30349814102558764 valid 0.191907624041077
LOSS train 0.30349814102558764 valid 0.19177784738333328
LOSS train 0.30349814102558764 valid 0.19158198368206297
LOSS train 0.30349814102558764 valid 0.19168185378823963
LOSS train 0.30349814102558764 valid 0.19176889501564892
LOSS train 0.30349814102558764 valid 0.191935545436933
LOSS train 0.30349814102558764 valid 0.19201269047660427
LOSS train 0.30349814102558764 valid 0.1921267785752813
LOSS train 0.30349814102558764 valid 0.1919830685031825
LOSS train 0.30349814102558764 valid 0.19195169404353182
LOSS train 0.30349814102558764 valid 0.19198454247445476
LOSS train 0.30349814102558764 valid 0.19185866435637344
LOSS train 0.30349814102558764 valid 0.19207798634599518
LOSS train 0.30349814102558764 valid 0.1920334005355835
LOSS train 0.30349814102558764 valid 0.19190926761027202
LOSS train 0.30349814102558764 valid 0.19188766436357246
LOSS train 0.30349814102558764 valid 0.19161706140228346
LOSS train 0.30349814102558764 valid 0.19167293666245103
LOSS train 0.30349814102558764 valid 0.19169518168895475
LOSS train 0.30349814102558764 valid 0.19191788939329293
LOSS train 0.30349814102558764 valid 0.1918861946672391
LOSS train 0.30349814102558764 valid 0.1918877671602406
LOSS train 0.30349814102558764 valid 0.19205890961413113
LOSS train 0.30349814102558764 valid 0.19209081996232272
LOSS train 0.30349814102558764 valid 0.1921384675162179
LOSS train 0.30349814102558764 valid 0.19216643503786604
LOSS train 0.30349814102558764 valid 0.19196712504500993
LOSS train 0.30349814102558764 valid 0.19203302436848965
LOSS train 0.30349814102558764 valid 0.19212012607039827
LOSS train 0.30349814102558764 valid 0.19195567475384975
LOSS train 0.30349814102558764 valid 0.1920306411331999
LOSS train 0.30349814102558764 valid 0.19187428944167637
LOSS train 0.30349814102558764 valid 0.19166588950792007
LOSS train 0.30349814102558764 valid 0.1916726084316478
LOSS train 0.30349814102558764 valid 0.191490628217396
LOSS train 0.30349814102558764 valid 0.19146264639011648
LOSS train 0.30349814102558764 valid 0.19148967229906536
LOSS train 0.30349814102558764 valid 0.19163648938310557
LOSS train 0.30349814102558764 valid 0.19151786225182668
LOSS train 0.30349814102558764 valid 0.19142365667291664
LOSS train 0.30349814102558764 valid 0.19142124553521475
LOSS train 0.30349814102558764 valid 0.19160188709417086
LOSS train 0.30349814102558764 valid 0.19170123363673355
LOSS train 0.30349814102558764 valid 0.1916268226173189
LOSS train 0.30349814102558764 valid 0.19157113637054823
LOSS train 0.30349814102558764 valid 0.19148827237742289
LOSS train 0.30349814102558764 valid 0.19146833263459753
LOSS train 0.30349814102558764 valid 0.19142807841948842
LOSS train 0.30349814102558764 valid 0.19146758664298702
LOSS train 0.30349814102558764 valid 0.1916400214997671
LOSS train 0.30349814102558764 valid 0.19151724197647788
LOSS train 0.30349814102558764 valid 0.19148776299775916
LOSS train 0.30349814102558764 valid 0.19138606791458432
LOSS train 0.30349814102558764 valid 0.19137229448870607
LOSS train 0.30349814102558764 valid 0.19152874799923122
LOSS train 0.30349814102558764 valid 0.19158264164191982
LOSS train 0.30349814102558764 valid 0.19148290682333122
LOSS train 0.30349814102558764 valid 0.19147853315183797
LOSS train 0.30349814102558764 valid 0.19137853957139528
LOSS train 0.30349814102558764 valid 0.19165387285911306
LOSS train 0.30349814102558764 valid 0.19160826988329138
LOSS train 0.30349814102558764 valid 0.19170630309316847
LOSS train 0.30349814102558764 valid 0.1916515011733501
LOSS train 0.30349814102558764 valid 0.19166622646152973
LOSS train 0.30349814102558764 valid 0.19162032742108873
LOSS train 0.30349814102558764 valid 0.19171341855337123
LOSS train 0.30349814102558764 valid 0.1918778067151901
LOSS train 0.30349814102558764 valid 0.19178939157841252
LOSS train 0.30349814102558764 valid 0.1917505730216096
LOSS train 0.30349814102558764 valid 0.19178173879107224
LOSS train 0.30349814102558764 valid 0.19173336568949878
LOSS train 0.30349814102558764 valid 0.19155369627361113
LOSS train 0.30349814102558764 valid 0.19157568810944353
LOSS train 0.30349814102558764 valid 0.19164821427492867
LOSS train 0.30349814102558764 valid 0.19188848363844704
LOSS train 0.30349814102558764 valid 0.19182375403788854
LOSS train 0.30349814102558764 valid 0.19177883401722975
LOSS train 0.30349814102558764 valid 0.19173918880313356
LOSS train 0.30349814102558764 valid 0.19146919319796007
LOSS train 0.30349814102558764 valid 0.19128165559636223
LOSS train 0.30349814102558764 valid 0.1911110833355908
LOSS train 0.30349814102558764 valid 0.1910819986134494
LOSS train 0.30349814102558764 valid 0.19110682375354854
LOSS train 0.30349814102558764 valid 0.19108427227898078
LOSS train 0.30349814102558764 valid 0.19101426538029406
LOSS train 0.30349814102558764 valid 0.19108041375875473
LOSS train 0.30349814102558764 valid 0.19103755056858063
LOSS train 0.30349814102558764 valid 0.1908832397311926
LOSS train 0.30349814102558764 valid 0.19077486587895287
LOSS train 0.30349814102558764 valid 0.19068758591877677
LOSS train 0.30349814102558764 valid 0.19065796633123827
LOSS train 0.30349814102558764 valid 0.1906197067807641
LOSS train 0.30349814102558764 valid 0.19058339558053747
LOSS train 0.30349814102558764 valid 0.19066130123708558
LOSS train 0.30349814102558764 valid 0.19070128206308787
LOSS train 0.30349814102558764 valid 0.19078577630992594
LOSS train 0.30349814102558764 valid 0.19098534924277932
LOSS train 0.30349814102558764 valid 0.19092010585670796
LOSS train 0.30349814102558764 valid 0.19091347994956565
LOSS train 0.30349814102558764 valid 0.1907878420236757
LOSS train 0.30349814102558764 valid 0.19085403440622337
LOSS train 0.30349814102558764 valid 0.19088405971767522
LOSS train 0.30349814102558764 valid 0.19084135620663856
LOSS train 0.30349814102558764 valid 0.19056635467956465
LOSS train 0.30349814102558764 valid 0.19042012194380226
LOSS train 0.30349814102558764 valid 0.19051059353942715
LOSS train 0.30349814102558764 valid 0.19053761912471473
LOSS train 0.30349814102558764 valid 0.19053233457637614
LOSS train 0.30349814102558764 valid 0.19059228720713633
LOSS train 0.30349814102558764 valid 0.19063591066657043
LOSS train 0.30349814102558764 valid 0.19070207240127843
LOSS train 0.30349814102558764 valid 0.19073441384300108
LOSS train 0.30349814102558764 valid 0.19060938963928376
LOSS train 0.30349814102558764 valid 0.1907609981894493
LOSS train 0.30349814102558764 valid 0.19087045031239788
LOSS train 0.30349814102558764 valid 0.19088566066726806
LOSS train 0.30349814102558764 valid 0.19079107982603458
LOSS train 0.30349814102558764 valid 0.19088773200596412
LOSS train 0.30349814102558764 valid 0.1908348008698108
LOSS train 0.30349814102558764 valid 0.1907922543468885
LOSS train 0.30349814102558764 valid 0.1908053122605795
LOSS train 0.30349814102558764 valid 0.19085915438657583
LOSS train 0.30349814102558764 valid 0.1908192701321311
LOSS train 0.30349814102558764 valid 0.19068326010153844
LOSS train 0.30349814102558764 valid 0.19070478560138937
LOSS train 0.30349814102558764 valid 0.1909000148982492
LOSS train 0.30349814102558764 valid 0.19079835933877487
LOSS train 0.30349814102558764 valid 0.1908447276236433
LOSS train 0.30349814102558764 valid 0.19079010599064378
LOSS train 0.30349814102558764 valid 0.19078347361401507
LOSS train 0.30349814102558764 valid 0.19076881789312827
LOSS train 0.30349814102558764 valid 0.190797993051472
LOSS train 0.30349814102558764 valid 0.19090176614686902
LOSS train 0.30349814102558764 valid 0.19079634276805102
LOSS train 0.30349814102558764 valid 0.1907329848551662
LOSS train 0.30349814102558764 valid 0.19059188477694988
LOSS train 0.30349814102558764 valid 0.19051017893321348
LOSS train 0.30349814102558764 valid 0.19048847268967734
LOSS train 0.30349814102558764 valid 0.19056531515988437
LOSS train 0.30349814102558764 valid 0.19059736810732578
LOSS train 0.30349814102558764 valid 0.19062538557964973
LOSS train 0.30349814102558764 valid 0.1906711396446331
LOSS train 0.30349814102558764 valid 0.1906686548050159
LOSS train 0.30349814102558764 valid 0.1905699919909239
LOSS train 0.30349814102558764 valid 0.19048647301477045
LOSS train 0.30349814102558764 valid 0.19045358843414495
LOSS train 0.30349814102558764 valid 0.19054189124292703
LOSS train 0.30349814102558764 valid 0.19042884049491143
LOSS train 0.30349814102558764 valid 0.19047266950732783
LOSS train 0.30349814102558764 valid 0.1904558160088279
LOSS train 0.30349814102558764 valid 0.19045975337045118
LOSS train 0.30349814102558764 valid 0.19045445271250275
LOSS train 0.30349814102558764 valid 0.19045543742839852
LOSS train 0.30349814102558764 valid 0.19039734519761184
LOSS train 0.30349814102558764 valid 0.1903996413311188
LOSS train 0.30349814102558764 valid 0.19037618967768263
LOSS train 0.30349814102558764 valid 0.19035609437743958
LOSS train 0.30349814102558764 valid 0.1903046007160427
LOSS train 0.30349814102558764 valid 0.1903177009800733
LOSS train 0.30349814102558764 valid 0.19032926771890474
LOSS train 0.30349814102558764 valid 0.19034277876017472
LOSS train 0.30349814102558764 valid 0.1903896630810411
LOSS train 0.30349814102558764 valid 0.19037324150270443
LOSS train 0.30349814102558764 valid 0.19032266994317373
LOSS train 0.30349814102558764 valid 0.19032027451104896
LOSS train 0.30349814102558764 valid 0.19035709590114505
LOSS train 0.30349814102558764 valid 0.19029468843842498
LOSS train 0.30349814102558764 valid 0.19028953840269855
LOSS train 0.30349814102558764 valid 0.19026647610742536
LOSS train 0.30349814102558764 valid 0.19042629495360494
LOSS train 0.30349814102558764 valid 0.19038603908658416
LOSS train 0.30349814102558764 valid 0.19048089596938778
LOSS train 0.30349814102558764 valid 0.1905470086822232
LOSS train 0.30349814102558764 valid 0.19056937334037596
LOSS train 0.30349814102558764 valid 0.19059667542241393
LOSS train 0.30349814102558764 valid 0.19059756894906363
LOSS train 0.30349814102558764 valid 0.1906896042176329
LOSS train 0.30349814102558764 valid 0.19069857321157577
LOSS train 0.30349814102558764 valid 0.19074016601320298
LOSS train 0.30349814102558764 valid 0.1908261033270178
LOSS train 0.30349814102558764 valid 0.1908046548204843
LOSS train 0.30349814102558764 valid 0.19080848396761613
LOSS train 0.30349814102558764 valid 0.190814335751683
LOSS train 0.30349814102558764 valid 0.19073978778906167
LOSS train 0.30349814102558764 valid 0.19077619446029545
LOSS train 0.30349814102558764 valid 0.19078652921670713
LOSS train 0.30349814102558764 valid 0.19068789491343424
LOSS train 0.30349814102558764 valid 0.19065267256932494
LOSS train 0.30349814102558764 valid 0.19051625508528489
LOSS train 0.30349814102558764 valid 0.1906282289719289
LOSS train 0.30349814102558764 valid 0.19057173571273092
LOSS train 0.30349814102558764 valid 0.19062525433738056
LOSS train 0.30349814102558764 valid 0.19055209293010386
LOSS train 0.30349814102558764 valid 0.1905700759454207
LOSS train 0.30349814102558764 valid 0.1904775356237982
LOSS train 0.30349814102558764 valid 0.19043837617858347
LOSS train 0.30349814102558764 valid 0.19054072795508503
LOSS train 0.30349814102558764 valid 0.19050922026177367
LOSS train 0.30349814102558764 valid 0.19047798105140232
LOSS train 0.30349814102558764 valid 0.19043643289201317
LOSS train 0.30349814102558764 valid 0.19060723651587433
LOSS train 0.30349814102558764 valid 0.1905630861778231
LOSS train 0.30349814102558764 valid 0.19050590636807557
LOSS train 0.30349814102558764 valid 0.19055329307037241
LOSS train 0.30349814102558764 valid 0.19054657614126233
LOSS train 0.30349814102558764 valid 0.19051913459572875
LOSS train 0.30349814102558764 valid 0.19041900858065824
LOSS train 0.30349814102558764 valid 0.19041945044557715
LOSS train 0.30349814102558764 valid 0.19049634747747063
LOSS train 0.30349814102558764 valid 0.19046917269167873
LOSS train 0.30349814102558764 valid 0.19040552442973904
LOSS train 0.30349814102558764 valid 0.19043652113826795
LOSS train 0.30349814102558764 valid 0.19036626444334287
LOSS train 0.30349814102558764 valid 0.1903970782671656
LOSS train 0.30349814102558764 valid 0.19043170000583018
LOSS train 0.30349814102558764 valid 0.19046814743937415
LOSS train 0.30349814102558764 valid 0.1905286476311535
LOSS train 0.30349814102558764 valid 0.19050048135936598
LOSS train 0.30349814102558764 valid 0.19040503338189194
LOSS train 0.30349814102558764 valid 0.19039783120322762
LOSS train 0.30349814102558764 valid 0.1904810819806171
LOSS train 0.30349814102558764 valid 0.19052674076077658
LOSS train 0.30349814102558764 valid 0.19054558445650224
LOSS train 0.30349814102558764 valid 0.19062496518923178
LOSS train 0.30349814102558764 valid 0.19058108148152147
LOSS train 0.30349814102558764 valid 0.1906853470930737
LOSS train 0.30349814102558764 valid 0.190741704268889
LOSS train 0.30349814102558764 valid 0.19069980134020795
LOSS train 0.30349814102558764 valid 0.19075019771922125
LOSS train 0.30349814102558764 valid 0.19075297800406732
LOSS train 0.30349814102558764 valid 0.19076421036707283
LOSS train 0.30349814102558764 valid 0.1906875896713008
LOSS train 0.30349814102558764 valid 0.1907109234956545
EPOCH 5:
  batch 1 loss: 0.3442755341529846
  batch 2 loss: 0.34569336473941803
  batch 3 loss: 0.33237673838933307
  batch 4 loss: 0.34181953966617584
  batch 5 loss: 0.34060442447662354
  batch 6 loss: 0.332975834608078
  batch 7 loss: 0.3301474026271275
  batch 8 loss: 0.32664912194013596
  batch 9 loss: 0.32271673281987506
  batch 10 loss: 0.3178061217069626
  batch 11 loss: 0.31720841201868927
  batch 12 loss: 0.314672773083051
  batch 13 loss: 0.3114603345210736
  batch 14 loss: 0.30954333926950184
  batch 15 loss: 0.31072137753168744
  batch 16 loss: 0.3106609471142292
  batch 17 loss: 0.31102192927809325
  batch 18 loss: 0.3097513020038605
  batch 19 loss: 0.3092835937675677
  batch 20 loss: 0.30679941624403
  batch 21 loss: 0.30916087542261395
  batch 22 loss: 0.3103214353322983
  batch 23 loss: 0.30726245175237243
  batch 24 loss: 0.30715175966421765
  batch 25 loss: 0.3075922656059265
  batch 26 loss: 0.3070467102986116
  batch 27 loss: 0.3086424591364684
  batch 28 loss: 0.3083027611885752
  batch 29 loss: 0.3078958155780003
  batch 30 loss: 0.30944969654083254
  batch 31 loss: 0.30911501088450033
  batch 32 loss: 0.3097564959898591
  batch 33 loss: 0.3109275788971872
  batch 34 loss: 0.31055788169888887
  batch 35 loss: 0.31167778628213066
  batch 36 loss: 0.31100104335281586
  batch 37 loss: 0.310757129578977
  batch 38 loss: 0.31079337941972834
  batch 39 loss: 0.31089880986091417
  batch 40 loss: 0.3105801708996296
  batch 41 loss: 0.3104140823934136
  batch 42 loss: 0.3097785526797885
  batch 43 loss: 0.3093984334967857
  batch 44 loss: 0.3089696311137893
  batch 45 loss: 0.30790420836872523
  batch 46 loss: 0.3061696028579836
  batch 47 loss: 0.3057147705174507
  batch 48 loss: 0.3054723171517253
  batch 49 loss: 0.30557088371442287
  batch 50 loss: 0.3051636078953743
  batch 51 loss: 0.3048502794083427
  batch 52 loss: 0.3045596597859493
  batch 53 loss: 0.3036709756783719
  batch 54 loss: 0.30311915968303327
  batch 55 loss: 0.3035574086687782
  batch 56 loss: 0.30383305597518173
  batch 57 loss: 0.3029862801756775
  batch 58 loss: 0.3023857970176072
  batch 59 loss: 0.3024537060725487
  batch 60 loss: 0.30272968535621964
  batch 61 loss: 0.30308164363024664
  batch 62 loss: 0.30363023785814164
  batch 63 loss: 0.30309540246214184
  batch 64 loss: 0.30258976691402495
  batch 65 loss: 0.3023322336948835
  batch 66 loss: 0.3018930506977168
  batch 67 loss: 0.30149050323820825
  batch 68 loss: 0.30155422560432377
  batch 69 loss: 0.30137122519638226
  batch 70 loss: 0.300710195515837
  batch 71 loss: 0.3008164316415787
  batch 72 loss: 0.30047292489972377
  batch 73 loss: 0.29983053121664754
  batch 74 loss: 0.2994685920106398
  batch 75 loss: 0.2991533754269282
  batch 76 loss: 0.29908453182954536
  batch 77 loss: 0.2986609059107768
  batch 78 loss: 0.2988624605230796
  batch 79 loss: 0.2989276385382761
  batch 80 loss: 0.2986346995458007
  batch 81 loss: 0.29822783540060493
  batch 82 loss: 0.2982771856392302
  batch 83 loss: 0.2979985193674823
  batch 84 loss: 0.2977492910410677
  batch 85 loss: 0.29802379520500405
  batch 86 loss: 0.29791747050922973
  batch 87 loss: 0.297727026302239
  batch 88 loss: 0.2978426314551722
  batch 89 loss: 0.2974908018714926
  batch 90 loss: 0.2978499089678129
  batch 91 loss: 0.2976617670648701
  batch 92 loss: 0.2974142194118189
  batch 93 loss: 0.2971740001311866
  batch 94 loss: 0.29730490658511505
  batch 95 loss: 0.29746164161908
  batch 96 loss: 0.29742463600511354
  batch 97 loss: 0.29812438202272984
  batch 98 loss: 0.29806520759451144
  batch 99 loss: 0.2980333288209607
  batch 100 loss: 0.29795532003045083
  batch 101 loss: 0.2977143506897558
  batch 102 loss: 0.2978685505542101
  batch 103 loss: 0.2974692494834511
  batch 104 loss: 0.2976495407235164
  batch 105 loss: 0.2974205929608572
  batch 106 loss: 0.2976637689291306
  batch 107 loss: 0.2976455288791211
  batch 108 loss: 0.29732031913267243
  batch 109 loss: 0.297458947388404
  batch 110 loss: 0.29732706966725264
  batch 111 loss: 0.29734148219362033
  batch 112 loss: 0.29714452515223194
  batch 113 loss: 0.296832546064284
  batch 114 loss: 0.2966936906440216
  batch 115 loss: 0.29633649496928505
  batch 116 loss: 0.2963171204359367
  batch 117 loss: 0.2963612901094632
  batch 118 loss: 0.29662347105094944
  batch 119 loss: 0.29702655795742483
  batch 120 loss: 0.296799095099171
  batch 121 loss: 0.29664579193946744
  batch 122 loss: 0.296609319502213
  batch 123 loss: 0.29650369250192876
  batch 124 loss: 0.2964177015087297
  batch 125 loss: 0.29609692871570586
  batch 126 loss: 0.29624731188255643
  batch 127 loss: 0.2962591846392849
  batch 128 loss: 0.29611061175819486
  batch 129 loss: 0.29613832667354467
  batch 130 loss: 0.2960731152158517
  batch 131 loss: 0.2958151565025781
  batch 132 loss: 0.29575127668001433
  batch 133 loss: 0.2954770363587186
  batch 134 loss: 0.2951795457681613
  batch 135 loss: 0.2951558655058896
  batch 136 loss: 0.2953835930675268
  batch 137 loss: 0.29558316645396016
  batch 138 loss: 0.2957129378033721
  batch 139 loss: 0.2958440899634533
  batch 140 loss: 0.2961616647030626
  batch 141 loss: 0.2960380369678457
  batch 142 loss: 0.29593192072401586
  batch 143 loss: 0.2961504101961643
  batch 144 loss: 0.2962564228930407
  batch 145 loss: 0.29622997610733426
  batch 146 loss: 0.2961480228664124
  batch 147 loss: 0.2962526653696891
  batch 148 loss: 0.2964119366495996
  batch 149 loss: 0.2959860810297448
  batch 150 loss: 0.2961023446917534
  batch 151 loss: 0.2959221519579161
  batch 152 loss: 0.2957943373016621
  batch 153 loss: 0.29614675288496456
  batch 154 loss: 0.29609734061863513
  batch 155 loss: 0.29603157379934864
  batch 156 loss: 0.2959358068421865
  batch 157 loss: 0.29586458652262476
  batch 158 loss: 0.29599079932970335
  batch 159 loss: 0.2961224086067212
  batch 160 loss: 0.2958905597217381
  batch 161 loss: 0.29558271678708353
  batch 162 loss: 0.29553875593859474
  batch 163 loss: 0.29523560468770244
  batch 164 loss: 0.2953826586102567
  batch 165 loss: 0.2954480928905083
  batch 166 loss: 0.29554795743112106
  batch 167 loss: 0.29539982796072245
  batch 168 loss: 0.29511865964602857
  batch 169 loss: 0.2952519474473931
  batch 170 loss: 0.2951780742582153
  batch 171 loss: 0.29531742806671657
  batch 172 loss: 0.29537145628832107
  batch 173 loss: 0.29553335758647475
  batch 174 loss: 0.2955446079715915
  batch 175 loss: 0.29543708162648336
  batch 176 loss: 0.2956588465551084
  batch 177 loss: 0.2959026765857039
  batch 178 loss: 0.2955307100763482
  batch 179 loss: 0.2956597832494608
  batch 180 loss: 0.2956702878905667
  batch 181 loss: 0.2955147786199717
  batch 182 loss: 0.2954559677413532
  batch 183 loss: 0.2952426627196901
  batch 184 loss: 0.29501033695819584
  batch 185 loss: 0.2948541331130105
  batch 186 loss: 0.2948378535207882
  batch 187 loss: 0.29493806921543286
  batch 188 loss: 0.29493766032317853
  batch 189 loss: 0.294880260156576
  batch 190 loss: 0.2948121868466076
  batch 191 loss: 0.2950383264824982
  batch 192 loss: 0.2948803537680457
  batch 193 loss: 0.2949164370026613
  batch 194 loss: 0.2948540864685147
  batch 195 loss: 0.2950788920506453
  batch 196 loss: 0.2950745579995671
  batch 197 loss: 0.29523211694913465
  batch 198 loss: 0.2954731886886587
  batch 199 loss: 0.2954577030099217
  batch 200 loss: 0.29538938604295256
  batch 201 loss: 0.2954802731821193
  batch 202 loss: 0.29552191901620073
  batch 203 loss: 0.2954548445388014
  batch 204 loss: 0.2956499364592281
  batch 205 loss: 0.2956699277569608
  batch 206 loss: 0.2956683056302441
  batch 207 loss: 0.2956975193847205
  batch 208 loss: 0.2956374092027545
  batch 209 loss: 0.2957236610673831
  batch 210 loss: 0.2957504683307239
  batch 211 loss: 0.29573045925223995
  batch 212 loss: 0.2958881913350438
  batch 213 loss: 0.2956995926972286
  batch 214 loss: 0.2956552294509433
  batch 215 loss: 0.2957288986721704
  batch 216 loss: 0.29584175658722717
  batch 217 loss: 0.2956186681985855
  batch 218 loss: 0.29559718120262163
  batch 219 loss: 0.2953781982262929
  batch 220 loss: 0.29540491077032954
  batch 221 loss: 0.2954384155942304
  batch 222 loss: 0.29551638931304486
  batch 223 loss: 0.29536188838193234
  batch 224 loss: 0.29542109476668493
  batch 225 loss: 0.29566466609636943
  batch 226 loss: 0.2959028747493187
  batch 227 loss: 0.29573675622498935
  batch 228 loss: 0.29580066287726686
  batch 229 loss: 0.2959157851845937
  batch 230 loss: 0.295953536292781
  batch 231 loss: 0.29599577046576
  batch 232 loss: 0.2958902242625582
  batch 233 loss: 0.2958536364234057
  batch 234 loss: 0.29579490130273706
  batch 235 loss: 0.29565575427197394
  batch 236 loss: 0.29561564932435247
  batch 237 loss: 0.2956429679685504
  batch 238 loss: 0.29555536856671344
  batch 239 loss: 0.2955662052002911
  batch 240 loss: 0.29544969983398917
  batch 241 loss: 0.2954133128229513
  batch 242 loss: 0.2953231635172505
  batch 243 loss: 0.29525911599520305
  batch 244 loss: 0.2952190719422747
  batch 245 loss: 0.2952636586160076
  batch 246 loss: 0.29526589665471054
  batch 247 loss: 0.2953087537877473
  batch 248 loss: 0.29542083761865096
  batch 249 loss: 0.295278156378183
  batch 250 loss: 0.29518652641773224
  batch 251 loss: 0.29502364779373563
  batch 252 loss: 0.29485895508338533
  batch 253 loss: 0.294784798216914
  batch 254 loss: 0.29490152611507203
  batch 255 loss: 0.29476869491969837
  batch 256 loss: 0.2947469465434551
  batch 257 loss: 0.29478599334041433
  batch 258 loss: 0.2946295506039331
  batch 259 loss: 0.2944325005570894
  batch 260 loss: 0.29437797373304
  batch 261 loss: 0.29453322879427696
  batch 262 loss: 0.2944670763748293
  batch 263 loss: 0.2943630985094114
  batch 264 loss: 0.29453790622452897
  batch 265 loss: 0.29459065035829
  batch 266 loss: 0.294534092839051
  batch 267 loss: 0.2943862230701839
  batch 268 loss: 0.29431075752893493
  batch 269 loss: 0.29430573729999004
  batch 270 loss: 0.2943767383694649
  batch 271 loss: 0.2945018711455194
  batch 272 loss: 0.2944144741905963
  batch 273 loss: 0.29437325447251944
  batch 274 loss: 0.29423391954959743
  batch 275 loss: 0.29420167993415486
  batch 276 loss: 0.29426937156181404
  batch 277 loss: 0.294231019564484
  batch 278 loss: 0.2941364109408941
  batch 279 loss: 0.2940993202736728
  batch 280 loss: 0.2940318447670766
  batch 281 loss: 0.2940231050991079
  batch 282 loss: 0.29402786667676684
  batch 283 loss: 0.29401345582606514
  batch 284 loss: 0.2939831896252196
  batch 285 loss: 0.29391879517781105
  batch 286 loss: 0.29399295719145063
  batch 287 loss: 0.2940031832622734
  batch 288 loss: 0.2940167417853243
  batch 289 loss: 0.2939598217451861
  batch 290 loss: 0.29396963021878536
  batch 291 loss: 0.293900049275549
  batch 292 loss: 0.29384734772451937
  batch 293 loss: 0.2939349805027145
  batch 294 loss: 0.2938580123238823
  batch 295 loss: 0.29383271311299275
  batch 296 loss: 0.2939859028100162
  batch 297 loss: 0.29401457364912387
  batch 298 loss: 0.2938926127432977
  batch 299 loss: 0.2939393980347592
  batch 300 loss: 0.29393157655994095
  batch 301 loss: 0.2939384851443807
  batch 302 loss: 0.2939847603637651
  batch 303 loss: 0.29390590384651727
  batch 304 loss: 0.29397242785872596
  batch 305 loss: 0.29394441437525826
  batch 306 loss: 0.29412510518739426
  batch 307 loss: 0.29404087500579973
  batch 308 loss: 0.29412012319866715
  batch 309 loss: 0.2940325148186637
  batch 310 loss: 0.2940977606561876
  batch 311 loss: 0.29394129091137094
  batch 312 loss: 0.29386549099133563
  batch 313 loss: 0.2937873359114979
  batch 314 loss: 0.2937861505397566
  batch 315 loss: 0.2937490436765883
  batch 316 loss: 0.2936500322215165
  batch 317 loss: 0.2938227135488288
  batch 318 loss: 0.2936563814679782
  batch 319 loss: 0.29346625777808105
  batch 320 loss: 0.2937197271268815
  batch 321 loss: 0.29363100546357046
  batch 322 loss: 0.2934818968454503
  batch 323 loss: 0.29348377989541635
  batch 324 loss: 0.29349321798409944
  batch 325 loss: 0.2935342826293065
  batch 326 loss: 0.29370952566708525
  batch 327 loss: 0.2936800674560967
  batch 328 loss: 0.293703514628294
  batch 329 loss: 0.29368608483427566
  batch 330 loss: 0.2936902676567887
  batch 331 loss: 0.2936326313054814
  batch 332 loss: 0.2937491935599281
  batch 333 loss: 0.2936935908980556
  batch 334 loss: 0.2938953257071044
  batch 335 loss: 0.2938991745016468
  batch 336 loss: 0.29391088035135043
  batch 337 loss: 0.29389706041763375
  batch 338 loss: 0.29385457112944335
  batch 339 loss: 0.2937925566086727
  batch 340 loss: 0.2937255503500209
  batch 341 loss: 0.29369818884606236
  batch 342 loss: 0.2937355381354951
  batch 343 loss: 0.29384957353853275
  batch 344 loss: 0.29386001998601957
  batch 345 loss: 0.2938870053360428
  batch 346 loss: 0.2938194694374338
  batch 347 loss: 0.2936428067320018
  batch 348 loss: 0.2936096118613221
  batch 349 loss: 0.2937291550602134
  batch 350 loss: 0.2937366316148213
  batch 351 loss: 0.29374097444732644
  batch 352 loss: 0.2937157794155858
  batch 353 loss: 0.29371920979394456
  batch 354 loss: 0.293754513485957
  batch 355 loss: 0.29368726434841963
  batch 356 loss: 0.2936811193488957
  batch 357 loss: 0.29348756777806106
  batch 358 loss: 0.29334594850433604
  batch 359 loss: 0.2933720036469462
  batch 360 loss: 0.2932929436365763
  batch 361 loss: 0.29326344634357254
  batch 362 loss: 0.2931728247779509
  batch 363 loss: 0.29320844572766097
  batch 364 loss: 0.29313377061715495
  batch 365 loss: 0.29313671058171414
  batch 366 loss: 0.293165900749587
  batch 367 loss: 0.29330671873014696
  batch 368 loss: 0.293192516769404
  batch 369 loss: 0.2932043731374146
  batch 370 loss: 0.2934369309528454
  batch 371 loss: 0.2934071567020005
  batch 372 loss: 0.293513194287336
  batch 373 loss: 0.2936570497364525
  batch 374 loss: 0.2936895692093487
  batch 375 loss: 0.2937309114933014
  batch 376 loss: 0.2938104916284693
  batch 377 loss: 0.29387654897704996
  batch 378 loss: 0.2938059611768319
  batch 379 loss: 0.29382968853205677
  batch 380 loss: 0.2937603436802563
  batch 381 loss: 0.293742688700283
  batch 382 loss: 0.2936992717038899
  batch 383 loss: 0.2936135006013178
  batch 384 loss: 0.29358117701485753
  batch 385 loss: 0.2935476364253403
  batch 386 loss: 0.2935080047252882
  batch 387 loss: 0.29352710382574904
  batch 388 loss: 0.29347018982024536
  batch 389 loss: 0.29343961213732134
  batch 390 loss: 0.2934370016440367
  batch 391 loss: 0.293314843455239
  batch 392 loss: 0.2934402764907905
  batch 393 loss: 0.29347282693586274
  batch 394 loss: 0.2935232310579513
  batch 395 loss: 0.29355527411533305
  batch 396 loss: 0.2936701761050658
  batch 397 loss: 0.29367403524648633
  batch 398 loss: 0.2936234676358688
  batch 399 loss: 0.29358011289945524
  batch 400 loss: 0.2936797177791595
  batch 401 loss: 0.293644617769189
  batch 402 loss: 0.2936253665543314
  batch 403 loss: 0.2935208274472144
  batch 404 loss: 0.29347555545887144
  batch 405 loss: 0.2935630648224442
  batch 406 loss: 0.2934725880622864
  batch 407 loss: 0.29346120672378256
  batch 408 loss: 0.2933760998295803
  batch 409 loss: 0.29336527339695134
  batch 410 loss: 0.2933168288411164
  batch 411 loss: 0.2932823886981556
  batch 412 loss: 0.29336117142901835
  batch 413 loss: 0.2933316134800345
  batch 414 loss: 0.29328054081702576
  batch 415 loss: 0.29321641189506253
  batch 416 loss: 0.2932479541318921
  batch 417 loss: 0.29314981118666467
  batch 418 loss: 0.2931213683346242
  batch 419 loss: 0.29312259129521956
  batch 420 loss: 0.2931212088181859
  batch 421 loss: 0.2931707701745339
  batch 422 loss: 0.2931230129647594
  batch 423 loss: 0.2930699069731061
  batch 424 loss: 0.29319590604249035
  batch 425 loss: 0.29308792678748863
  batch 426 loss: 0.2929485417167905
  batch 427 loss: 0.2929431706038794
  batch 428 loss: 0.29295047107979516
  batch 429 loss: 0.2928712604901729
  batch 430 loss: 0.2928596824407578
  batch 431 loss: 0.2928021648367154
  batch 432 loss: 0.2928361661594223
  batch 433 loss: 0.29273405433664695
  batch 434 loss: 0.2926678769118775
  batch 435 loss: 0.29278950797415326
  batch 436 loss: 0.29280348877431056
  batch 437 loss: 0.2927173628755238
  batch 438 loss: 0.2926484074562652
  batch 439 loss: 0.2925522857279875
  batch 440 loss: 0.29263476414436645
  batch 441 loss: 0.2926662307815487
  batch 442 loss: 0.2926812766971092
  batch 443 loss: 0.29258398694862897
  batch 444 loss: 0.29271375595986304
  batch 445 loss: 0.2927049214920301
  batch 446 loss: 0.29273859815747216
  batch 447 loss: 0.29275087195488164
  batch 448 loss: 0.2927839905023575
  batch 449 loss: 0.2927715287840446
  batch 450 loss: 0.2927265644073486
  batch 451 loss: 0.2926880607583835
  batch 452 loss: 0.2926161311632764
  batch 453 loss: 0.29257369765119573
  batch 454 loss: 0.2925480654717542
  batch 455 loss: 0.2924843608023046
  batch 456 loss: 0.29241123259590385
  batch 457 loss: 0.29242842691732446
  batch 458 loss: 0.2923456323719441
  batch 459 loss: 0.29229633071843314
  batch 460 loss: 0.2922778314870337
  batch 461 loss: 0.29230015817019533
  batch 462 loss: 0.29231166865402486
  batch 463 loss: 0.29230736772133514
  batch 464 loss: 0.29222687173249395
  batch 465 loss: 0.2922284685796307
  batch 466 loss: 0.2922278936200899
  batch 467 loss: 0.292275611159633
  batch 468 loss: 0.2922050710289906
  batch 469 loss: 0.2921765705924044
  batch 470 loss: 0.29232106836552313
  batch 471 loss: 0.2922993576956909
  batch 472 loss: 0.2923014130506475
LOSS train 0.2923014130506475 valid 0.1860913634300232
LOSS train 0.2923014130506475 valid 0.1808251515030861
LOSS train 0.2923014130506475 valid 0.1983045885960261
LOSS train 0.2923014130506475 valid 0.19016816467046738
LOSS train 0.2923014130506475 valid 0.19567238986492158
LOSS train 0.2923014130506475 valid 0.1977463960647583
LOSS train 0.2923014130506475 valid 0.19231748155185155
LOSS train 0.2923014130506475 valid 0.1920722909271717
LOSS train 0.2923014130506475 valid 0.19285353852642906
LOSS train 0.2923014130506475 valid 0.19112387597560881
LOSS train 0.2923014130506475 valid 0.1897613826123151
LOSS train 0.2923014130506475 valid 0.19131031135718027
LOSS train 0.2923014130506475 valid 0.19077937190349287
LOSS train 0.2923014130506475 valid 0.18854563470397676
LOSS train 0.2923014130506475 valid 0.1875032881895701
LOSS train 0.2923014130506475 valid 0.1899602860212326
LOSS train 0.2923014130506475 valid 0.18937119052690618
LOSS train 0.2923014130506475 valid 0.18907559000783497
LOSS train 0.2923014130506475 valid 0.18969769462158806
LOSS train 0.2923014130506475 valid 0.18938750475645066
LOSS train 0.2923014130506475 valid 0.1903227972132819
LOSS train 0.2923014130506475 valid 0.18914529071612793
LOSS train 0.2923014130506475 valid 0.18822082229282544
LOSS train 0.2923014130506475 valid 0.1880624983459711
LOSS train 0.2923014130506475 valid 0.18797658681869506
LOSS train 0.2923014130506475 valid 0.18754183501005173
LOSS train 0.2923014130506475 valid 0.1879450160044211
LOSS train 0.2923014130506475 valid 0.18804099410772324
LOSS train 0.2923014130506475 valid 0.18709475264467043
LOSS train 0.2923014130506475 valid 0.18627285361289977
LOSS train 0.2923014130506475 valid 0.18609674419126204
LOSS train 0.2923014130506475 valid 0.1865065163001418
LOSS train 0.2923014130506475 valid 0.1856395656412298
LOSS train 0.2923014130506475 valid 0.1851885274929159
LOSS train 0.2923014130506475 valid 0.1856147676706314
LOSS train 0.2923014130506475 valid 0.18622263189819124
LOSS train 0.2923014130506475 valid 0.1853983825122988
LOSS train 0.2923014130506475 valid 0.1851175694089187
LOSS train 0.2923014130506475 valid 0.1847800856981522
LOSS train 0.2923014130506475 valid 0.18537517562508582
LOSS train 0.2923014130506475 valid 0.1850375838396026
LOSS train 0.2923014130506475 valid 0.18629362256754012
LOSS train 0.2923014130506475 valid 0.18660446446995402
LOSS train 0.2923014130506475 valid 0.18572797558524393
LOSS train 0.2923014130506475 valid 0.18553574548827279
LOSS train 0.2923014130506475 valid 0.18503779723592426
LOSS train 0.2923014130506475 valid 0.18478670779694903
LOSS train 0.2923014130506475 valid 0.186377115547657
LOSS train 0.2923014130506475 valid 0.18603348397478767
LOSS train 0.2923014130506475 valid 0.18665752619504927
LOSS train 0.2923014130506475 valid 0.1864333377749312
LOSS train 0.2923014130506475 valid 0.18630143312307504
LOSS train 0.2923014130506475 valid 0.1871519442999138
LOSS train 0.2923014130506475 valid 0.187048163678911
LOSS train 0.2923014130506475 valid 0.18703309135003524
LOSS train 0.2923014130506475 valid 0.18686595958258426
LOSS train 0.2923014130506475 valid 0.1863239310812532
LOSS train 0.2923014130506475 valid 0.18696592953698388
LOSS train 0.2923014130506475 valid 0.18687770982920113
LOSS train 0.2923014130506475 valid 0.18687253048022587
LOSS train 0.2923014130506475 valid 0.18690350089893967
LOSS train 0.2923014130506475 valid 0.1865912423499169
LOSS train 0.2923014130506475 valid 0.18640781418671684
LOSS train 0.2923014130506475 valid 0.18624336901120842
LOSS train 0.2923014130506475 valid 0.18549046631042773
LOSS train 0.2923014130506475 valid 0.1854212602431124
LOSS train 0.2923014130506475 valid 0.18602002712328042
LOSS train 0.2923014130506475 valid 0.18553917898851283
LOSS train 0.2923014130506475 valid 0.18596208657043567
LOSS train 0.2923014130506475 valid 0.1863482734986714
LOSS train 0.2923014130506475 valid 0.18657778245462497
LOSS train 0.2923014130506475 valid 0.18686849251389503
LOSS train 0.2923014130506475 valid 0.18728604443269234
LOSS train 0.2923014130506475 valid 0.1873561455994039
LOSS train 0.2923014130506475 valid 0.18703456302483876
LOSS train 0.2923014130506475 valid 0.18716357158202873
LOSS train 0.2923014130506475 valid 0.18730084752881682
LOSS train 0.2923014130506475 valid 0.18712824907822487
LOSS train 0.2923014130506475 valid 0.18714307493801358
LOSS train 0.2923014130506475 valid 0.18691308721899985
LOSS train 0.2923014130506475 valid 0.1869380286446324
LOSS train 0.2923014130506475 valid 0.18700203844686833
LOSS train 0.2923014130506475 valid 0.18690699769789915
LOSS train 0.2923014130506475 valid 0.18685432700883775
LOSS train 0.2923014130506475 valid 0.1871146179297391
LOSS train 0.2923014130506475 valid 0.1871855842859246
LOSS train 0.2923014130506475 valid 0.1866590171709828
LOSS train 0.2923014130506475 valid 0.1867536289448088
LOSS train 0.2923014130506475 valid 0.18702468326252497
LOSS train 0.2923014130506475 valid 0.1870528679754999
LOSS train 0.2923014130506475 valid 0.18715080667983044
LOSS train 0.2923014130506475 valid 0.18695068310784257
LOSS train 0.2923014130506475 valid 0.18697913471729524
LOSS train 0.2923014130506475 valid 0.18711436492331485
LOSS train 0.2923014130506475 valid 0.18723508652887846
LOSS train 0.2923014130506475 valid 0.18753301926578084
LOSS train 0.2923014130506475 valid 0.18760380594386267
LOSS train 0.2923014130506475 valid 0.18785342634940633
LOSS train 0.2923014130506475 valid 0.1879561525402647
LOSS train 0.2923014130506475 valid 0.187907804697752
LOSS train 0.2923014130506475 valid 0.18796303573221262
LOSS train 0.2923014130506475 valid 0.18828060711715736
LOSS train 0.2923014130506475 valid 0.1880941678887432
LOSS train 0.2923014130506475 valid 0.18786604745456806
LOSS train 0.2923014130506475 valid 0.1881605767068409
LOSS train 0.2923014130506475 valid 0.18818309144029077
LOSS train 0.2923014130506475 valid 0.1879746207845545
LOSS train 0.2923014130506475 valid 0.18793696040908495
LOSS train 0.2923014130506475 valid 0.18740391225442973
LOSS train 0.2923014130506475 valid 0.18719381459734658
LOSS train 0.2923014130506475 valid 0.18728626418758082
LOSS train 0.2923014130506475 valid 0.18730559519359044
LOSS train 0.2923014130506475 valid 0.18725251953686234
LOSS train 0.2923014130506475 valid 0.1871724434589085
LOSS train 0.2923014130506475 valid 0.1875779347575229
LOSS train 0.2923014130506475 valid 0.18739260270677763
LOSS train 0.2923014130506475 valid 0.18763087460627922
LOSS train 0.2923014130506475 valid 0.18782359854144565
LOSS train 0.2923014130506475 valid 0.1875602211020574
LOSS train 0.2923014130506475 valid 0.18738694327572983
LOSS train 0.2923014130506475 valid 0.18741136345981566
LOSS train 0.2923014130506475 valid 0.18749412185833103
LOSS train 0.2923014130506475 valid 0.1874849206306101
LOSS train 0.2923014130506475 valid 0.1874556826247323
LOSS train 0.2923014130506475 valid 0.1875045782327652
LOSS train 0.2923014130506475 valid 0.18757453241518565
LOSS train 0.2923014130506475 valid 0.1874754643581045
LOSS train 0.2923014130506475 valid 0.18742732715327293
LOSS train 0.2923014130506475 valid 0.18719380943812142
LOSS train 0.2923014130506475 valid 0.18692672355816914
LOSS train 0.2923014130506475 valid 0.18689953905480508
LOSS train 0.2923014130506475 valid 0.1868122610630411
LOSS train 0.2923014130506475 valid 0.18663249091994494
LOSS train 0.2923014130506475 valid 0.1868005023518605
LOSS train 0.2923014130506475 valid 0.18682043530322887
LOSS train 0.2923014130506475 valid 0.1869686405886622
LOSS train 0.2923014130506475 valid 0.18698157979189045
LOSS train 0.2923014130506475 valid 0.18687623469293982
LOSS train 0.2923014130506475 valid 0.18669361765864942
LOSS train 0.2923014130506475 valid 0.18681266627141407
LOSS train 0.2923014130506475 valid 0.18691176958117925
LOSS train 0.2923014130506475 valid 0.18713880424768153
LOSS train 0.2923014130506475 valid 0.18720719697592142
LOSS train 0.2923014130506475 valid 0.18730684287018246
LOSS train 0.2923014130506475 valid 0.18720093040630736
LOSS train 0.2923014130506475 valid 0.18719239696247936
LOSS train 0.2923014130506475 valid 0.187192509064869
LOSS train 0.2923014130506475 valid 0.18701795418117498
LOSS train 0.2923014130506475 valid 0.18721765169761326
LOSS train 0.2923014130506475 valid 0.18717104683319727
LOSS train 0.2923014130506475 valid 0.18699524003938334
LOSS train 0.2923014130506475 valid 0.18697744490284668
LOSS train 0.2923014130506475 valid 0.1867362630912681
LOSS train 0.2923014130506475 valid 0.18676973859985152
LOSS train 0.2923014130506475 valid 0.1868152843367669
LOSS train 0.2923014130506475 valid 0.1870621072176175
LOSS train 0.2923014130506475 valid 0.18700743300519931
LOSS train 0.2923014130506475 valid 0.1870307899728606
LOSS train 0.2923014130506475 valid 0.1871406226975363
LOSS train 0.2923014130506475 valid 0.18715705620124937
LOSS train 0.2923014130506475 valid 0.18724350927409178
LOSS train 0.2923014130506475 valid 0.18728693298719548
LOSS train 0.2923014130506475 valid 0.187085179463486
LOSS train 0.2923014130506475 valid 0.1871647179671904
LOSS train 0.2923014130506475 valid 0.18726921207977063
LOSS train 0.2923014130506475 valid 0.18711081048451275
LOSS train 0.2923014130506475 valid 0.1871587943709539
LOSS train 0.2923014130506475 valid 0.18697260107312882
LOSS train 0.2923014130506475 valid 0.18676573681760822
LOSS train 0.2923014130506475 valid 0.18678519462837892
LOSS train 0.2923014130506475 valid 0.18661949639780478
LOSS train 0.2923014130506475 valid 0.1865790797354177
LOSS train 0.2923014130506475 valid 0.1865861033428611
LOSS train 0.2923014130506475 valid 0.186712602859941
LOSS train 0.2923014130506475 valid 0.18661028027534485
LOSS train 0.2923014130506475 valid 0.18651392505588857
LOSS train 0.2923014130506475 valid 0.1864792156522557
LOSS train 0.2923014130506475 valid 0.18663616645871922
LOSS train 0.2923014130506475 valid 0.18673742450149367
LOSS train 0.2923014130506475 valid 0.18668826611505615
LOSS train 0.2923014130506475 valid 0.1866001433265802
LOSS train 0.2923014130506475 valid 0.1865240963441985
LOSS train 0.2923014130506475 valid 0.18647774117566196
LOSS train 0.2923014130506475 valid 0.1864762758107289
LOSS train 0.2923014130506475 valid 0.1865091225585422
LOSS train 0.2923014130506475 valid 0.186672683204374
LOSS train 0.2923014130506475 valid 0.1865232480879136
LOSS train 0.2923014130506475 valid 0.18652795167996528
LOSS train 0.2923014130506475 valid 0.18639307819977008
LOSS train 0.2923014130506475 valid 0.18636064678430558
LOSS train 0.2923014130506475 valid 0.18651824409424947
LOSS train 0.2923014130506475 valid 0.18654429578843215
LOSS train 0.2923014130506475 valid 0.18641951355909436
LOSS train 0.2923014130506475 valid 0.1864213867439437
LOSS train 0.2923014130506475 valid 0.18633389258996033
LOSS train 0.2923014130506475 valid 0.18656334075696615
LOSS train 0.2923014130506475 valid 0.1865309414827279
LOSS train 0.2923014130506475 valid 0.18664052585760751
LOSS train 0.2923014130506475 valid 0.18657684026651047
LOSS train 0.2923014130506475 valid 0.18654527217149736
LOSS train 0.2923014130506475 valid 0.18653150851750255
LOSS train 0.2923014130506475 valid 0.1866047852995372
LOSS train 0.2923014130506475 valid 0.18678232719158305
LOSS train 0.2923014130506475 valid 0.18668112187993294
LOSS train 0.2923014130506475 valid 0.18663109228378388
LOSS train 0.2923014130506475 valid 0.186656246052205
LOSS train 0.2923014130506475 valid 0.186613861201466
LOSS train 0.2923014130506475 valid 0.18645223136991262
LOSS train 0.2923014130506475 valid 0.18646196842763982
LOSS train 0.2923014130506475 valid 0.18651574878465563
LOSS train 0.2923014130506475 valid 0.1867476933375354
LOSS train 0.2923014130506475 valid 0.1866498828496573
LOSS train 0.2923014130506475 valid 0.18663520145584162
LOSS train 0.2923014130506475 valid 0.18661808647284997
LOSS train 0.2923014130506475 valid 0.18635465329469636
LOSS train 0.2923014130506475 valid 0.18615131666538892
LOSS train 0.2923014130506475 valid 0.18596114499777694
LOSS train 0.2923014130506475 valid 0.18592894364387616
LOSS train 0.2923014130506475 valid 0.1859606111158519
LOSS train 0.2923014130506475 valid 0.1859314199198376
LOSS train 0.2923014130506475 valid 0.18587738268785348
LOSS train 0.2923014130506475 valid 0.1859319761126965
LOSS train 0.2923014130506475 valid 0.18587397199307856
LOSS train 0.2923014130506475 valid 0.18570891062596015
LOSS train 0.2923014130506475 valid 0.18559454520543417
LOSS train 0.2923014130506475 valid 0.18548944869400125
LOSS train 0.2923014130506475 valid 0.18546616847294542
LOSS train 0.2923014130506475 valid 0.18539831146859287
LOSS train 0.2923014130506475 valid 0.1853634600556053
LOSS train 0.2923014130506475 valid 0.18542442749375881
LOSS train 0.2923014130506475 valid 0.18545975107135196
LOSS train 0.2923014130506475 valid 0.1855515851799784
LOSS train 0.2923014130506475 valid 0.1857597326516082
LOSS train 0.2923014130506475 valid 0.18570280272481787
LOSS train 0.2923014130506475 valid 0.18567857602809337
LOSS train 0.2923014130506475 valid 0.18551900256740844
LOSS train 0.2923014130506475 valid 0.185582081916966
LOSS train 0.2923014130506475 valid 0.18563490066708638
LOSS train 0.2923014130506475 valid 0.1855828445326833
LOSS train 0.2923014130506475 valid 0.1853089368591706
LOSS train 0.2923014130506475 valid 0.18519733190783821
LOSS train 0.2923014130506475 valid 0.18528829527295326
LOSS train 0.2923014130506475 valid 0.18530415617880017
LOSS train 0.2923014130506475 valid 0.18528770154616872
LOSS train 0.2923014130506475 valid 0.18533102620621117
LOSS train 0.2923014130506475 valid 0.18536130696292816
LOSS train 0.2923014130506475 valid 0.18540601172910529
LOSS train 0.2923014130506475 valid 0.1854315257120517
LOSS train 0.2923014130506475 valid 0.18532094634680385
LOSS train 0.2923014130506475 valid 0.18546444892883301
LOSS train 0.2923014130506475 valid 0.1855811455335275
LOSS train 0.2923014130506475 valid 0.18560305586646472
LOSS train 0.2923014130506475 valid 0.18549468194543137
LOSS train 0.2923014130506475 valid 0.18559829785129217
LOSS train 0.2923014130506475 valid 0.18551606436570486
LOSS train 0.2923014130506475 valid 0.18548208760330454
LOSS train 0.2923014130506475 valid 0.18549078221682908
LOSS train 0.2923014130506475 valid 0.18551499321479206
LOSS train 0.2923014130506475 valid 0.1854569495414675
LOSS train 0.2923014130506475 valid 0.18530408734312423
LOSS train 0.2923014130506475 valid 0.1853257317195907
LOSS train 0.2923014130506475 valid 0.1855172595677485
LOSS train 0.2923014130506475 valid 0.1854067648771598
LOSS train 0.2923014130506475 valid 0.18546103522407287
LOSS train 0.2923014130506475 valid 0.18541570699439858
LOSS train 0.2923014130506475 valid 0.18543722357292822
LOSS train 0.2923014130506475 valid 0.18542720595102632
LOSS train 0.2923014130506475 valid 0.18549275859745581
LOSS train 0.2923014130506475 valid 0.1856141811631426
LOSS train 0.2923014130506475 valid 0.18551117656407534
LOSS train 0.2923014130506475 valid 0.185459483043734
LOSS train 0.2923014130506475 valid 0.18533084004679146
LOSS train 0.2923014130506475 valid 0.1852609463882097
LOSS train 0.2923014130506475 valid 0.1852273809975081
LOSS train 0.2923014130506475 valid 0.18531852667981927
LOSS train 0.2923014130506475 valid 0.18534890486710315
LOSS train 0.2923014130506475 valid 0.18536165507261504
LOSS train 0.2923014130506475 valid 0.18544107102018467
LOSS train 0.2923014130506475 valid 0.1854433689172977
LOSS train 0.2923014130506475 valid 0.1853359860501119
LOSS train 0.2923014130506475 valid 0.1852450970547055
LOSS train 0.2923014130506475 valid 0.1852259894317769
LOSS train 0.2923014130506475 valid 0.1853265490106475
LOSS train 0.2923014130506475 valid 0.18520730907018756
LOSS train 0.2923014130506475 valid 0.18525877511292174
LOSS train 0.2923014130506475 valid 0.18523964327532094
LOSS train 0.2923014130506475 valid 0.18524831045380039
LOSS train 0.2923014130506475 valid 0.18522591940644714
LOSS train 0.2923014130506475 valid 0.18522835056262033
LOSS train 0.2923014130506475 valid 0.18516701650002906
LOSS train 0.2923014130506475 valid 0.18517157717054242
LOSS train 0.2923014130506475 valid 0.18515907028970652
LOSS train 0.2923014130506475 valid 0.18511607027501376
LOSS train 0.2923014130506475 valid 0.1850595583214241
LOSS train 0.2923014130506475 valid 0.18505705256583327
LOSS train 0.2923014130506475 valid 0.1850758267918954
LOSS train 0.2923014130506475 valid 0.18509715273725463
LOSS train 0.2923014130506475 valid 0.18515105665530135
LOSS train 0.2923014130506475 valid 0.18511516114939813
LOSS train 0.2923014130506475 valid 0.1850568317870299
LOSS train 0.2923014130506475 valid 0.18504107943999015
LOSS train 0.2923014130506475 valid 0.1850489992278301
LOSS train 0.2923014130506475 valid 0.18497350015262565
LOSS train 0.2923014130506475 valid 0.18499543125692167
LOSS train 0.2923014130506475 valid 0.18497476963723292
LOSS train 0.2923014130506475 valid 0.18515772406571832
LOSS train 0.2923014130506475 valid 0.1851310125197184
LOSS train 0.2923014130506475 valid 0.18523942688842873
LOSS train 0.2923014130506475 valid 0.18530152366770897
LOSS train 0.2923014130506475 valid 0.1853309093944488
LOSS train 0.2923014130506475 valid 0.18535437164199314
LOSS train 0.2923014130506475 valid 0.18534806494911513
LOSS train 0.2923014130506475 valid 0.18547365992975692
LOSS train 0.2923014130506475 valid 0.18547855512161923
LOSS train 0.2923014130506475 valid 0.1855195433847488
LOSS train 0.2923014130506475 valid 0.18560454004173038
LOSS train 0.2923014130506475 valid 0.18558082727227676
LOSS train 0.2923014130506475 valid 0.18559415563472412
LOSS train 0.2923014130506475 valid 0.18558413640458757
LOSS train 0.2923014130506475 valid 0.18551853289827705
LOSS train 0.2923014130506475 valid 0.1855494733261542
LOSS train 0.2923014130506475 valid 0.18554666189488417
LOSS train 0.2923014130506475 valid 0.1854400588631999
LOSS train 0.2923014130506475 valid 0.18542285879820952
LOSS train 0.2923014130506475 valid 0.18528148458554194
LOSS train 0.2923014130506475 valid 0.1853816200710513
LOSS train 0.2923014130506475 valid 0.18534338105162348
LOSS train 0.2923014130506475 valid 0.18537628078242627
LOSS train 0.2923014130506475 valid 0.18529309694709023
LOSS train 0.2923014130506475 valid 0.18530472991141406
LOSS train 0.2923014130506475 valid 0.1852302815978981
LOSS train 0.2923014130506475 valid 0.18518002506300626
LOSS train 0.2923014130506475 valid 0.1852824501357637
LOSS train 0.2923014130506475 valid 0.18525630369514762
LOSS train 0.2923014130506475 valid 0.1852278261042353
LOSS train 0.2923014130506475 valid 0.1851887951738068
LOSS train 0.2923014130506475 valid 0.18534586072321815
LOSS train 0.2923014130506475 valid 0.18528604353144323
LOSS train 0.2923014130506475 valid 0.18525267495694062
LOSS train 0.2923014130506475 valid 0.1853223661727765
LOSS train 0.2923014130506475 valid 0.1853308803524789
LOSS train 0.2923014130506475 valid 0.18529621537840157
LOSS train 0.2923014130506475 valid 0.185201383106215
LOSS train 0.2923014130506475 valid 0.18521744519645392
LOSS train 0.2923014130506475 valid 0.18528317735678906
LOSS train 0.2923014130506475 valid 0.18524090204969307
LOSS train 0.2923014130506475 valid 0.18517628020099672
LOSS train 0.2923014130506475 valid 0.1852199942216106
LOSS train 0.2923014130506475 valid 0.18515407599316627
LOSS train 0.2923014130506475 valid 0.18517773142882757
LOSS train 0.2923014130506475 valid 0.18520908770907638
LOSS train 0.2923014130506475 valid 0.18524428457021713
LOSS train 0.2923014130506475 valid 0.1853034154382711
LOSS train 0.2923014130506475 valid 0.18527249128973416
LOSS train 0.2923014130506475 valid 0.18516518725475795
LOSS train 0.2923014130506475 valid 0.1851563048078103
LOSS train 0.2923014130506475 valid 0.18523904455810034
LOSS train 0.2923014130506475 valid 0.18527535952669283
LOSS train 0.2923014130506475 valid 0.1852954022980666
LOSS train 0.2923014130506475 valid 0.18539387091166443
LOSS train 0.2923014130506475 valid 0.1853448308158119
LOSS train 0.2923014130506475 valid 0.18542818177635498
LOSS train 0.2923014130506475 valid 0.18549228301554016
LOSS train 0.2923014130506475 valid 0.18543835111208015
LOSS train 0.2923014130506475 valid 0.18550903923707465
LOSS train 0.2923014130506475 valid 0.1855183040998021
LOSS train 0.2923014130506475 valid 0.18551967831008764
LOSS train 0.2923014130506475 valid 0.1854356745986835
LOSS train 0.2923014130506475 valid 0.18549973822544583
EPOCH 6:
  batch 1 loss: 0.32960882782936096
  batch 2 loss: 0.3336222618818283
  batch 3 loss: 0.3198000490665436
  batch 4 loss: 0.32880906760692596
  batch 5 loss: 0.328793877363205
  batch 6 loss: 0.3214259097973506
  batch 7 loss: 0.31901552421706064
  batch 8 loss: 0.3173844553530216
  batch 9 loss: 0.31369303663571674
  batch 10 loss: 0.3087819069623947
  batch 11 loss: 0.3077563562176444
  batch 12 loss: 0.3052259013056755
  batch 13 loss: 0.30338215827941895
  batch 14 loss: 0.3023048937320709
  batch 15 loss: 0.3029879113038381
  batch 16 loss: 0.30273150838911533
  batch 17 loss: 0.301825142958585
  batch 18 loss: 0.30018264220820534
  batch 19 loss: 0.29980334953257914
  batch 20 loss: 0.29668648466467856
  batch 21 loss: 0.29886215854258763
  batch 22 loss: 0.29956025088375265
  batch 23 loss: 0.29627423247565393
  batch 24 loss: 0.29614302950600785
  batch 25 loss: 0.29672677695751193
  batch 26 loss: 0.29652395328650105
  batch 27 loss: 0.2971139522614302
  batch 28 loss: 0.29721030965447426
  batch 29 loss: 0.2966887688842313
  batch 30 loss: 0.29787524590889614
  batch 31 loss: 0.2981478634380525
  batch 32 loss: 0.29901649011299014
  batch 33 loss: 0.29980109902945434
  batch 34 loss: 0.29956259052543077
  batch 35 loss: 0.3009132900408336
  batch 36 loss: 0.3002318710916572
  batch 37 loss: 0.29995398303946935
  batch 38 loss: 0.3002101869175309
  batch 39 loss: 0.30054411330284214
  batch 40 loss: 0.30017302744090557
  batch 41 loss: 0.30002770300318554
  batch 42 loss: 0.2991319124897321
  batch 43 loss: 0.2985820690559786
  batch 44 loss: 0.29798704555088823
  batch 45 loss: 0.29763432575596704
  batch 46 loss: 0.29599378419959027
  batch 47 loss: 0.2953820482213446
  batch 48 loss: 0.295259893561403
  batch 49 loss: 0.2958037117306067
  batch 50 loss: 0.29539346873760225
  batch 51 loss: 0.29503532309158176
  batch 52 loss: 0.2944363842789943
  batch 53 loss: 0.29342653391496193
  batch 54 loss: 0.29340781216268186
  batch 55 loss: 0.29369871507991446
  batch 56 loss: 0.2940838022955826
  batch 57 loss: 0.29344667049876433
  batch 58 loss: 0.2927652345649127
  batch 59 loss: 0.29281234488648883
  batch 60 loss: 0.293174746632576
  batch 61 loss: 0.2936125422110323
  batch 62 loss: 0.29410943965758046
  batch 63 loss: 0.29388052887386745
  batch 64 loss: 0.29335065651685
  batch 65 loss: 0.292961441553556
  batch 66 loss: 0.29249893761042395
  batch 67 loss: 0.29205277384217104
  batch 68 loss: 0.29204699835356546
  batch 69 loss: 0.29205088123031286
  batch 70 loss: 0.29110991933516095
  batch 71 loss: 0.2909859378992672
  batch 72 loss: 0.2906466939797004
  batch 73 loss: 0.29002294250547067
  batch 74 loss: 0.28941238631267807
  batch 75 loss: 0.28911666572093964
  batch 76 loss: 0.2889924698362225
  batch 77 loss: 0.28866037513528553
  batch 78 loss: 0.28879545877377194
  batch 79 loss: 0.2888644648880898
  batch 80 loss: 0.28858116213232277
  batch 81 loss: 0.2881426555506977
  batch 82 loss: 0.288289136457734
  batch 83 loss: 0.28800852560853385
  batch 84 loss: 0.287700390709298
  batch 85 loss: 0.28788580070523656
  batch 86 loss: 0.28784135243920395
  batch 87 loss: 0.28776582167751485
  batch 88 loss: 0.28771595301275904
  batch 89 loss: 0.2875427230355445
  batch 90 loss: 0.28783313549227185
  batch 91 loss: 0.28765954273742633
  batch 92 loss: 0.2874516904678034
  batch 93 loss: 0.28722828161972824
  batch 94 loss: 0.28723317654208935
  batch 95 loss: 0.28739763545362573
  batch 96 loss: 0.2873370763845742
  batch 97 loss: 0.2877910347636213
  batch 98 loss: 0.28770462241099803
  batch 99 loss: 0.28761695987648433
  batch 100 loss: 0.2875641293823719
  batch 101 loss: 0.28734475625033423
  batch 102 loss: 0.28759247984956293
  batch 103 loss: 0.2874239169567534
  batch 104 loss: 0.28768845891150147
  batch 105 loss: 0.28760881381375447
  batch 106 loss: 0.2878214049170602
  batch 107 loss: 0.2877763196408192
  batch 108 loss: 0.28743111687125983
  batch 109 loss: 0.2875976077186952
  batch 110 loss: 0.2876069593158635
  batch 111 loss: 0.28763183159334166
  batch 112 loss: 0.28739648192588774
  batch 113 loss: 0.28704914147347477
  batch 114 loss: 0.28693459182977676
  batch 115 loss: 0.2865110432324202
  batch 116 loss: 0.28640914027547015
  batch 117 loss: 0.28654815434900105
  batch 118 loss: 0.28666656186520045
  batch 119 loss: 0.2869483927468292
  batch 120 loss: 0.2866454401363929
  batch 121 loss: 0.2863490417722828
  batch 122 loss: 0.2863439188384619
  batch 123 loss: 0.28616384895351843
  batch 124 loss: 0.2860578386293304
  batch 125 loss: 0.28580489575862883
  batch 126 loss: 0.2859235679109891
  batch 127 loss: 0.28600931554798065
  batch 128 loss: 0.2858251052675769
  batch 129 loss: 0.28588840839012647
  batch 130 loss: 0.2858899819163176
  batch 131 loss: 0.28579273581049824
  batch 132 loss: 0.2857307725106225
  batch 133 loss: 0.2854130800281252
  batch 134 loss: 0.2852222030954574
  batch 135 loss: 0.285075393098372
  batch 136 loss: 0.28526766565354433
  batch 137 loss: 0.2853207219473637
  batch 138 loss: 0.28549600003854086
  batch 139 loss: 0.28561508065933805
  batch 140 loss: 0.2859798568700041
  batch 141 loss: 0.28592366899581667
  batch 142 loss: 0.28589745401076866
  batch 143 loss: 0.28615743682517875
  batch 144 loss: 0.2862494413016571
  batch 145 loss: 0.2863161556679627
  batch 146 loss: 0.28620657437059976
  batch 147 loss: 0.2862700348605915
  batch 148 loss: 0.2864531728665571
  batch 149 loss: 0.286111225617812
  batch 150 loss: 0.28613667527834574
  batch 151 loss: 0.28595668116942147
  batch 152 loss: 0.28581555952367027
  batch 153 loss: 0.28609297361249236
  batch 154 loss: 0.28612191355847694
  batch 155 loss: 0.28612422097113827
  batch 156 loss: 0.2860196257630984
  batch 157 loss: 0.28596192750201865
  batch 158 loss: 0.28607697615140604
  batch 159 loss: 0.2862533707663698
  batch 160 loss: 0.28605538569390776
  batch 161 loss: 0.2858410862662037
  batch 162 loss: 0.28586026547867577
  batch 163 loss: 0.2856219638527537
  batch 164 loss: 0.2858242675116876
  batch 165 loss: 0.28589649046912335
  batch 166 loss: 0.28608482313084316
  batch 167 loss: 0.2859364461577581
  batch 168 loss: 0.28562584945133757
  batch 169 loss: 0.2857967128415079
  batch 170 loss: 0.2858029256848728
  batch 171 loss: 0.28593090577432284
  batch 172 loss: 0.28596290859372114
  batch 173 loss: 0.2860553247391144
  batch 174 loss: 0.2861761452480294
  batch 175 loss: 0.28615740145955765
  batch 176 loss: 0.2863695809448307
  batch 177 loss: 0.2865794038368484
  batch 178 loss: 0.28628052536691173
  batch 179 loss: 0.2864583700728816
  batch 180 loss: 0.2864152698053254
  batch 181 loss: 0.2863220396292144
  batch 182 loss: 0.2862361785802212
  batch 183 loss: 0.28601512996876827
  batch 184 loss: 0.28577622690278554
  batch 185 loss: 0.285669834388269
  batch 186 loss: 0.2856931740878731
  batch 187 loss: 0.2859417445519391
  batch 188 loss: 0.2858573463051877
  batch 189 loss: 0.2858029655993931
  batch 190 loss: 0.2858241037318581
  batch 191 loss: 0.28603730557476664
  batch 192 loss: 0.285918683744967
  batch 193 loss: 0.2859740981475059
  batch 194 loss: 0.2859827756267233
  batch 195 loss: 0.28612844944000243
  batch 196 loss: 0.28608727515960225
  batch 197 loss: 0.28615569901950466
  batch 198 loss: 0.2862578160835035
  batch 199 loss: 0.28619584306400625
  batch 200 loss: 0.28620540142059325
  batch 201 loss: 0.28607715006491435
  batch 202 loss: 0.28605674930138164
  batch 203 loss: 0.28603354008326975
  batch 204 loss: 0.28596677470440957
  batch 205 loss: 0.2858331100242894
  batch 206 loss: 0.28581996305474955
  batch 207 loss: 0.28569930907032914
  batch 208 loss: 0.28558459476782727
  batch 209 loss: 0.2855322586292285
  batch 210 loss: 0.28556652509030844
  batch 211 loss: 0.2854875309207428
  batch 212 loss: 0.2855436377367883
  batch 213 loss: 0.2853522499402364
  batch 214 loss: 0.28535111025672094
  batch 215 loss: 0.28531320829724155
  batch 216 loss: 0.2853953056037426
  batch 217 loss: 0.28513388095363496
  batch 218 loss: 0.285118240555492
  batch 219 loss: 0.2848427145992784
  batch 220 loss: 0.2848545242439617
  batch 221 loss: 0.28486444426877466
  batch 222 loss: 0.2848863891653112
  batch 223 loss: 0.284736129200512
  batch 224 loss: 0.28483791689255405
  batch 225 loss: 0.28505339423815407
  batch 226 loss: 0.2852708570725095
  batch 227 loss: 0.2851646314108424
  batch 228 loss: 0.28521952765029773
  batch 229 loss: 0.28530412821269974
  batch 230 loss: 0.2853993694419446
  batch 231 loss: 0.28550693960416884
  batch 232 loss: 0.28540966158797
  batch 233 loss: 0.285404729996628
  batch 234 loss: 0.28538551493587655
  batch 235 loss: 0.285299587630211
  batch 236 loss: 0.28521550876104224
  batch 237 loss: 0.28529859155039244
  batch 238 loss: 0.28525131239610557
  batch 239 loss: 0.2852852082651529
  batch 240 loss: 0.2852202178289493
  batch 241 loss: 0.28519271592381584
  batch 242 loss: 0.28513199435777903
  batch 243 loss: 0.2850868765709331
  batch 244 loss: 0.2850715754950633
  batch 245 loss: 0.2850859117751219
  batch 246 loss: 0.28508472539545077
  batch 247 loss: 0.28502668205060455
  batch 248 loss: 0.28509858683232336
  batch 249 loss: 0.28495007160916386
  batch 250 loss: 0.2848249230980873
  batch 251 loss: 0.2846698780577496
  batch 252 loss: 0.28446521136968855
  batch 253 loss: 0.28446339241600793
  batch 254 loss: 0.2845895614211015
  batch 255 loss: 0.2843993774231742
  batch 256 loss: 0.2844141831737943
  batch 257 loss: 0.2844289221768249
  batch 258 loss: 0.2842815548990124
  batch 259 loss: 0.28411132793398897
  batch 260 loss: 0.28403412751280344
  batch 261 loss: 0.28419801564280556
  batch 262 loss: 0.2840845294239867
  batch 263 loss: 0.2840015272337221
  batch 264 loss: 0.28421117935442564
  batch 265 loss: 0.28424608150743086
  batch 266 loss: 0.2841536988105093
  batch 267 loss: 0.2840004146433948
  batch 268 loss: 0.2839640224936293
  batch 269 loss: 0.28398498554876744
  batch 270 loss: 0.28400637755791347
  batch 271 loss: 0.28412793664694713
  batch 272 loss: 0.2840470959268072
  batch 273 loss: 0.28396552206177417
  batch 274 loss: 0.28386276150054307
  batch 275 loss: 0.28381161510944364
  batch 276 loss: 0.28385568491142726
  batch 277 loss: 0.283875979510025
  batch 278 loss: 0.2837615625785409
  batch 279 loss: 0.28372091797303983
  batch 280 loss: 0.28362220541707106
  batch 281 loss: 0.28364764790305885
  batch 282 loss: 0.2836221034328143
  batch 283 loss: 0.28360274218207115
  batch 284 loss: 0.2835424195607783
  batch 285 loss: 0.28350971565957656
  batch 286 loss: 0.28355429717502395
  batch 287 loss: 0.28354145202279507
  batch 288 loss: 0.28354613001768786
  batch 289 loss: 0.28351861687061286
  batch 290 loss: 0.2834680499187831
  batch 291 loss: 0.28338519440278975
  batch 292 loss: 0.28329634263295017
  batch 293 loss: 0.28334502336718526
  batch 294 loss: 0.28325786999091
  batch 295 loss: 0.28329909731776026
  batch 296 loss: 0.2834485415369272
  batch 297 loss: 0.28343679422280604
  batch 298 loss: 0.28333622222178734
  batch 299 loss: 0.2834062757201019
  batch 300 loss: 0.28340097670753794
  batch 301 loss: 0.28343653159086096
  batch 302 loss: 0.28348827080813466
  batch 303 loss: 0.2834045610117047
  batch 304 loss: 0.2834919796472317
  batch 305 loss: 0.2834467677307911
  batch 306 loss: 0.28363586996310675
  batch 307 loss: 0.2835575104827601
  batch 308 loss: 0.2836178877427206
  batch 309 loss: 0.2835749117205444
  batch 310 loss: 0.2836800821846531
  batch 311 loss: 0.2835443499003959
  batch 312 loss: 0.2834659818655405
  batch 313 loss: 0.2834418657869577
  batch 314 loss: 0.28346314039199977
  batch 315 loss: 0.28338837122160293
  batch 316 loss: 0.28330495623470864
  batch 317 loss: 0.2834910353651558
  batch 318 loss: 0.2833134905909592
  batch 319 loss: 0.2831509228223544
  batch 320 loss: 0.28334523299708964
  batch 321 loss: 0.28326807772259105
  batch 322 loss: 0.2831206997359021
  batch 323 loss: 0.2831291355215728
  batch 324 loss: 0.28317550468591995
  batch 325 loss: 0.2831984674013578
  batch 326 loss: 0.2833994162594614
  batch 327 loss: 0.28336819012959796
  batch 328 loss: 0.2834079261960053
  batch 329 loss: 0.2834313538117974
  batch 330 loss: 0.28343909256386035
  batch 331 loss: 0.28341158162070906
  batch 332 loss: 0.28350933739937934
  batch 333 loss: 0.2834844216033145
  batch 334 loss: 0.2837316428651353
  batch 335 loss: 0.2837658252289046
  batch 336 loss: 0.28376725484572707
  batch 337 loss: 0.2837488751206271
  batch 338 loss: 0.2837632949182973
  batch 339 loss: 0.2837011170598258
  batch 340 loss: 0.2836217834669001
  batch 341 loss: 0.2836170945524121
  batch 342 loss: 0.28362960538320375
  batch 343 loss: 0.2837413789231993
  batch 344 loss: 0.2837821110043415
  batch 345 loss: 0.28378477260686347
  batch 346 loss: 0.28373795635782917
  batch 347 loss: 0.28355215648752813
  batch 348 loss: 0.2835252642974086
  batch 349 loss: 0.2836377347437905
  batch 350 loss: 0.28360658492360796
  batch 351 loss: 0.2836609124458074
  batch 352 loss: 0.28363321840085765
  batch 353 loss: 0.2836401803635335
  batch 354 loss: 0.2837266802451031
  batch 355 loss: 0.28365529439818693
  batch 356 loss: 0.2836338500963168
  batch 357 loss: 0.2834597373542999
  batch 358 loss: 0.2833344068107658
  batch 359 loss: 0.2833235404119518
  batch 360 loss: 0.2832238751153151
  batch 361 loss: 0.2831845950551971
  batch 362 loss: 0.2830738337988353
  batch 363 loss: 0.2830961698014546
  batch 364 loss: 0.28302880622200915
  batch 365 loss: 0.2829877490866674
  batch 366 loss: 0.28303461696932225
  batch 367 loss: 0.2832276160125836
  batch 368 loss: 0.28311042718427337
  batch 369 loss: 0.28315109882735945
  batch 370 loss: 0.2833952685868418
  batch 371 loss: 0.2833599682365145
  batch 372 loss: 0.2835008504810513
  batch 373 loss: 0.28366632304146844
  batch 374 loss: 0.28370138966941577
  batch 375 loss: 0.2837409561872482
  batch 376 loss: 0.2838616172604738
  batch 377 loss: 0.2839358222500399
  batch 378 loss: 0.2838797140767965
  batch 379 loss: 0.2838931157202079
  batch 380 loss: 0.28382774320872206
  batch 381 loss: 0.2838077935881502
  batch 382 loss: 0.28378111869096756
  batch 383 loss: 0.2836805469606937
  batch 384 loss: 0.28364511047645163
  batch 385 loss: 0.2835966913344024
  batch 386 loss: 0.2835491452887268
  batch 387 loss: 0.2835397977930631
  batch 388 loss: 0.2834998090780273
  batch 389 loss: 0.2835359545400639
  batch 390 loss: 0.2835339849193891
  batch 391 loss: 0.2834138513144935
  batch 392 loss: 0.28350333470319
  batch 393 loss: 0.28356646388086654
  batch 394 loss: 0.28363617872677477
  batch 395 loss: 0.28368058819559555
  batch 396 loss: 0.2837284553156357
  batch 397 loss: 0.28372815360200193
  batch 398 loss: 0.28371919328393647
  batch 399 loss: 0.28367012421738236
  batch 400 loss: 0.28378751199692487
  batch 401 loss: 0.2837654212094899
  batch 402 loss: 0.283764253912577
  batch 403 loss: 0.2836635109064893
  batch 404 loss: 0.2836279878669446
  batch 405 loss: 0.2836701312918722
  batch 406 loss: 0.2836216884880818
  batch 407 loss: 0.28363541888956356
  batch 408 loss: 0.28357261222075014
  batch 409 loss: 0.28363490789618645
  batch 410 loss: 0.2835465007438892
  batch 411 loss: 0.28353732672050924
  batch 412 loss: 0.28364550358462104
  batch 413 loss: 0.2836226715591283
  batch 414 loss: 0.2836188830183324
  batch 415 loss: 0.2835722883781755
  batch 416 loss: 0.28365234550661766
  batch 417 loss: 0.2835834513036467
  batch 418 loss: 0.2835788520757091
  batch 419 loss: 0.28359588582362083
  batch 420 loss: 0.2836190002305167
  batch 421 loss: 0.2836615090698641
  batch 422 loss: 0.28357149893638645
  batch 423 loss: 0.2835779225290808
  batch 424 loss: 0.283717607622439
  batch 425 loss: 0.2836081285336438
  batch 426 loss: 0.2834891133325201
  batch 427 loss: 0.2834662929770539
  batch 428 loss: 0.28345527729698433
  batch 429 loss: 0.2834042416308032
  batch 430 loss: 0.2834504432456438
  batch 431 loss: 0.2833757858265304
  batch 432 loss: 0.2834162200904555
  batch 433 loss: 0.28329929007393656
  batch 434 loss: 0.28317231746725224
  batch 435 loss: 0.28332470672569054
  batch 436 loss: 0.28334401257404496
  batch 437 loss: 0.2832576029974481
  batch 438 loss: 0.283200274013221
  batch 439 loss: 0.283075411390335
  batch 440 loss: 0.28315636854280124
  batch 441 loss: 0.2831872190747942
  batch 442 loss: 0.2832117984197798
  batch 443 loss: 0.2831139225117509
  batch 444 loss: 0.28320690724361053
  batch 445 loss: 0.28318288584103746
  batch 446 loss: 0.2831999811556842
  batch 447 loss: 0.28319154546431513
  batch 448 loss: 0.2832532649128033
  batch 449 loss: 0.28324701223846005
  batch 450 loss: 0.283194760647085
  batch 451 loss: 0.2831471115350723
  batch 452 loss: 0.2830647116411576
  batch 453 loss: 0.2830225045383634
  batch 454 loss: 0.2829636234609566
  batch 455 loss: 0.28288531889627266
  batch 456 loss: 0.28282777733055126
  batch 457 loss: 0.2828355366669807
  batch 458 loss: 0.28275976277985426
  batch 459 loss: 0.28269479337730696
  batch 460 loss: 0.2826819827051266
  batch 461 loss: 0.2826927225075161
  batch 462 loss: 0.28271120815695105
  batch 463 loss: 0.2827233183911758
  batch 464 loss: 0.2826402958418275
  batch 465 loss: 0.28264032528605515
  batch 466 loss: 0.2826675439149525
  batch 467 loss: 0.282706973479953
  batch 468 loss: 0.282656365320978
  batch 469 loss: 0.28264738771834097
  batch 470 loss: 0.2828766948048105
  batch 471 loss: 0.28287423375179305
  batch 472 loss: 0.2828533132639477
LOSS train 0.2828533132639477 valid 0.17452408373355865
LOSS train 0.2828533132639477 valid 0.16605348140001297
LOSS train 0.2828533132639477 valid 0.18115328749020895
LOSS train 0.2828533132639477 valid 0.1742006316781044
LOSS train 0.2828533132639477 valid 0.17921102941036224
LOSS train 0.2828533132639477 valid 0.1810237392783165
LOSS train 0.2828533132639477 valid 0.17493046607289994
LOSS train 0.2828533132639477 valid 0.17540095373988152
LOSS train 0.2828533132639477 valid 0.17607278459601933
LOSS train 0.2828533132639477 valid 0.17415853291749955
LOSS train 0.2828533132639477 valid 0.17311358451843262
LOSS train 0.2828533132639477 valid 0.17444473256667456
LOSS train 0.2828533132639477 valid 0.17426576637304747
LOSS train 0.2828533132639477 valid 0.1720838908638273
LOSS train 0.2828533132639477 valid 0.17122174600760143
LOSS train 0.2828533132639477 valid 0.1737178722396493
LOSS train 0.2828533132639477 valid 0.17312006477047415
LOSS train 0.2828533132639477 valid 0.17250072376595604
LOSS train 0.2828533132639477 valid 0.17322183204324623
LOSS train 0.2828533132639477 valid 0.17305746749043466
LOSS train 0.2828533132639477 valid 0.17408195918514616
LOSS train 0.2828533132639477 valid 0.17280244759537958
LOSS train 0.2828533132639477 valid 0.17182193048622296
LOSS train 0.2828533132639477 valid 0.17184502569337687
LOSS train 0.2828533132639477 valid 0.17171113669872284
LOSS train 0.2828533132639477 valid 0.17129046355302519
LOSS train 0.2828533132639477 valid 0.17169079018963707
LOSS train 0.2828533132639477 valid 0.1715886811060565
LOSS train 0.2828533132639477 valid 0.17062393293298525
LOSS train 0.2828533132639477 valid 0.1697743356227875
LOSS train 0.2828533132639477 valid 0.16944356262683868
LOSS train 0.2828533132639477 valid 0.16978638526052237
LOSS train 0.2828533132639477 valid 0.16916193248647632
LOSS train 0.2828533132639477 valid 0.16860000263242161
LOSS train 0.2828533132639477 valid 0.16915882357529233
LOSS train 0.2828533132639477 valid 0.16976406549414
LOSS train 0.2828533132639477 valid 0.16899531757509387
LOSS train 0.2828533132639477 valid 0.1686256943564666
LOSS train 0.2828533132639477 valid 0.16828713776209417
LOSS train 0.2828533132639477 valid 0.16874356269836427
LOSS train 0.2828533132639477 valid 0.16847610182878447
LOSS train 0.2828533132639477 valid 0.16967971729380743
LOSS train 0.2828533132639477 valid 0.16998299748398538
LOSS train 0.2828533132639477 valid 0.1692864095622843
LOSS train 0.2828533132639477 valid 0.16912761727968853
LOSS train 0.2828533132639477 valid 0.16864794006814127
LOSS train 0.2828533132639477 valid 0.168505608084354
LOSS train 0.2828533132639477 valid 0.17000698453436294
LOSS train 0.2828533132639477 valid 0.16961639754626215
LOSS train 0.2828533132639477 valid 0.1703941747546196
LOSS train 0.2828533132639477 valid 0.17012440164883932
LOSS train 0.2828533132639477 valid 0.1701199346436904
LOSS train 0.2828533132639477 valid 0.1708485981765783
LOSS train 0.2828533132639477 valid 0.17077879856030145
LOSS train 0.2828533132639477 valid 0.17071531414985658
LOSS train 0.2828533132639477 valid 0.1705326453915664
LOSS train 0.2828533132639477 valid 0.16998362645768283
LOSS train 0.2828533132639477 valid 0.17065947579926458
LOSS train 0.2828533132639477 valid 0.17053625972594244
LOSS train 0.2828533132639477 valid 0.17057323281963666
LOSS train 0.2828533132639477 valid 0.17070364952087402
LOSS train 0.2828533132639477 valid 0.17032931624881684
LOSS train 0.2828533132639477 valid 0.17010219253244854
LOSS train 0.2828533132639477 valid 0.17001505056396127
LOSS train 0.2828533132639477 valid 0.16925818507487958
LOSS train 0.2828533132639477 valid 0.1691141945846153
LOSS train 0.2828533132639477 valid 0.16980990478351934
LOSS train 0.2828533132639477 valid 0.1693155303159181
LOSS train 0.2828533132639477 valid 0.1696484408516815
LOSS train 0.2828533132639477 valid 0.17002225943974086
LOSS train 0.2828533132639477 valid 0.17033355168893305
LOSS train 0.2828533132639477 valid 0.17052788328793314
LOSS train 0.2828533132639477 valid 0.1710609252730461
LOSS train 0.2828533132639477 valid 0.1711780263765438
LOSS train 0.2828533132639477 valid 0.17098400910695394
LOSS train 0.2828533132639477 valid 0.1710120514035225
LOSS train 0.2828533132639477 valid 0.17110039816274272
LOSS train 0.2828533132639477 valid 0.17093968849915725
LOSS train 0.2828533132639477 valid 0.17094055270846886
LOSS train 0.2828533132639477 valid 0.1707198303192854
LOSS train 0.2828533132639477 valid 0.17068191792493986
LOSS train 0.2828533132639477 valid 0.17067552176190587
LOSS train 0.2828533132639477 valid 0.17053776764007936
LOSS train 0.2828533132639477 valid 0.17041519284248352
LOSS train 0.2828533132639477 valid 0.17059808198143453
LOSS train 0.2828533132639477 valid 0.17062823457080264
LOSS train 0.2828533132639477 valid 0.1701346536477407
LOSS train 0.2828533132639477 valid 0.17017614011737434
LOSS train 0.2828533132639477 valid 0.1704457298423467
LOSS train 0.2828533132639477 valid 0.17045336994859908
LOSS train 0.2828533132639477 valid 0.17052725841710856
LOSS train 0.2828533132639477 valid 0.17033523774665335
LOSS train 0.2828533132639477 valid 0.1703604196989408
LOSS train 0.2828533132639477 valid 0.17051934070409613
LOSS train 0.2828533132639477 valid 0.17060956123628115
LOSS train 0.2828533132639477 valid 0.1709071098205944
LOSS train 0.2828533132639477 valid 0.17093195912149764
LOSS train 0.2828533132639477 valid 0.17109817038385236
LOSS train 0.2828533132639477 valid 0.17119190984904165
LOSS train 0.2828533132639477 valid 0.17113021343946458
LOSS train 0.2828533132639477 valid 0.1712285163674024
LOSS train 0.2828533132639477 valid 0.17145712001650942
LOSS train 0.2828533132639477 valid 0.17126078107982007
LOSS train 0.2828533132639477 valid 0.17107632054159275
LOSS train 0.2828533132639477 valid 0.17138559037730808
LOSS train 0.2828533132639477 valid 0.17142158985700248
LOSS train 0.2828533132639477 valid 0.17122614091244814
LOSS train 0.2828533132639477 valid 0.17117660782403415
LOSS train 0.2828533132639477 valid 0.17071328934179533
LOSS train 0.2828533132639477 valid 0.17048616436394778
LOSS train 0.2828533132639477 valid 0.17060383751585678
LOSS train 0.2828533132639477 valid 0.17063461669853755
LOSS train 0.2828533132639477 valid 0.17054765598963847
LOSS train 0.2828533132639477 valid 0.1704965521369064
LOSS train 0.2828533132639477 valid 0.1709079763163691
LOSS train 0.2828533132639477 valid 0.1707216203212738
LOSS train 0.2828533132639477 valid 0.17092147928017837
LOSS train 0.2828533132639477 valid 0.1711288388755362
LOSS train 0.2828533132639477 valid 0.17089220881462097
LOSS train 0.2828533132639477 valid 0.1707055812080701
LOSS train 0.2828533132639477 valid 0.17068661072037436
LOSS train 0.2828533132639477 valid 0.17076747937769185
LOSS train 0.2828533132639477 valid 0.17073722959049348
LOSS train 0.2828533132639477 valid 0.17069673994856496
LOSS train 0.2828533132639477 valid 0.1707498277425766
LOSS train 0.2828533132639477 valid 0.17086097775470643
LOSS train 0.2828533132639477 valid 0.17077601120227903
LOSS train 0.2828533132639477 valid 0.17076846223790199
LOSS train 0.2828533132639477 valid 0.17055736886438475
LOSS train 0.2828533132639477 valid 0.17027484212930385
LOSS train 0.2828533132639477 valid 0.17022381734302025
LOSS train 0.2828533132639477 valid 0.17015667327425696
LOSS train 0.2828533132639477 valid 0.17005686854061328
LOSS train 0.2828533132639477 valid 0.17020512083128317
LOSS train 0.2828533132639477 valid 0.1702449001647808
LOSS train 0.2828533132639477 valid 0.17042374567073934
LOSS train 0.2828533132639477 valid 0.17044008289375445
LOSS train 0.2828533132639477 valid 0.17034930444282034
LOSS train 0.2828533132639477 valid 0.17020532681787615
LOSS train 0.2828533132639477 valid 0.1703027789081846
LOSS train 0.2828533132639477 valid 0.17038278979189853
LOSS train 0.2828533132639477 valid 0.1706051995426836
LOSS train 0.2828533132639477 valid 0.17068851577652083
LOSS train 0.2828533132639477 valid 0.17079784162342548
LOSS train 0.2828533132639477 valid 0.17066855625859623
LOSS train 0.2828533132639477 valid 0.170652667341167
LOSS train 0.2828533132639477 valid 0.17065774066513087
LOSS train 0.2828533132639477 valid 0.17049202614942113
LOSS train 0.2828533132639477 valid 0.17069024917663345
LOSS train 0.2828533132639477 valid 0.17064478635787964
LOSS train 0.2828533132639477 valid 0.17048905118806473
LOSS train 0.2828533132639477 valid 0.17048862988227292
LOSS train 0.2828533132639477 valid 0.1702795004338221
LOSS train 0.2828533132639477 valid 0.1703439788384871
LOSS train 0.2828533132639477 valid 0.17039024551068582
LOSS train 0.2828533132639477 valid 0.1706344155737987
LOSS train 0.2828533132639477 valid 0.17060011425975022
LOSS train 0.2828533132639477 valid 0.17064655496727063
LOSS train 0.2828533132639477 valid 0.17074088002525786
LOSS train 0.2828533132639477 valid 0.1707495807670057
LOSS train 0.2828533132639477 valid 0.1708464437389966
LOSS train 0.2828533132639477 valid 0.1708860481042921
LOSS train 0.2828533132639477 valid 0.17070838434198882
LOSS train 0.2828533132639477 valid 0.17077395101872886
LOSS train 0.2828533132639477 valid 0.17088121934370562
LOSS train 0.2828533132639477 valid 0.17075892860989972
LOSS train 0.2828533132639477 valid 0.17082280732557445
LOSS train 0.2828533132639477 valid 0.17065775101738317
LOSS train 0.2828533132639477 valid 0.17047407747020382
LOSS train 0.2828533132639477 valid 0.17051361606401555
LOSS train 0.2828533132639477 valid 0.17037340989935468
LOSS train 0.2828533132639477 valid 0.17033660160594208
LOSS train 0.2828533132639477 valid 0.17030200448339386
LOSS train 0.2828533132639477 valid 0.17039789687628035
LOSS train 0.2828533132639477 valid 0.1703444504737854
LOSS train 0.2828533132639477 valid 0.17028200838037513
LOSS train 0.2828533132639477 valid 0.17026125079831161
LOSS train 0.2828533132639477 valid 0.17038397750492845
LOSS train 0.2828533132639477 valid 0.1704989623424061
LOSS train 0.2828533132639477 valid 0.17044316058357556
LOSS train 0.2828533132639477 valid 0.17033875968245513
LOSS train 0.2828533132639477 valid 0.17027348484639282
LOSS train 0.2828533132639477 valid 0.17024044024814022
LOSS train 0.2828533132639477 valid 0.17020398961461108
LOSS train 0.2828533132639477 valid 0.1702487186000154
LOSS train 0.2828533132639477 valid 0.17039482047160467
LOSS train 0.2828533132639477 valid 0.17024528534017144
LOSS train 0.2828533132639477 valid 0.17021423063062607
LOSS train 0.2828533132639477 valid 0.17006304797041352
LOSS train 0.2828533132639477 valid 0.1700340844298664
LOSS train 0.2828533132639477 valid 0.17016848430271547
LOSS train 0.2828533132639477 valid 0.17018799980481467
LOSS train 0.2828533132639477 valid 0.17006707129700813
LOSS train 0.2828533132639477 valid 0.17010000010126644
LOSS train 0.2828533132639477 valid 0.17003319714313897
LOSS train 0.2828533132639477 valid 0.1702694902766724
LOSS train 0.2828533132639477 valid 0.17025449425738476
LOSS train 0.2828533132639477 valid 0.17033890702507712
LOSS train 0.2828533132639477 valid 0.17027974982357505
LOSS train 0.2828533132639477 valid 0.1702405396848917
LOSS train 0.2828533132639477 valid 0.1702122734257238
LOSS train 0.2828533132639477 valid 0.1702967511840386
LOSS train 0.2828533132639477 valid 0.17046092957111414
LOSS train 0.2828533132639477 valid 0.1703917233675134
LOSS train 0.2828533132639477 valid 0.17034160058672834
LOSS train 0.2828533132639477 valid 0.17038663397136244
LOSS train 0.2828533132639477 valid 0.1703581858948233
LOSS train 0.2828533132639477 valid 0.1702131896208112
LOSS train 0.2828533132639477 valid 0.1702134443099419
LOSS train 0.2828533132639477 valid 0.17025843880006244
LOSS train 0.2828533132639477 valid 0.1704474410739555
LOSS train 0.2828533132639477 valid 0.17033672831811994
LOSS train 0.2828533132639477 valid 0.17036963537545272
LOSS train 0.2828533132639477 valid 0.17032999649783162
LOSS train 0.2828533132639477 valid 0.17008222965307013
LOSS train 0.2828533132639477 valid 0.1698946420931154
LOSS train 0.2828533132639477 valid 0.1697202653791498
LOSS train 0.2828533132639477 valid 0.16970308152360652
LOSS train 0.2828533132639477 valid 0.16974015962587644
LOSS train 0.2828533132639477 valid 0.16971378150311384
LOSS train 0.2828533132639477 valid 0.16966037128575787
LOSS train 0.2828533132639477 valid 0.1697305540915008
LOSS train 0.2828533132639477 valid 0.1696635110495871
LOSS train 0.2828533132639477 valid 0.16950302983501128
LOSS train 0.2828533132639477 valid 0.1693904705180062
LOSS train 0.2828533132639477 valid 0.16933717980849006
LOSS train 0.2828533132639477 valid 0.1692961666421218
LOSS train 0.2828533132639477 valid 0.16922500768774434
LOSS train 0.2828533132639477 valid 0.16916682359693352
LOSS train 0.2828533132639477 valid 0.1692095690447351
LOSS train 0.2828533132639477 valid 0.169230593811898
LOSS train 0.2828533132639477 valid 0.16932519101377191
LOSS train 0.2828533132639477 valid 0.16953489996588794
LOSS train 0.2828533132639477 valid 0.1694790355415426
LOSS train 0.2828533132639477 valid 0.16944741777917172
LOSS train 0.2828533132639477 valid 0.16930781759447971
LOSS train 0.2828533132639477 valid 0.16938134751239406
LOSS train 0.2828533132639477 valid 0.16942467646939413
LOSS train 0.2828533132639477 valid 0.16936627298468826
LOSS train 0.2828533132639477 valid 0.16911316045249503
LOSS train 0.2828533132639477 valid 0.16901550707851704
LOSS train 0.2828533132639477 valid 0.1690812140149026
LOSS train 0.2828533132639477 valid 0.16910526374117338
LOSS train 0.2828533132639477 valid 0.16909133639858395
LOSS train 0.2828533132639477 valid 0.169123630773048
LOSS train 0.2828533132639477 valid 0.16915936189574923
LOSS train 0.2828533132639477 valid 0.16918958081045615
LOSS train 0.2828533132639477 valid 0.16922635618116585
LOSS train 0.2828533132639477 valid 0.16911501724676914
LOSS train 0.2828533132639477 valid 0.1692443602979183
LOSS train 0.2828533132639477 valid 0.1693725097048805
LOSS train 0.2828533132639477 valid 0.1694129914459255
LOSS train 0.2828533132639477 valid 0.169327360220813
LOSS train 0.2828533132639477 valid 0.169414965685193
LOSS train 0.2828533132639477 valid 0.16934903226062364
LOSS train 0.2828533132639477 valid 0.16929526402964257
LOSS train 0.2828533132639477 valid 0.16928633174891602
LOSS train 0.2828533132639477 valid 0.16928895704390468
LOSS train 0.2828533132639477 valid 0.1692296170762607
LOSS train 0.2828533132639477 valid 0.16909406010348063
LOSS train 0.2828533132639477 valid 0.16913555947871045
LOSS train 0.2828533132639477 valid 0.16929396938617902
LOSS train 0.2828533132639477 valid 0.16920876097883109
LOSS train 0.2828533132639477 valid 0.16925872650674798
LOSS train 0.2828533132639477 valid 0.1692230500421434
LOSS train 0.2828533132639477 valid 0.16924093341580906
LOSS train 0.2828533132639477 valid 0.16921300987960694
LOSS train 0.2828533132639477 valid 0.16927370159372465
LOSS train 0.2828533132639477 valid 0.16938685243568455
LOSS train 0.2828533132639477 valid 0.16929276897399514
LOSS train 0.2828533132639477 valid 0.16925468180342354
LOSS train 0.2828533132639477 valid 0.16914365037947016
LOSS train 0.2828533132639477 valid 0.16907878817765268
LOSS train 0.2828533132639477 valid 0.1690668779948767
LOSS train 0.2828533132639477 valid 0.1691470020738515
LOSS train 0.2828533132639477 valid 0.16915818230937357
LOSS train 0.2828533132639477 valid 0.16915655881166458
LOSS train 0.2828533132639477 valid 0.1692342489904208
LOSS train 0.2828533132639477 valid 0.16926945850092878
LOSS train 0.2828533132639477 valid 0.16916285175830126
LOSS train 0.2828533132639477 valid 0.16907730033071017
LOSS train 0.2828533132639477 valid 0.16903788963636607
LOSS train 0.2828533132639477 valid 0.16913374313301416
LOSS train 0.2828533132639477 valid 0.16902694355329156
LOSS train 0.2828533132639477 valid 0.16907625054581124
LOSS train 0.2828533132639477 valid 0.16906647730644767
LOSS train 0.2828533132639477 valid 0.1690827269327765
LOSS train 0.2828533132639477 valid 0.16904133105547064
LOSS train 0.2828533132639477 valid 0.16906654992008704
LOSS train 0.2828533132639477 valid 0.16900571844187276
LOSS train 0.2828533132639477 valid 0.16902703505099023
LOSS train 0.2828533132639477 valid 0.1690397019919059
LOSS train 0.2828533132639477 valid 0.16900251696439325
LOSS train 0.2828533132639477 valid 0.16893319200192178
LOSS train 0.2828533132639477 valid 0.16894361182289608
LOSS train 0.2828533132639477 valid 0.1689496785150589
LOSS train 0.2828533132639477 valid 0.1689699643639603
LOSS train 0.2828533132639477 valid 0.16902132059803746
LOSS train 0.2828533132639477 valid 0.1689631771233967
LOSS train 0.2828533132639477 valid 0.16891687251627446
LOSS train 0.2828533132639477 valid 0.16889945036450096
LOSS train 0.2828533132639477 valid 0.16892568493224927
LOSS train 0.2828533132639477 valid 0.1688616587826521
LOSS train 0.2828533132639477 valid 0.16889078902864926
LOSS train 0.2828533132639477 valid 0.1688652666621521
LOSS train 0.2828533132639477 valid 0.1690502923736775
LOSS train 0.2828533132639477 valid 0.16903706169186664
LOSS train 0.2828533132639477 valid 0.16913039418687295
LOSS train 0.2828533132639477 valid 0.1691761709098677
LOSS train 0.2828533132639477 valid 0.1692255445305378
LOSS train 0.2828533132639477 valid 0.16924036227142697
LOSS train 0.2828533132639477 valid 0.1692151832036101
LOSS train 0.2828533132639477 valid 0.169335739181255
LOSS train 0.2828533132639477 valid 0.16935288823049538
LOSS train 0.2828533132639477 valid 0.16937424434082848
LOSS train 0.2828533132639477 valid 0.1694417835649433
LOSS train 0.2828533132639477 valid 0.16941379290367903
LOSS train 0.2828533132639477 valid 0.1694244000381269
LOSS train 0.2828533132639477 valid 0.16941040228807067
LOSS train 0.2828533132639477 valid 0.16934543668758123
LOSS train 0.2828533132639477 valid 0.16939014361190646
LOSS train 0.2828533132639477 valid 0.16938586771395636
LOSS train 0.2828533132639477 valid 0.1692793781497161
LOSS train 0.2828533132639477 valid 0.16925458304216096
LOSS train 0.2828533132639477 valid 0.16911985129117965
LOSS train 0.2828533132639477 valid 0.1692117324407481
LOSS train 0.2828533132639477 valid 0.16918986459299695
LOSS train 0.2828533132639477 valid 0.16921705877544677
LOSS train 0.2828533132639477 valid 0.1691503673112501
LOSS train 0.2828533132639477 valid 0.16916041110049596
LOSS train 0.2828533132639477 valid 0.16908652060791804
LOSS train 0.2828533132639477 valid 0.16904252186327814
LOSS train 0.2828533132639477 valid 0.1691302212106215
LOSS train 0.2828533132639477 valid 0.16908087405169794
LOSS train 0.2828533132639477 valid 0.16907137274297315
LOSS train 0.2828533132639477 valid 0.16904396193456792
LOSS train 0.2828533132639477 valid 0.1691714050110087
LOSS train 0.2828533132639477 valid 0.16912467493372557
LOSS train 0.2828533132639477 valid 0.16910160917535996
LOSS train 0.2828533132639477 valid 0.1691607761251576
LOSS train 0.2828533132639477 valid 0.16917184757784315
LOSS train 0.2828533132639477 valid 0.16911276662994545
LOSS train 0.2828533132639477 valid 0.16902953410200752
LOSS train 0.2828533132639477 valid 0.16903790049688067
LOSS train 0.2828533132639477 valid 0.16908021688029387
LOSS train 0.2828533132639477 valid 0.16903164007329527
LOSS train 0.2828533132639477 valid 0.16897728088008568
LOSS train 0.2828533132639477 valid 0.1690170106574379
LOSS train 0.2828533132639477 valid 0.16895559107762012
LOSS train 0.2828533132639477 valid 0.1689988815145833
LOSS train 0.2828533132639477 valid 0.16904916593067326
LOSS train 0.2828533132639477 valid 0.16908445689742538
LOSS train 0.2828533132639477 valid 0.1691403235110953
LOSS train 0.2828533132639477 valid 0.16908907846121465
LOSS train 0.2828533132639477 valid 0.16899912531526995
LOSS train 0.2828533132639477 valid 0.169003437472026
LOSS train 0.2828533132639477 valid 0.16909640862577768
LOSS train 0.2828533132639477 valid 0.16913036862648398
LOSS train 0.2828533132639477 valid 0.16913861312324954
LOSS train 0.2828533132639477 valid 0.16922941491421725
LOSS train 0.2828533132639477 valid 0.16917844453099032
LOSS train 0.2828533132639477 valid 0.1692518564095484
LOSS train 0.2828533132639477 valid 0.16930491485461058
LOSS train 0.2828533132639477 valid 0.16924724873196292
LOSS train 0.2828533132639477 valid 0.1693185689310505
LOSS train 0.2828533132639477 valid 0.16932304523111694
LOSS train 0.2828533132639477 valid 0.169332727079333
LOSS train 0.2828533132639477 valid 0.16925952686806736
LOSS train 0.2828533132639477 valid 0.16933302118930066
EPOCH 7:
  batch 1 loss: 0.3490421175956726
  batch 2 loss: 0.34722812473773956
  batch 3 loss: 0.3247200946013133
  batch 4 loss: 0.32939331978559494
  batch 5 loss: 0.3301962435245514
  batch 6 loss: 0.3214442878961563
  batch 7 loss: 0.31953280738421846
  batch 8 loss: 0.3162056878209114
  batch 9 loss: 0.3129091527726915
  batch 10 loss: 0.3065555915236473
  batch 11 loss: 0.3055602677843787
  batch 12 loss: 0.30314214155077934
  batch 13 loss: 0.30148230493068695
  batch 14 loss: 0.3011542952486447
  batch 15 loss: 0.29964366455872854
  batch 16 loss: 0.2992569440975785
  batch 17 loss: 0.2988202913719065
  batch 18 loss: 0.2968141676651107
  batch 19 loss: 0.2961906798576054
  batch 20 loss: 0.29274404495954515
  batch 21 loss: 0.2947900536514464
  batch 22 loss: 0.29529565979133954
  batch 23 loss: 0.29300944118396094
  batch 24 loss: 0.2915846835821867
  batch 25 loss: 0.2914665514230728
  batch 26 loss: 0.2918202183567561
  batch 27 loss: 0.2917099832384675
  batch 28 loss: 0.2914638875850609
  batch 29 loss: 0.2908004106118761
  batch 30 loss: 0.29149569223324456
  batch 31 loss: 0.29129606821844656
  batch 32 loss: 0.2920140358619392
  batch 33 loss: 0.2929616854949431
  batch 34 loss: 0.29278187848189297
  batch 35 loss: 0.2936432008232389
  batch 36 loss: 0.293278814603885
  batch 37 loss: 0.292946919798851
  batch 38 loss: 0.29351183576019185
  batch 39 loss: 0.29357600173889065
  batch 40 loss: 0.29340525828301905
  batch 41 loss: 0.293150073507937
  batch 42 loss: 0.2925865242169017
  batch 43 loss: 0.29189455543839654
  batch 44 loss: 0.29176085408438335
  batch 45 loss: 0.29173726008998024
  batch 46 loss: 0.2899070009589195
  batch 47 loss: 0.28935648564328537
  batch 48 loss: 0.2892996572578947
  batch 49 loss: 0.2894915825858408
  batch 50 loss: 0.28897122949361803
  batch 51 loss: 0.2884889744660434
  batch 52 loss: 0.2879475507025535
  batch 53 loss: 0.2867740308338741
  batch 54 loss: 0.2871103137731552
  batch 55 loss: 0.28745809143239803
  batch 56 loss: 0.28780671102660044
  batch 57 loss: 0.2871912984471572
  batch 58 loss: 0.28691309330792264
  batch 59 loss: 0.2866892380229497
  batch 60 loss: 0.28721764336029687
  batch 61 loss: 0.28750367633631974
  batch 62 loss: 0.28792259866191494
  batch 63 loss: 0.28767085169988965
  batch 64 loss: 0.2873044847510755
  batch 65 loss: 0.28710806232232317
  batch 66 loss: 0.28671873609224957
  batch 67 loss: 0.2860693382238274
  batch 68 loss: 0.285874153542168
  batch 69 loss: 0.28587743834308954
  batch 70 loss: 0.2848163651568549
  batch 71 loss: 0.28458334610495767
  batch 72 loss: 0.28424950275156236
  batch 73 loss: 0.2836285890781716
  batch 74 loss: 0.2829902045227386
  batch 75 loss: 0.28251533846060434
  batch 76 loss: 0.2823933937439793
  batch 77 loss: 0.28197407625712356
  batch 78 loss: 0.2819804860613285
  batch 79 loss: 0.28219901440264306
  batch 80 loss: 0.2818833729252219
  batch 81 loss: 0.2814629977499997
  batch 82 loss: 0.2815198514883111
  batch 83 loss: 0.28117424447134315
  batch 84 loss: 0.28068940430170014
  batch 85 loss: 0.2806344111176098
  batch 86 loss: 0.28060826953760415
  batch 87 loss: 0.28041589345740175
  batch 88 loss: 0.2803179498084567
  batch 89 loss: 0.2800534385978506
  batch 90 loss: 0.2804824224776692
  batch 91 loss: 0.2803723145644743
  batch 92 loss: 0.2801322345824345
  batch 93 loss: 0.28006544532955335
  batch 94 loss: 0.280216024435581
  batch 95 loss: 0.28040206353915365
  batch 96 loss: 0.2803442336929341
  batch 97 loss: 0.2808483330552111
  batch 98 loss: 0.2808466372745378
  batch 99 loss: 0.2806725449333287
  batch 100 loss: 0.28054242447018624
  batch 101 loss: 0.280359619767359
  batch 102 loss: 0.2805115956009603
  batch 103 loss: 0.28037565150885907
  batch 104 loss: 0.2806062502070115
  batch 105 loss: 0.28045960792473384
  batch 106 loss: 0.28071227993042963
  batch 107 loss: 0.2806910951839429
  batch 108 loss: 0.28030455291823103
  batch 109 loss: 0.28042822078280494
  batch 110 loss: 0.2803972047838298
  batch 111 loss: 0.2802785177220095
  batch 112 loss: 0.28006592486053705
  batch 113 loss: 0.27978286529536794
  batch 114 loss: 0.279599990630359
  batch 115 loss: 0.2790892567323602
  batch 116 loss: 0.2789963054759749
  batch 117 loss: 0.2789406671992734
  batch 118 loss: 0.2790976073277199
  batch 119 loss: 0.27933133124303416
  batch 120 loss: 0.27893007968862854
  batch 121 loss: 0.2786563854079601
  batch 122 loss: 0.278570325403917
  batch 123 loss: 0.2782485705565631
  batch 124 loss: 0.27801331721486583
  batch 125 loss: 0.2777553697824478
  batch 126 loss: 0.2779427555108827
  batch 127 loss: 0.27789091160447577
  batch 128 loss: 0.2777966194553301
  batch 129 loss: 0.27793621728124546
  batch 130 loss: 0.27787702117974944
  batch 131 loss: 0.2777971105493662
  batch 132 loss: 0.2777160861049638
  batch 133 loss: 0.2775224217570814
  batch 134 loss: 0.27725792375963126
  batch 135 loss: 0.27717349220205234
  batch 136 loss: 0.2772771185811828
  batch 137 loss: 0.2774328711259104
  batch 138 loss: 0.2774698222460954
  batch 139 loss: 0.2775857609381779
  batch 140 loss: 0.27791811781270165
  batch 141 loss: 0.2778360093316288
  batch 142 loss: 0.27779392331418856
  batch 143 loss: 0.27801734709239506
  batch 144 loss: 0.2780625381403499
  batch 145 loss: 0.2781469863036583
  batch 146 loss: 0.2780382261700826
  batch 147 loss: 0.278081535887556
  batch 148 loss: 0.27827898030345505
  batch 149 loss: 0.27793364256820424
  batch 150 loss: 0.27786986728509266
  batch 151 loss: 0.2776253219471862
  batch 152 loss: 0.2775118488230203
  batch 153 loss: 0.2778187191564273
  batch 154 loss: 0.27783921341617385
  batch 155 loss: 0.27781153936539926
  batch 156 loss: 0.27766516479926234
  batch 157 loss: 0.27762637795156736
  batch 158 loss: 0.2777743143371389
  batch 159 loss: 0.2779097622670468
  batch 160 loss: 0.2777074420824647
  batch 161 loss: 0.27752466157356404
  batch 162 loss: 0.27750775935473265
  batch 163 loss: 0.27726976189510955
  batch 164 loss: 0.2774147512164058
  batch 165 loss: 0.27747979751138974
  batch 166 loss: 0.27761932965143615
  batch 167 loss: 0.27744007726272424
  batch 168 loss: 0.2771017046733981
  batch 169 loss: 0.2772240435054316
  batch 170 loss: 0.2771665865007569
  batch 171 loss: 0.2773916166253954
  batch 172 loss: 0.2775188676666382
  batch 173 loss: 0.27765416659716236
  batch 174 loss: 0.27778804276523916
  batch 175 loss: 0.27770152117524827
  batch 176 loss: 0.2779634143649177
  batch 177 loss: 0.2782474363085914
  batch 178 loss: 0.27806426475891904
  batch 179 loss: 0.2781642439811589
  batch 180 loss: 0.2781580922504266
  batch 181 loss: 0.27806886709526757
  batch 182 loss: 0.27802318768514384
  batch 183 loss: 0.27788203718558036
  batch 184 loss: 0.2776752793756516
  batch 185 loss: 0.27753854031498365
  batch 186 loss: 0.27752854258462945
  batch 187 loss: 0.27772092460629777
  batch 188 loss: 0.2776559357471923
  batch 189 loss: 0.27761637715120163
  batch 190 loss: 0.27755140497496256
  batch 191 loss: 0.2777551227683172
  batch 192 loss: 0.2775833261354516
  batch 193 loss: 0.2776364453252733
  batch 194 loss: 0.2776220349460533
  batch 195 loss: 0.27775668692894473
  batch 196 loss: 0.2776317063490955
  batch 197 loss: 0.2776992026773201
  batch 198 loss: 0.2777514908500392
  batch 199 loss: 0.27777496481960146
  batch 200 loss: 0.27772536404430864
  batch 201 loss: 0.2775646525977263
  batch 202 loss: 0.2775556094723173
  batch 203 loss: 0.2775287179670898
  batch 204 loss: 0.277457093272139
  batch 205 loss: 0.27737877841402847
  batch 206 loss: 0.2773764043033702
  batch 207 loss: 0.27733259147779954
  batch 208 loss: 0.27713176837334264
  batch 209 loss: 0.2770750277350394
  batch 210 loss: 0.2771191154207502
  batch 211 loss: 0.27699104911908157
  batch 212 loss: 0.2770053633260277
  batch 213 loss: 0.27687734071637543
  batch 214 loss: 0.27682961774206605
  batch 215 loss: 0.2767859110998553
  batch 216 loss: 0.27686890863158087
  batch 217 loss: 0.2765990169504271
  batch 218 loss: 0.27650435699508824
  batch 219 loss: 0.2762633302168215
  batch 220 loss: 0.27633621733297004
  batch 221 loss: 0.2763042255764094
  batch 222 loss: 0.27633079779040703
  batch 223 loss: 0.276169070882113
  batch 224 loss: 0.2762502364016005
  batch 225 loss: 0.2764390089776781
  batch 226 loss: 0.2765924759143222
  batch 227 loss: 0.27649568395467583
  batch 228 loss: 0.276579621721778
  batch 229 loss: 0.2767197219044881
  batch 230 loss: 0.27680119042811185
  batch 231 loss: 0.27692314685681163
  batch 232 loss: 0.2768155445569548
  batch 233 loss: 0.2768363764613483
  batch 234 loss: 0.2767888361062759
  batch 235 loss: 0.27668354574670184
  batch 236 loss: 0.2766325347504373
  batch 237 loss: 0.27674775850420763
  batch 238 loss: 0.27671712824777395
  batch 239 loss: 0.2767628628339728
  batch 240 loss: 0.2768053670724233
  batch 241 loss: 0.27676676380683773
  batch 242 loss: 0.2767530709258781
  batch 243 loss: 0.2767338484157751
  batch 244 loss: 0.2767067317102776
  batch 245 loss: 0.2767309620672343
  batch 246 loss: 0.27677181581171545
  batch 247 loss: 0.2767212569472278
  batch 248 loss: 0.2768122905925397
  batch 249 loss: 0.2766137306829054
  batch 250 loss: 0.27647734558582304
  batch 251 loss: 0.27635827375598165
  batch 252 loss: 0.27611807615511
  batch 253 loss: 0.27609984064290644
  batch 254 loss: 0.2761874779706865
  batch 255 loss: 0.27602378772754294
  batch 256 loss: 0.27600305376108736
  batch 257 loss: 0.2760320832293321
  batch 258 loss: 0.27587952412838157
  batch 259 loss: 0.2757048437148908
  batch 260 loss: 0.2756001708599237
  batch 261 loss: 0.27577752868334454
  batch 262 loss: 0.27573971655077606
  batch 263 loss: 0.27569986239585587
  batch 264 loss: 0.27587162432345474
  batch 265 loss: 0.27592611121681504
  batch 266 loss: 0.2758226029406813
  batch 267 loss: 0.27569561917683605
  batch 268 loss: 0.27562303529746496
  batch 269 loss: 0.2756225324917903
  batch 270 loss: 0.2756148155088778
  batch 271 loss: 0.2757543129674623
  batch 272 loss: 0.2756674152744167
  batch 273 loss: 0.27559804086720113
  batch 274 loss: 0.275522817986725
  batch 275 loss: 0.2755014059760354
  batch 276 loss: 0.27556245892808057
  batch 277 loss: 0.27557793270379627
  batch 278 loss: 0.27549279754539185
  batch 279 loss: 0.27547948388215887
  batch 280 loss: 0.2754044067646776
  batch 281 loss: 0.2754787163165965
  batch 282 loss: 0.2754339686733611
  batch 283 loss: 0.2754520621190223
  batch 284 loss: 0.2753965766497061
  batch 285 loss: 0.27532279763305395
  batch 286 loss: 0.27538548649607836
  batch 287 loss: 0.27539927722685015
  batch 288 loss: 0.27542990404698586
  batch 289 loss: 0.27539518403346974
  batch 290 loss: 0.2752698992860728
  batch 291 loss: 0.2752289014173947
  batch 292 loss: 0.2751206903629107
  batch 293 loss: 0.27516470935971254
  batch 294 loss: 0.2750871043626954
  batch 295 loss: 0.2750973178168475
  batch 296 loss: 0.2752598164653456
  batch 297 loss: 0.27525022656026515
  batch 298 loss: 0.2751373039596033
  batch 299 loss: 0.27523620601083126
  batch 300 loss: 0.2751992196838061
  batch 301 loss: 0.27520228788702195
  batch 302 loss: 0.275244640494814
  batch 303 loss: 0.27516884034616323
  batch 304 loss: 0.27523263210528776
  batch 305 loss: 0.275205979386314
  batch 306 loss: 0.2753800125488269
  batch 307 loss: 0.27533730893647634
  batch 308 loss: 0.2753713766669298
  batch 309 loss: 0.2753506498622277
  batch 310 loss: 0.2755091487399993
  batch 311 loss: 0.2753956866992632
  batch 312 loss: 0.27534709421870035
  batch 313 loss: 0.27534251548230837
  batch 314 loss: 0.2753492920262039
  batch 315 loss: 0.27529497326366487
  batch 316 loss: 0.2751939499962933
  batch 317 loss: 0.2754450646766729
  batch 318 loss: 0.2752884677464857
  batch 319 loss: 0.27513176697922354
  batch 320 loss: 0.2753176829777658
  batch 321 loss: 0.2752593497620936
  batch 322 loss: 0.2750840357371739
  batch 323 loss: 0.27508628995795
  batch 324 loss: 0.27512134538020616
  batch 325 loss: 0.27513453611960775
  batch 326 loss: 0.2753540777538452
  batch 327 loss: 0.27532834525502053
  batch 328 loss: 0.2753819720592441
  batch 329 loss: 0.27548364475142995
  batch 330 loss: 0.2755214021061406
  batch 331 loss: 0.2755144007976682
  batch 332 loss: 0.2756329837692789
  batch 333 loss: 0.2756068624175705
  batch 334 loss: 0.27583314710391493
  batch 335 loss: 0.275853797570983
  batch 336 loss: 0.2758752956454243
  batch 337 loss: 0.27584929644884626
  batch 338 loss: 0.2758389741358672
  batch 339 loss: 0.27578844143226083
  batch 340 loss: 0.275738958839108
  batch 341 loss: 0.27571584043964265
  batch 342 loss: 0.27571742646178304
  batch 343 loss: 0.2758147874656989
  batch 344 loss: 0.2758434576004051
  batch 345 loss: 0.2758509073568427
  batch 346 loss: 0.27583693183226393
  batch 347 loss: 0.2756745318428584
  batch 348 loss: 0.27561502890854045
  batch 349 loss: 0.275722814878625
  batch 350 loss: 0.2757023701071739
  batch 351 loss: 0.2757379388452595
  batch 352 loss: 0.2757118247364732
  batch 353 loss: 0.27570725791683953
  batch 354 loss: 0.2758111804302803
  batch 355 loss: 0.27572562648376947
  batch 356 loss: 0.27571559751803953
  batch 357 loss: 0.27554088451114356
  batch 358 loss: 0.2754204905482644
  batch 359 loss: 0.27541396075802593
  batch 360 loss: 0.2753242133392228
  batch 361 loss: 0.27530107058976827
  batch 362 loss: 0.275193873484161
  batch 363 loss: 0.2751959125939808
  batch 364 loss: 0.2751100025661699
  batch 365 loss: 0.275110806912592
  batch 366 loss: 0.27514574118976387
  batch 367 loss: 0.27524922134440993
  batch 368 loss: 0.2751411868907187
  batch 369 loss: 0.2751911270828428
  batch 370 loss: 0.2753733755366222
  batch 371 loss: 0.2753111072226033
  batch 372 loss: 0.2753783902253515
  batch 373 loss: 0.2754976490946941
  batch 374 loss: 0.27548436673089144
  batch 375 loss: 0.2755188186566035
  batch 376 loss: 0.27566390551943726
  batch 377 loss: 0.27569718785406105
  batch 378 loss: 0.27562843535154585
  batch 379 loss: 0.27561859825827517
  batch 380 loss: 0.27553921910492996
  batch 381 loss: 0.2754905248486151
  batch 382 loss: 0.27542687301080265
  batch 383 loss: 0.2753416636360532
  batch 384 loss: 0.2753242306644097
  batch 385 loss: 0.27526583961852186
  batch 386 loss: 0.2752252748052691
  batch 387 loss: 0.27519601464117527
  batch 388 loss: 0.2751690400860359
  batch 389 loss: 0.27516255307473375
  batch 390 loss: 0.2751414232147046
  batch 391 loss: 0.27503456335390924
  batch 392 loss: 0.27514518527504134
  batch 393 loss: 0.2752167046752595
  batch 394 loss: 0.2752543753249391
  batch 395 loss: 0.2752885477075094
  batch 396 loss: 0.2753033793409063
  batch 397 loss: 0.2753137185306333
  batch 398 loss: 0.27530136199003485
  batch 399 loss: 0.27524704768096
  batch 400 loss: 0.27540533054620026
  batch 401 loss: 0.27536606435615224
  batch 402 loss: 0.27537635676748123
  batch 403 loss: 0.27529336933610454
  batch 404 loss: 0.2752466243461217
  batch 405 loss: 0.2752962659538528
  batch 406 loss: 0.27524539296116146
  batch 407 loss: 0.27525861363592546
  batch 408 loss: 0.27520509688731504
  batch 409 loss: 0.27523880399089573
  batch 410 loss: 0.2751913805196925
  batch 411 loss: 0.2751697316004412
  batch 412 loss: 0.2751920405160455
  batch 413 loss: 0.2752073874412957
  batch 414 loss: 0.2751955072157049
  batch 415 loss: 0.27514284687588014
  batch 416 loss: 0.27521042701286763
  batch 417 loss: 0.2751150549673062
  batch 418 loss: 0.27509464049025584
  batch 419 loss: 0.275148519148121
  batch 420 loss: 0.27519204279496556
  batch 421 loss: 0.275202173624639
  batch 422 loss: 0.2751283787932441
  batch 423 loss: 0.27517791374214434
  batch 424 loss: 0.2752626587127177
  batch 425 loss: 0.2751768583059311
  batch 426 loss: 0.2750835887772936
  batch 427 loss: 0.2750886085789991
  batch 428 loss: 0.27504635208817285
  batch 429 loss: 0.27501004180569194
  batch 430 loss: 0.2750691674822985
  batch 431 loss: 0.275024386922334
  batch 432 loss: 0.27507213906695444
  batch 433 loss: 0.27498919566701796
  batch 434 loss: 0.2748607644974361
  batch 435 loss: 0.27499397513510165
  batch 436 loss: 0.27501946024665047
  batch 437 loss: 0.27496338879489246
  batch 438 loss: 0.2749206399264401
  batch 439 loss: 0.2747921317747894
  batch 440 loss: 0.27486110207709397
  batch 441 loss: 0.2748957657489647
  batch 442 loss: 0.2748880629760647
  batch 443 loss: 0.2748313855640506
  batch 444 loss: 0.2749038427262693
  batch 445 loss: 0.27486916740288897
  batch 446 loss: 0.27488191846774834
  batch 447 loss: 0.2748738583302338
  batch 448 loss: 0.2749057680235377
  batch 449 loss: 0.2749109292083434
  batch 450 loss: 0.2748639631933636
  batch 451 loss: 0.2748308122951017
  batch 452 loss: 0.2747317339598605
  batch 453 loss: 0.2746876131488261
  batch 454 loss: 0.2745966167570736
  batch 455 loss: 0.2745006752210659
  batch 456 loss: 0.2744830291587533
  batch 457 loss: 0.2745043741991536
  batch 458 loss: 0.27447157288092194
  batch 459 loss: 0.274413428697451
  batch 460 loss: 0.27441190471467763
  batch 461 loss: 0.2744685223653363
  batch 462 loss: 0.27448847402870913
  batch 463 loss: 0.27447498678259924
  batch 464 loss: 0.2744238721206784
  batch 465 loss: 0.27442390242571474
  batch 466 loss: 0.2744224902578178
  batch 467 loss: 0.27447751620724575
  batch 468 loss: 0.2744469464653068
  batch 469 loss: 0.2744322352127226
  batch 470 loss: 0.2747177610689021
  batch 471 loss: 0.2747605424967541
  batch 472 loss: 0.2746742882971036
LOSS train 0.2746742882971036 valid 0.18331260979175568
LOSS train 0.2746742882971036 valid 0.17759573459625244
LOSS train 0.2746742882971036 valid 0.19545127948125204
LOSS train 0.2746742882971036 valid 0.18992922455072403
LOSS train 0.2746742882971036 valid 0.19671933650970458
LOSS train 0.2746742882971036 valid 0.19733276963233948
LOSS train 0.2746742882971036 valid 0.19072793211255754
LOSS train 0.2746742882971036 valid 0.1910872794687748
LOSS train 0.2746742882971036 valid 0.19179009232256147
LOSS train 0.2746742882971036 valid 0.1894830897450447
LOSS train 0.2746742882971036 valid 0.18868511779741806
LOSS train 0.2746742882971036 valid 0.19043651595711708
LOSS train 0.2746742882971036 valid 0.19095527438017038
LOSS train 0.2746742882971036 valid 0.18921680854899542
LOSS train 0.2746742882971036 valid 0.1885207494099935
LOSS train 0.2746742882971036 valid 0.19139011949300766
LOSS train 0.2746742882971036 valid 0.19081824842621298
LOSS train 0.2746742882971036 valid 0.19039959543281132
LOSS train 0.2746742882971036 valid 0.19107462857898913
LOSS train 0.2746742882971036 valid 0.1908894181251526
LOSS train 0.2746742882971036 valid 0.19231363563310533
LOSS train 0.2746742882971036 valid 0.19090473448688333
LOSS train 0.2746742882971036 valid 0.18983718169772107
LOSS train 0.2746742882971036 valid 0.18989081618686518
LOSS train 0.2746742882971036 valid 0.19000691950321197
LOSS train 0.2746742882971036 valid 0.1893190128298906
LOSS train 0.2746742882971036 valid 0.1897945558583295
LOSS train 0.2746742882971036 valid 0.1897334467087473
LOSS train 0.2746742882971036 valid 0.1887328614448679
LOSS train 0.2746742882971036 valid 0.18788617551326753
LOSS train 0.2746742882971036 valid 0.1875466319822496
LOSS train 0.2746742882971036 valid 0.18789445981383324
LOSS train 0.2746742882971036 valid 0.18723263252865185
LOSS train 0.2746742882971036 valid 0.18666858357541702
LOSS train 0.2746742882971036 valid 0.18736483710152763
LOSS train 0.2746742882971036 valid 0.18803037661645147
LOSS train 0.2746742882971036 valid 0.18727959531384544
LOSS train 0.2746742882971036 valid 0.18701830231829694
LOSS train 0.2746742882971036 valid 0.18657685892704207
LOSS train 0.2746742882971036 valid 0.18688227348029612
LOSS train 0.2746742882971036 valid 0.1865452189997929
LOSS train 0.2746742882971036 valid 0.1878099874371574
LOSS train 0.2746742882971036 valid 0.18819620269675588
LOSS train 0.2746742882971036 valid 0.18750856139443137
LOSS train 0.2746742882971036 valid 0.18733115626706018
LOSS train 0.2746742882971036 valid 0.1869130001767822
LOSS train 0.2746742882971036 valid 0.1867831878839655
LOSS train 0.2746742882971036 valid 0.1884959088638425
LOSS train 0.2746742882971036 valid 0.1881410646803525
LOSS train 0.2746742882971036 valid 0.1889172950387001
LOSS train 0.2746742882971036 valid 0.18872765437060712
LOSS train 0.2746742882971036 valid 0.188782853862414
LOSS train 0.2746742882971036 valid 0.1895522154727072
LOSS train 0.2746742882971036 valid 0.18954848360132287
LOSS train 0.2746742882971036 valid 0.18929635150866075
LOSS train 0.2746742882971036 valid 0.1892833531435047
LOSS train 0.2746742882971036 valid 0.18858318344542854
LOSS train 0.2746742882971036 valid 0.18909694151631717
LOSS train 0.2746742882971036 valid 0.18908286827095486
LOSS train 0.2746742882971036 valid 0.18913561974962553
LOSS train 0.2746742882971036 valid 0.1892220275323899
LOSS train 0.2746742882971036 valid 0.18892214423225773
LOSS train 0.2746742882971036 valid 0.18870966538550363
LOSS train 0.2746742882971036 valid 0.18854666221886873
LOSS train 0.2746742882971036 valid 0.18772885111662058
LOSS train 0.2746742882971036 valid 0.18757245396122788
LOSS train 0.2746742882971036 valid 0.1882454363712624
LOSS train 0.2746742882971036 valid 0.18772072673720472
LOSS train 0.2746742882971036 valid 0.1879693379868632
LOSS train 0.2746742882971036 valid 0.18826855804239
LOSS train 0.2746742882971036 valid 0.18865203185820242
LOSS train 0.2746742882971036 valid 0.18881894337634245
LOSS train 0.2746742882971036 valid 0.18934485989890687
LOSS train 0.2746742882971036 valid 0.18951330797092333
LOSS train 0.2746742882971036 valid 0.18929233868916828
LOSS train 0.2746742882971036 valid 0.1893610766059474
LOSS train 0.2746742882971036 valid 0.18955168785986962
LOSS train 0.2746742882971036 valid 0.18936265546541947
LOSS train 0.2746742882971036 valid 0.18936294449281088
LOSS train 0.2746742882971036 valid 0.18920328393578528
LOSS train 0.2746742882971036 valid 0.1892036144380216
LOSS train 0.2746742882971036 valid 0.18916675702827732
LOSS train 0.2746742882971036 valid 0.18901664880384883
LOSS train 0.2746742882971036 valid 0.1888131776026317
LOSS train 0.2746742882971036 valid 0.1890164852142334
LOSS train 0.2746742882971036 valid 0.18898088155790818
LOSS train 0.2746742882971036 valid 0.18847834344567924
LOSS train 0.2746742882971036 valid 0.18846230022609234
LOSS train 0.2746742882971036 valid 0.18869019308117
LOSS train 0.2746742882971036 valid 0.18873927609788047
LOSS train 0.2746742882971036 valid 0.18882972788024735
LOSS train 0.2746742882971036 valid 0.18868346431333086
LOSS train 0.2746742882971036 valid 0.18869849943345593
LOSS train 0.2746742882971036 valid 0.1889420490949712
LOSS train 0.2746742882971036 valid 0.18906540258934623
LOSS train 0.2746742882971036 valid 0.18944134501119456
LOSS train 0.2746742882971036 valid 0.18951745254477276
LOSS train 0.2746742882971036 valid 0.18971365370920726
LOSS train 0.2746742882971036 valid 0.1898034145735731
LOSS train 0.2746742882971036 valid 0.1897619268298149
LOSS train 0.2746742882971036 valid 0.1898178711976155
LOSS train 0.2746742882971036 valid 0.19007976586912192
LOSS train 0.2746742882971036 valid 0.18980810118531719
LOSS train 0.2746742882971036 valid 0.1896279945683021
LOSS train 0.2746742882971036 valid 0.1899383801789511
LOSS train 0.2746742882971036 valid 0.19004223017760044
LOSS train 0.2746742882971036 valid 0.1897866848473237
LOSS train 0.2746742882971036 valid 0.18973250466364402
LOSS train 0.2746742882971036 valid 0.18923463296452794
LOSS train 0.2746742882971036 valid 0.1890301617709073
LOSS train 0.2746742882971036 valid 0.18918091944746068
LOSS train 0.2746742882971036 valid 0.18922291350151813
LOSS train 0.2746742882971036 valid 0.18908261567090465
LOSS train 0.2746742882971036 valid 0.18900997087098004
LOSS train 0.2746742882971036 valid 0.18940401310506075
LOSS train 0.2746742882971036 valid 0.189190152004875
LOSS train 0.2746742882971036 valid 0.18943449434561607
LOSS train 0.2746742882971036 valid 0.18958320392895553
LOSS train 0.2746742882971036 valid 0.18932019199142938
LOSS train 0.2746742882971036 valid 0.18909981921315194
LOSS train 0.2746742882971036 valid 0.1890656925922583
LOSS train 0.2746742882971036 valid 0.1892135377790107
LOSS train 0.2746742882971036 valid 0.18921647808416103
LOSS train 0.2746742882971036 valid 0.18917637426526315
LOSS train 0.2746742882971036 valid 0.1892241612672806
LOSS train 0.2746742882971036 valid 0.18932982001985824
LOSS train 0.2746742882971036 valid 0.18926770339800617
LOSS train 0.2746742882971036 valid 0.18923633126541972
LOSS train 0.2746742882971036 valid 0.18901791082796202
LOSS train 0.2746742882971036 valid 0.18873585703281257
LOSS train 0.2746742882971036 valid 0.18870489242422672
LOSS train 0.2746742882971036 valid 0.18864664175745213
LOSS train 0.2746742882971036 valid 0.18859526974366123
LOSS train 0.2746742882971036 valid 0.18875231171277032
LOSS train 0.2746742882971036 valid 0.18882329364617664
LOSS train 0.2746742882971036 valid 0.18903200536528053
LOSS train 0.2746742882971036 valid 0.1890629378766039
LOSS train 0.2746742882971036 valid 0.1889949632079705
LOSS train 0.2746742882971036 valid 0.18883052800627922
LOSS train 0.2746742882971036 valid 0.1889579099203859
LOSS train 0.2746742882971036 valid 0.18899213876707335
LOSS train 0.2746742882971036 valid 0.1891861114073807
LOSS train 0.2746742882971036 valid 0.18922549465319494
LOSS train 0.2746742882971036 valid 0.18932810487846533
LOSS train 0.2746742882971036 valid 0.18921273648738862
LOSS train 0.2746742882971036 valid 0.18917440684282616
LOSS train 0.2746742882971036 valid 0.18914428150572746
LOSS train 0.2746742882971036 valid 0.1889488981986368
LOSS train 0.2746742882971036 valid 0.18909821084281742
LOSS train 0.2746742882971036 valid 0.1890486730138461
LOSS train 0.2746742882971036 valid 0.18891250149698446
LOSS train 0.2746742882971036 valid 0.18892518303504116
LOSS train 0.2746742882971036 valid 0.1887225139569613
LOSS train 0.2746742882971036 valid 0.18877268224567562
LOSS train 0.2746742882971036 valid 0.18886534744693387
LOSS train 0.2746742882971036 valid 0.1891498148250274
LOSS train 0.2746742882971036 valid 0.18912982095958322
LOSS train 0.2746742882971036 valid 0.1891172079345848
LOSS train 0.2746742882971036 valid 0.1892088827846935
LOSS train 0.2746742882971036 valid 0.18924172213301063
LOSS train 0.2746742882971036 valid 0.18932920962757205
LOSS train 0.2746742882971036 valid 0.18939135334006063
LOSS train 0.2746742882971036 valid 0.18919964216969495
LOSS train 0.2746742882971036 valid 0.18925123916166584
LOSS train 0.2746742882971036 valid 0.1893953527465011
LOSS train 0.2746742882971036 valid 0.18931377643203162
LOSS train 0.2746742882971036 valid 0.1893520632725276
LOSS train 0.2746742882971036 valid 0.1891727844874064
LOSS train 0.2746742882971036 valid 0.18897956768436544
LOSS train 0.2746742882971036 valid 0.18902680383009068
LOSS train 0.2746742882971036 valid 0.18889868372713614
LOSS train 0.2746742882971036 valid 0.18884481351042903
LOSS train 0.2746742882971036 valid 0.1888256289194085
LOSS train 0.2746742882971036 valid 0.1889371771750779
LOSS train 0.2746742882971036 valid 0.1888865829365594
LOSS train 0.2746742882971036 valid 0.1888142590495673
LOSS train 0.2746742882971036 valid 0.18880764129808394
LOSS train 0.2746742882971036 valid 0.18893973094024016
LOSS train 0.2746742882971036 valid 0.18902144477021096
LOSS train 0.2746742882971036 valid 0.18898574461539586
LOSS train 0.2746742882971036 valid 0.18884719702420313
LOSS train 0.2746742882971036 valid 0.1887598700903274
LOSS train 0.2746742882971036 valid 0.18870525053941486
LOSS train 0.2746742882971036 valid 0.1886784925085047
LOSS train 0.2746742882971036 valid 0.18872323591966886
LOSS train 0.2746742882971036 valid 0.1888691413626876
LOSS train 0.2746742882971036 valid 0.18872524774010807
LOSS train 0.2746742882971036 valid 0.18871688501949005
LOSS train 0.2746742882971036 valid 0.1885944905262145
LOSS train 0.2746742882971036 valid 0.18854198063674726
LOSS train 0.2746742882971036 valid 0.18870501352854424
LOSS train 0.2746742882971036 valid 0.18874107503021756
LOSS train 0.2746742882971036 valid 0.1886312573221681
LOSS train 0.2746742882971036 valid 0.1886522856723402
LOSS train 0.2746742882971036 valid 0.18858888080486885
LOSS train 0.2746742882971036 valid 0.18883212648180067
LOSS train 0.2746742882971036 valid 0.18879073949029604
LOSS train 0.2746742882971036 valid 0.18893817398283216
LOSS train 0.2746742882971036 valid 0.18889491057875168
LOSS train 0.2746742882971036 valid 0.1888595087826252
LOSS train 0.2746742882971036 valid 0.18885833165835386
LOSS train 0.2746742882971036 valid 0.18896440269038228
LOSS train 0.2746742882971036 valid 0.18913601221415796
LOSS train 0.2746742882971036 valid 0.18904758171707975
LOSS train 0.2746742882971036 valid 0.188988135864095
LOSS train 0.2746742882971036 valid 0.18901716165461588
LOSS train 0.2746742882971036 valid 0.1890163068898058
LOSS train 0.2746742882971036 valid 0.18886727932840586
LOSS train 0.2746742882971036 valid 0.1888661389573339
LOSS train 0.2746742882971036 valid 0.1889127629143851
LOSS train 0.2746742882971036 valid 0.18912787022183858
LOSS train 0.2746742882971036 valid 0.1890152760271756
LOSS train 0.2746742882971036 valid 0.18905856009100525
LOSS train 0.2746742882971036 valid 0.18899701236285896
LOSS train 0.2746742882971036 valid 0.18873853912187177
LOSS train 0.2746742882971036 valid 0.188525028526783
LOSS train 0.2746742882971036 valid 0.18833621807636752
LOSS train 0.2746742882971036 valid 0.18832139281231328
LOSS train 0.2746742882971036 valid 0.18836682061898655
LOSS train 0.2746742882971036 valid 0.18831247646700253
LOSS train 0.2746742882971036 valid 0.18821658403085906
LOSS train 0.2746742882971036 valid 0.18829358355687545
LOSS train 0.2746742882971036 valid 0.18821054356247854
LOSS train 0.2746742882971036 valid 0.18806008701877935
LOSS train 0.2746742882971036 valid 0.18793626070022584
LOSS train 0.2746742882971036 valid 0.1878851724013818
LOSS train 0.2746742882971036 valid 0.18784602485301735
LOSS train 0.2746742882971036 valid 0.18778244301415326
LOSS train 0.2746742882971036 valid 0.1877014534181903
LOSS train 0.2746742882971036 valid 0.18774554677631544
LOSS train 0.2746742882971036 valid 0.18775135549631985
LOSS train 0.2746742882971036 valid 0.18784939234369788
LOSS train 0.2746742882971036 valid 0.18808102447843347
LOSS train 0.2746742882971036 valid 0.18798862588711274
LOSS train 0.2746742882971036 valid 0.18797809037756413
LOSS train 0.2746742882971036 valid 0.1878358781085176
LOSS train 0.2746742882971036 valid 0.18789806591056069
LOSS train 0.2746742882971036 valid 0.18797746306958318
LOSS train 0.2746742882971036 valid 0.18791728395049043
LOSS train 0.2746742882971036 valid 0.18763951963434616
LOSS train 0.2746742882971036 valid 0.1875391681659271
LOSS train 0.2746742882971036 valid 0.1875911404894403
LOSS train 0.2746742882971036 valid 0.18761449702727942
LOSS train 0.2746742882971036 valid 0.18759169282971835
LOSS train 0.2746742882971036 valid 0.1875970585005624
LOSS train 0.2746742882971036 valid 0.18762575598751627
LOSS train 0.2746742882971036 valid 0.18766954873013592
LOSS train 0.2746742882971036 valid 0.18772501726785012
LOSS train 0.2746742882971036 valid 0.18760704593246721
LOSS train 0.2746742882971036 valid 0.18774264365434645
LOSS train 0.2746742882971036 valid 0.18787565673015033
LOSS train 0.2746742882971036 valid 0.18792503031473312
LOSS train 0.2746742882971036 valid 0.18785569248463324
LOSS train 0.2746742882971036 valid 0.18793463689370418
LOSS train 0.2746742882971036 valid 0.18785314893021304
LOSS train 0.2746742882971036 valid 0.1877838386571966
LOSS train 0.2746742882971036 valid 0.18778430400887353
LOSS train 0.2746742882971036 valid 0.1878111430609873
LOSS train 0.2746742882971036 valid 0.18772765522297746
LOSS train 0.2746742882971036 valid 0.18761880489496086
LOSS train 0.2746742882971036 valid 0.18763413385869898
LOSS train 0.2746742882971036 valid 0.18780164028169544
LOSS train 0.2746742882971036 valid 0.18771383347620077
LOSS train 0.2746742882971036 valid 0.18775555869620858
LOSS train 0.2746742882971036 valid 0.18773687330057037
LOSS train 0.2746742882971036 valid 0.18771954099262567
LOSS train 0.2746742882971036 valid 0.18769656568207543
LOSS train 0.2746742882971036 valid 0.18778403381358333
LOSS train 0.2746742882971036 valid 0.18788398120926214
LOSS train 0.2746742882971036 valid 0.18780133558644188
LOSS train 0.2746742882971036 valid 0.18776233598754855
LOSS train 0.2746742882971036 valid 0.18766722055699894
LOSS train 0.2746742882971036 valid 0.18761821036592072
LOSS train 0.2746742882971036 valid 0.18758298478422375
LOSS train 0.2746742882971036 valid 0.18766797282479025
LOSS train 0.2746742882971036 valid 0.18765165052120236
LOSS train 0.2746742882971036 valid 0.1876619699199277
LOSS train 0.2746742882971036 valid 0.18775690882969245
LOSS train 0.2746742882971036 valid 0.1878010080493052
LOSS train 0.2746742882971036 valid 0.18767541269106525
LOSS train 0.2746742882971036 valid 0.18759876956294863
LOSS train 0.2746742882971036 valid 0.1875606779511093
LOSS train 0.2746742882971036 valid 0.1876618054435447
LOSS train 0.2746742882971036 valid 0.18753357803527737
LOSS train 0.2746742882971036 valid 0.18756144747399447
LOSS train 0.2746742882971036 valid 0.18756002333614377
LOSS train 0.2746742882971036 valid 0.18759765073605114
LOSS train 0.2746742882971036 valid 0.18757188734081057
LOSS train 0.2746742882971036 valid 0.18756725992298456
LOSS train 0.2746742882971036 valid 0.187502819813531
LOSS train 0.2746742882971036 valid 0.18752627873543612
LOSS train 0.2746742882971036 valid 0.18753798627485968
LOSS train 0.2746742882971036 valid 0.18750752579840377
LOSS train 0.2746742882971036 valid 0.1874292849683437
LOSS train 0.2746742882971036 valid 0.1874651355258489
LOSS train 0.2746742882971036 valid 0.187468037260948
LOSS train 0.2746742882971036 valid 0.18750182366130327
LOSS train 0.2746742882971036 valid 0.18756956952130235
LOSS train 0.2746742882971036 valid 0.18751791239183482
LOSS train 0.2746742882971036 valid 0.187472815712293
LOSS train 0.2746742882971036 valid 0.18743476410244786
LOSS train 0.2746742882971036 valid 0.18746426612730846
LOSS train 0.2746742882971036 valid 0.18739487137338115
LOSS train 0.2746742882971036 valid 0.1874220460947407
LOSS train 0.2746742882971036 valid 0.1873883133540388
LOSS train 0.2746742882971036 valid 0.18755801957027585
LOSS train 0.2746742882971036 valid 0.1875285813008535
LOSS train 0.2746742882971036 valid 0.18763981536998378
LOSS train 0.2746742882971036 valid 0.1876962281353651
LOSS train 0.2746742882971036 valid 0.18776203259345023
LOSS train 0.2746742882971036 valid 0.18778332818742732
LOSS train 0.2746742882971036 valid 0.18776414438317984
LOSS train 0.2746742882971036 valid 0.18788968983549661
LOSS train 0.2746742882971036 valid 0.18787745623641713
LOSS train 0.2746742882971036 valid 0.1878955315975916
LOSS train 0.2746742882971036 valid 0.1879398095645482
LOSS train 0.2746742882971036 valid 0.18792181440896416
LOSS train 0.2746742882971036 valid 0.18792384755124086
LOSS train 0.2746742882971036 valid 0.18789427131881534
LOSS train 0.2746742882971036 valid 0.1878061753232032
LOSS train 0.2746742882971036 valid 0.1878746812960069
LOSS train 0.2746742882971036 valid 0.1878527884620317
LOSS train 0.2746742882971036 valid 0.18772952646109342
LOSS train 0.2746742882971036 valid 0.18769002765601064
LOSS train 0.2746742882971036 valid 0.18753686285935917
LOSS train 0.2746742882971036 valid 0.18764299154281616
LOSS train 0.2746742882971036 valid 0.18764234359293538
LOSS train 0.2746742882971036 valid 0.18768105337896
LOSS train 0.2746742882971036 valid 0.18761895411282686
LOSS train 0.2746742882971036 valid 0.18760972799676837
LOSS train 0.2746742882971036 valid 0.18755984580948995
LOSS train 0.2746742882971036 valid 0.1875210102782192
LOSS train 0.2746742882971036 valid 0.18759680120973615
LOSS train 0.2746742882971036 valid 0.18752859813902906
LOSS train 0.2746742882971036 valid 0.18750384672364193
LOSS train 0.2746742882971036 valid 0.18744770410869802
LOSS train 0.2746742882971036 valid 0.18757138814699756
LOSS train 0.2746742882971036 valid 0.18750793649952793
LOSS train 0.2746742882971036 valid 0.1874912190841714
LOSS train 0.2746742882971036 valid 0.18755717452834633
LOSS train 0.2746742882971036 valid 0.1875607179931182
LOSS train 0.2746742882971036 valid 0.18750300367324674
LOSS train 0.2746742882971036 valid 0.1874390601590493
LOSS train 0.2746742882971036 valid 0.18744232626848442
LOSS train 0.2746742882971036 valid 0.18747320909431014
LOSS train 0.2746742882971036 valid 0.1874041432417886
LOSS train 0.2746742882971036 valid 0.18734198903144264
LOSS train 0.2746742882971036 valid 0.18736998539888997
LOSS train 0.2746742882971036 valid 0.18729141077032063
LOSS train 0.2746742882971036 valid 0.18730122753552028
LOSS train 0.2746742882971036 valid 0.18735764619292017
LOSS train 0.2746742882971036 valid 0.1874013147316873
LOSS train 0.2746742882971036 valid 0.18747328002290753
LOSS train 0.2746742882971036 valid 0.18742643031528441
LOSS train 0.2746742882971036 valid 0.18732753467391913
LOSS train 0.2746742882971036 valid 0.1873270697640569
LOSS train 0.2746742882971036 valid 0.1874217487015978
LOSS train 0.2746742882971036 valid 0.18744775382333628
LOSS train 0.2746742882971036 valid 0.1874505455779498
LOSS train 0.2746742882971036 valid 0.18755467012524604
LOSS train 0.2746742882971036 valid 0.18750162567128104
LOSS train 0.2746742882971036 valid 0.18758439412597794
LOSS train 0.2746742882971036 valid 0.18763952860326477
LOSS train 0.2746742882971036 valid 0.18757299882370038
LOSS train 0.2746742882971036 valid 0.1876594416082722
LOSS train 0.2746742882971036 valid 0.18767191367885455
LOSS train 0.2746742882971036 valid 0.18765680678696334
LOSS train 0.2746742882971036 valid 0.18757631880757603
LOSS train 0.2746742882971036 valid 0.18765856781949194
EPOCH 8:
  batch 1 loss: 0.32936930656433105
  batch 2 loss: 0.3350139558315277
  batch 3 loss: 0.3116294542948405
  batch 4 loss: 0.31175102293491364
  batch 5 loss: 0.31445773839950564
  batch 6 loss: 0.3067628393570582
  batch 7 loss: 0.3070052010672433
  batch 8 loss: 0.30128248035907745
  batch 9 loss: 0.2989669077926212
  batch 10 loss: 0.2934887707233429
  batch 11 loss: 0.2933961261402477
  batch 12 loss: 0.2908407151699066
  batch 13 loss: 0.2888217866420746
  batch 14 loss: 0.2883679526192801
  batch 15 loss: 0.288693634668986
  batch 16 loss: 0.2880723215639591
  batch 17 loss: 0.2879240880994236
  batch 18 loss: 0.2868691401349174
  batch 19 loss: 0.2866273130241193
  batch 20 loss: 0.2824312224984169
  batch 21 loss: 0.28497766455014545
  batch 22 loss: 0.2852431522174315
  batch 23 loss: 0.2821657489175382
  batch 24 loss: 0.28061150511105853
  batch 25 loss: 0.28060190320014955
  batch 26 loss: 0.28089232857410723
  batch 27 loss: 0.28069522314601475
  batch 28 loss: 0.28116705481495174
  batch 29 loss: 0.2805097298375491
  batch 30 loss: 0.28132330874602
  batch 31 loss: 0.28168789129103383
  batch 32 loss: 0.282117604278028
  batch 33 loss: 0.28311504197843146
  batch 34 loss: 0.28328994617742653
  batch 35 loss: 0.28424986515726364
  batch 36 loss: 0.2838451597425673
  batch 37 loss: 0.283754223907316
  batch 38 loss: 0.2850494580833535
  batch 39 loss: 0.28517048099102116
  batch 40 loss: 0.28512812703847884
  batch 41 loss: 0.2856247875748611
  batch 42 loss: 0.28570417917910074
  batch 43 loss: 0.28536692192388136
  batch 44 loss: 0.28515858406370337
  batch 45 loss: 0.285785534646776
  batch 46 loss: 0.2846155276764994
  batch 47 loss: 0.28401474179105557
  batch 48 loss: 0.2835808700571458
  batch 49 loss: 0.2840515575846847
  batch 50 loss: 0.28359618425369265
  batch 51 loss: 0.283362535869374
  batch 52 loss: 0.28294889285014224
  batch 53 loss: 0.282171849372252
  batch 54 loss: 0.2823854778651838
  batch 55 loss: 0.28271224011074414
  batch 56 loss: 0.28334733311619076
  batch 57 loss: 0.2827655929222442
  batch 58 loss: 0.28247845327032023
  batch 59 loss: 0.2829462365578797
  batch 60 loss: 0.28291897028684615
  batch 61 loss: 0.2831050763364698
  batch 62 loss: 0.28349343567125257
  batch 63 loss: 0.2834222487040928
  batch 64 loss: 0.2830741605721414
  batch 65 loss: 0.2827710559734931
  batch 66 loss: 0.28234409924709436
  batch 67 loss: 0.28210335035822287
  batch 68 loss: 0.2821283932117855
  batch 69 loss: 0.28245836150818976
  batch 70 loss: 0.2816250862819808
  batch 71 loss: 0.2815247180176453
  batch 72 loss: 0.2813486417548524
  batch 73 loss: 0.2807587783222329
  batch 74 loss: 0.28026866389287486
  batch 75 loss: 0.2798891198635101
  batch 76 loss: 0.27962781449681834
  batch 77 loss: 0.2792374197538797
  batch 78 loss: 0.2793225947863016
  batch 79 loss: 0.2792866297915012
  batch 80 loss: 0.27906534895300866
  batch 81 loss: 0.2788424289520876
  batch 82 loss: 0.27911860368600705
  batch 83 loss: 0.2787694180586252
  batch 84 loss: 0.27823670563243685
  batch 85 loss: 0.27811809427597944
  batch 86 loss: 0.27798673649166905
  batch 87 loss: 0.27759672684231024
  batch 88 loss: 0.2775014540688558
  batch 89 loss: 0.27721939997726613
  batch 90 loss: 0.2776095711522632
  batch 91 loss: 0.27751537698965806
  batch 92 loss: 0.2773251494635706
  batch 93 loss: 0.2770764677114384
  batch 94 loss: 0.2771766794488785
  batch 95 loss: 0.27744881447992825
  batch 96 loss: 0.2774759167805314
  batch 97 loss: 0.27791881038970556
  batch 98 loss: 0.2780003161454687
  batch 99 loss: 0.27780006780768884
  batch 100 loss: 0.2776910510659218
  batch 101 loss: 0.2774876562675627
  batch 102 loss: 0.27755029236569123
  batch 103 loss: 0.277433233932384
  batch 104 loss: 0.27772857363407427
  batch 105 loss: 0.2773582882824398
  batch 106 loss: 0.27768716089568046
  batch 107 loss: 0.27755636687033647
  batch 108 loss: 0.2771524802126266
  batch 109 loss: 0.2770658823054865
  batch 110 loss: 0.2769208511168307
  batch 111 loss: 0.27675931934300846
  batch 112 loss: 0.27668547883097616
  batch 113 loss: 0.27642418470530383
  batch 114 loss: 0.27641978951399787
  batch 115 loss: 0.27586763747360393
  batch 116 loss: 0.27572694022593824
  batch 117 loss: 0.27551337860078895
  batch 118 loss: 0.2755652657252247
  batch 119 loss: 0.2756702305138612
  batch 120 loss: 0.27521672981480755
  batch 121 loss: 0.2748731095189891
  batch 122 loss: 0.2746805178581691
  batch 123 loss: 0.2743118427148679
  batch 124 loss: 0.2742485995254209
  batch 125 loss: 0.27400687348842623
  batch 126 loss: 0.2741321792441701
  batch 127 loss: 0.2742736948522057
  batch 128 loss: 0.2741713864961639
  batch 129 loss: 0.2741925174644751
  batch 130 loss: 0.2742072899754231
  batch 131 loss: 0.2742548413176573
  batch 132 loss: 0.27413764923359407
  batch 133 loss: 0.2738799444938961
  batch 134 loss: 0.2737330897752918
  batch 135 loss: 0.27364964761115884
  batch 136 loss: 0.27374536072944894
  batch 137 loss: 0.2738670392410599
  batch 138 loss: 0.27392926585415134
  batch 139 loss: 0.2741463579934278
  batch 140 loss: 0.27443294748663905
  batch 141 loss: 0.27424689303053185
  batch 142 loss: 0.274292861701737
  batch 143 loss: 0.2745510350157331
  batch 144 loss: 0.27454589907493854
  batch 145 loss: 0.27457538699281625
  batch 146 loss: 0.274510434637331
  batch 147 loss: 0.2744260148531726
  batch 148 loss: 0.27447491864094864
  batch 149 loss: 0.2741782648251361
  batch 150 loss: 0.27393827935059867
  batch 151 loss: 0.27354591255945876
  batch 152 loss: 0.27345159453781026
  batch 153 loss: 0.27353856805103277
  batch 154 loss: 0.27345090078843104
  batch 155 loss: 0.27344476234528325
  batch 156 loss: 0.27330175577065885
  batch 157 loss: 0.273170220434286
  batch 158 loss: 0.27329808756520474
  batch 159 loss: 0.2735757882115226
  batch 160 loss: 0.2733611211180687
  batch 161 loss: 0.2731802195125485
  batch 162 loss: 0.2731150344566063
  batch 163 loss: 0.27286244042080604
  batch 164 loss: 0.27298998669153307
  batch 165 loss: 0.2730331955534039
  batch 166 loss: 0.27308740870780257
  batch 167 loss: 0.272978390405278
  batch 168 loss: 0.2727215320226692
  batch 169 loss: 0.2729355301377336
  batch 170 loss: 0.27294809695552374
  batch 171 loss: 0.27307591033957856
  batch 172 loss: 0.27310633590055067
  batch 173 loss: 0.27320678747458266
  batch 174 loss: 0.2733032448195863
  batch 175 loss: 0.2731958896773202
  batch 176 loss: 0.2734276130795479
  batch 177 loss: 0.2738004143291948
  batch 178 loss: 0.27356051009022786
  batch 179 loss: 0.27369403689267247
  batch 180 loss: 0.2737174035774337
  batch 181 loss: 0.2735757854926652
  batch 182 loss: 0.27348296155968865
  batch 183 loss: 0.27323219790810443
  batch 184 loss: 0.27297209268030914
  batch 185 loss: 0.27294145355353483
  batch 186 loss: 0.272843871546048
  batch 187 loss: 0.2730818140634241
  batch 188 loss: 0.2730131220627338
  batch 189 loss: 0.27297189106386177
  batch 190 loss: 0.27294957810326625
  batch 191 loss: 0.2731456854892651
  batch 192 loss: 0.2729524925816804
  batch 193 loss: 0.27296988944006706
  batch 194 loss: 0.2729560171359593
  batch 195 loss: 0.2730809946090747
  batch 196 loss: 0.2728699708775598
  batch 197 loss: 0.2729691105143068
  batch 198 loss: 0.272973681188593
  batch 199 loss: 0.27293652056449624
  batch 200 loss: 0.27293400555849073
  batch 201 loss: 0.27275926786572186
  batch 202 loss: 0.2728185838844516
  batch 203 loss: 0.2727197039450331
  batch 204 loss: 0.2725875578528526
  batch 205 loss: 0.27246168779163826
  batch 206 loss: 0.27249939537164075
  batch 207 loss: 0.2724299410691008
  batch 208 loss: 0.27221893497671074
  batch 209 loss: 0.2721295664042377
  batch 210 loss: 0.27218338229826516
  batch 211 loss: 0.2720539921550389
  batch 212 loss: 0.27204747975997207
  batch 213 loss: 0.2718859955738408
  batch 214 loss: 0.2718029511030589
  batch 215 loss: 0.27170322579006817
  batch 216 loss: 0.2717563780369582
  batch 217 loss: 0.27148710967208933
  batch 218 loss: 0.2714005033357428
  batch 219 loss: 0.27113553718344807
  batch 220 loss: 0.27119059359485453
  batch 221 loss: 0.2710227126179777
  batch 222 loss: 0.27101904529709
  batch 223 loss: 0.27089871486206224
  batch 224 loss: 0.2709677715652755
  batch 225 loss: 0.271192446284824
  batch 226 loss: 0.27141398270573236
  batch 227 loss: 0.2713275880015369
  batch 228 loss: 0.27133710603964956
  batch 229 loss: 0.271445074976792
  batch 230 loss: 0.2715971516526264
  batch 231 loss: 0.27170590004879674
  batch 232 loss: 0.27155052119031037
  batch 233 loss: 0.27152973841955735
  batch 234 loss: 0.2715023529962597
  batch 235 loss: 0.27134836265381346
  batch 236 loss: 0.2713221819218943
  batch 237 loss: 0.2714079402167083
  batch 238 loss: 0.27135096177333545
  batch 239 loss: 0.2714086867027203
  batch 240 loss: 0.27133903863529363
  batch 241 loss: 0.2713234174795665
  batch 242 loss: 0.27124034053037976
  batch 243 loss: 0.27118567453980935
  batch 244 loss: 0.2712636390914682
  batch 245 loss: 0.2712768234768692
  batch 246 loss: 0.2713338231168142
  batch 247 loss: 0.2712661445382153
  batch 248 loss: 0.27134285723009416
  batch 249 loss: 0.27118788845089065
  batch 250 loss: 0.27102174854278566
  batch 251 loss: 0.27093016400755165
  batch 252 loss: 0.2707116576650786
  batch 253 loss: 0.2706761754784188
  batch 254 loss: 0.27084107403680097
  batch 255 loss: 0.2707101083853666
  batch 256 loss: 0.27066034550080076
  batch 257 loss: 0.27070073679031564
  batch 258 loss: 0.2705662629401037
  batch 259 loss: 0.27037039347366937
  batch 260 loss: 0.2702681488715685
  batch 261 loss: 0.2704174297066027
  batch 262 loss: 0.2704051176782783
  batch 263 loss: 0.27034803941675917
  batch 264 loss: 0.27059573567274847
  batch 265 loss: 0.2706720304938982
  batch 266 loss: 0.27056848095323804
  batch 267 loss: 0.2704599254140247
  batch 268 loss: 0.2704143186113728
  batch 269 loss: 0.2704331061653931
  batch 270 loss: 0.2704269328603038
  batch 271 loss: 0.2705162995635803
  batch 272 loss: 0.2704348775065121
  batch 273 loss: 0.27037174704965655
  batch 274 loss: 0.27026062431561687
  batch 275 loss: 0.27025686860084536
  batch 276 loss: 0.27027488942595496
  batch 277 loss: 0.27030017982751453
  batch 278 loss: 0.2702574229283298
  batch 279 loss: 0.27022575082317474
  batch 280 loss: 0.2700663244617837
  batch 281 loss: 0.27011469807276944
  batch 282 loss: 0.2700992480765843
  batch 283 loss: 0.27004124898160725
  batch 284 loss: 0.27000466361641884
  batch 285 loss: 0.2699596224646819
  batch 286 loss: 0.27000614516176547
  batch 287 loss: 0.2699490958493761
  batch 288 loss: 0.2699469403984646
  batch 289 loss: 0.2698436653737791
  batch 290 loss: 0.26973289857650623
  batch 291 loss: 0.2696597489601968
  batch 292 loss: 0.2695355962400567
  batch 293 loss: 0.2695006586789271
  batch 294 loss: 0.26944545296584665
  batch 295 loss: 0.2693545344522444
  batch 296 loss: 0.26940316032316236
  batch 297 loss: 0.26944778633840155
  batch 298 loss: 0.2693486417199941
  batch 299 loss: 0.26939409418050264
  batch 300 loss: 0.2693359060585499
  batch 301 loss: 0.26933869447224956
  batch 302 loss: 0.26940521573191445
  batch 303 loss: 0.26938919339439654
  batch 304 loss: 0.26943023870454025
  batch 305 loss: 0.26942925252875344
  batch 306 loss: 0.26964041527385024
  batch 307 loss: 0.26965603013962797
  batch 308 loss: 0.2696924402632497
  batch 309 loss: 0.26969971842943274
  batch 310 loss: 0.26989258542176214
  batch 311 loss: 0.2697731130951087
  batch 312 loss: 0.2697284523493204
  batch 313 loss: 0.2697671383333663
  batch 314 loss: 0.26975088210622217
  batch 315 loss: 0.2696882720977541
  batch 316 loss: 0.2695808272007145
  batch 317 loss: 0.26978423523978107
  batch 318 loss: 0.2696748254527836
  batch 319 loss: 0.26954467473172095
  batch 320 loss: 0.2697374154347926
  batch 321 loss: 0.26969912365032506
  batch 322 loss: 0.2695900524810234
  batch 323 loss: 0.26956760385088135
  batch 324 loss: 0.26966944925579023
  batch 325 loss: 0.2697424211868873
  batch 326 loss: 0.2699168448799227
  batch 327 loss: 0.2699067016624894
  batch 328 loss: 0.2701005603118641
  batch 329 loss: 0.27023023982903155
  batch 330 loss: 0.27024620879780165
  batch 331 loss: 0.2702455925797408
  batch 332 loss: 0.2704959374414869
  batch 333 loss: 0.27053068913854994
  batch 334 loss: 0.27067815365191705
  batch 335 loss: 0.2707253753249325
  batch 336 loss: 0.2707882279618865
  batch 337 loss: 0.27081264418381257
  batch 338 loss: 0.27078319451159977
  batch 339 loss: 0.270720261695814
  batch 340 loss: 0.27071003953323647
  batch 341 loss: 0.27066554400053894
  batch 342 loss: 0.2707295379879182
  batch 343 loss: 0.2707859837713464
  batch 344 loss: 0.27081808816035124
  batch 345 loss: 0.27085727060186693
  batch 346 loss: 0.27078671098789037
  batch 347 loss: 0.27061555596868314
  batch 348 loss: 0.27061574049722187
  batch 349 loss: 0.27072853917378753
  batch 350 loss: 0.2707733531509127
  batch 351 loss: 0.27080441879750655
  batch 352 loss: 0.27083565709604457
  batch 353 loss: 0.2708461821585809
  batch 354 loss: 0.27094830210599524
  batch 355 loss: 0.2708768554136787
  batch 356 loss: 0.27087472178293076
  batch 357 loss: 0.2707163536665486
  batch 358 loss: 0.27061398393947983
  batch 359 loss: 0.2706697844695248
  batch 360 loss: 0.270586296916008
  batch 361 loss: 0.27050779243915696
  batch 362 loss: 0.27040485959520655
  batch 363 loss: 0.2704337597632211
  batch 364 loss: 0.270379495121293
  batch 365 loss: 0.2703845788762994
  batch 366 loss: 0.27039766958991035
  batch 367 loss: 0.27050357549489357
  batch 368 loss: 0.27040928914009227
  batch 369 loss: 0.27038801463477335
  batch 370 loss: 0.27065670123776875
  batch 371 loss: 0.27068914392727084
  batch 372 loss: 0.27073013570199733
  batch 373 loss: 0.2708387845083472
  batch 374 loss: 0.2708709050707001
  batch 375 loss: 0.27092758762836455
  batch 376 loss: 0.271001846985297
  batch 377 loss: 0.2710252009351943
  batch 378 loss: 0.27099065123884764
  batch 379 loss: 0.2710140203973549
  batch 380 loss: 0.2709705918635193
  batch 381 loss: 0.2709026881872513
  batch 382 loss: 0.2709075675116784
  batch 383 loss: 0.27077198448753853
  batch 384 loss: 0.27076868899166584
  batch 385 loss: 0.27068877998110535
  batch 386 loss: 0.2706384848359335
  batch 387 loss: 0.27064292933743744
  batch 388 loss: 0.27060020511451455
  batch 389 loss: 0.27053923556308207
  batch 390 loss: 0.2705909808476766
  batch 391 loss: 0.2704832627035468
  batch 392 loss: 0.27057083194353143
  batch 393 loss: 0.27064401768242735
  batch 394 loss: 0.2706509688632742
  batch 395 loss: 0.2706697789174092
  batch 396 loss: 0.27069924041779353
  batch 397 loss: 0.2707285572509622
  batch 398 loss: 0.2706988986263323
  batch 399 loss: 0.27063462171787606
  batch 400 loss: 0.27080210220068696
  batch 401 loss: 0.27082867333270666
  batch 402 loss: 0.27083130814691087
  batch 403 loss: 0.270739636446347
  batch 404 loss: 0.2706904443405052
  batch 405 loss: 0.27075825388784763
  batch 406 loss: 0.27071753727831865
  batch 407 loss: 0.270749901927074
  batch 408 loss: 0.2706941598113261
  batch 409 loss: 0.2707477800347694
  batch 410 loss: 0.2707141564023204
  batch 411 loss: 0.2706789437799268
  batch 412 loss: 0.2707274635367602
  batch 413 loss: 0.27076296363991054
  batch 414 loss: 0.2707639592497245
  batch 415 loss: 0.27066208777657474
  batch 416 loss: 0.2707068162898605
  batch 417 loss: 0.2706327178566862
  batch 418 loss: 0.27059588103630894
  batch 419 loss: 0.2705935217386783
  batch 420 loss: 0.2706271318097909
  batch 421 loss: 0.27067264868499546
  batch 422 loss: 0.27060770794254907
  batch 423 loss: 0.27062190892989474
  batch 424 loss: 0.27068538953251436
  batch 425 loss: 0.27056665904381694
  batch 426 loss: 0.27044857545218
  batch 427 loss: 0.2704281610473816
  batch 428 loss: 0.27039498289194064
  batch 429 loss: 0.2703259094750687
  batch 430 loss: 0.27035673641881275
  batch 431 loss: 0.27030901168311
  batch 432 loss: 0.27028226628209706
  batch 433 loss: 0.27017677931532297
  batch 434 loss: 0.27005867261765737
  batch 435 loss: 0.2701534288368006
  batch 436 loss: 0.27015377499094795
  batch 437 loss: 0.27006629982311064
  batch 438 loss: 0.27003529940021637
  batch 439 loss: 0.26991875948286814
  batch 440 loss: 0.26994406018744815
  batch 441 loss: 0.26993389140451307
  batch 442 loss: 0.2699423958947756
  batch 443 loss: 0.26983016228164564
  batch 444 loss: 0.2699091773111004
  batch 445 loss: 0.26990121519297694
  batch 446 loss: 0.26993406530586594
  batch 447 loss: 0.2699276161873901
  batch 448 loss: 0.26998650059769197
  batch 449 loss: 0.26997312661135914
  batch 450 loss: 0.2698938874072499
  batch 451 loss: 0.2698585770677304
  batch 452 loss: 0.2697425899384296
  batch 453 loss: 0.2696828241182479
  batch 454 loss: 0.26960122549192495
  batch 455 loss: 0.26955293141878567
  batch 456 loss: 0.26955841592790786
  batch 457 loss: 0.26955256335584027
  batch 458 loss: 0.26953955411130165
  batch 459 loss: 0.2694861879470821
  batch 460 loss: 0.2694778042966905
  batch 461 loss: 0.2695084402010394
  batch 462 loss: 0.26952371739850933
  batch 463 loss: 0.26948619754602793
  batch 464 loss: 0.2693990275970307
  batch 465 loss: 0.26941167362915575
  batch 466 loss: 0.2693436659328927
  batch 467 loss: 0.2693763642035409
  batch 468 loss: 0.26939105809244335
  batch 469 loss: 0.2692798077742428
  batch 470 loss: 0.26940676642859235
  batch 471 loss: 0.26945701687578943
  batch 472 loss: 0.26927799491559046
LOSS train 0.26927799491559046 valid 0.20513233542442322
LOSS train 0.26927799491559046 valid 0.2025664821267128
LOSS train 0.26927799491559046 valid 0.2228470096985499
LOSS train 0.26927799491559046 valid 0.21551010385155678
LOSS train 0.26927799491559046 valid 0.2238577514886856
LOSS train 0.26927799491559046 valid 0.22527432441711426
LOSS train 0.26927799491559046 valid 0.2170177698135376
LOSS train 0.26927799491559046 valid 0.21755517460405827
LOSS train 0.26927799491559046 valid 0.2188628249698215
LOSS train 0.26927799491559046 valid 0.2167287990450859
LOSS train 0.26927799491559046 valid 0.21609606526114725
LOSS train 0.26927799491559046 valid 0.21834193418423334
LOSS train 0.26927799491559046 valid 0.21936175456413856
LOSS train 0.26927799491559046 valid 0.21702002095324652
LOSS train 0.26927799491559046 valid 0.21635351479053497
LOSS train 0.26927799491559046 valid 0.21952079329639673
LOSS train 0.26927799491559046 valid 0.21929685683811412
LOSS train 0.26927799491559046 valid 0.21907787356111738
LOSS train 0.26927799491559046 valid 0.21967555033533195
LOSS train 0.26927799491559046 valid 0.21960130333900452
LOSS train 0.26927799491559046 valid 0.22116728056044804
LOSS train 0.26927799491559046 valid 0.21958063339645212
LOSS train 0.26927799491559046 valid 0.2181695142517919
LOSS train 0.26927799491559046 valid 0.21805945225059986
LOSS train 0.26927799491559046 valid 0.21817823648452758
LOSS train 0.26927799491559046 valid 0.21749715163157538
LOSS train 0.26927799491559046 valid 0.21783585515287188
LOSS train 0.26927799491559046 valid 0.21811900128211295
LOSS train 0.26927799491559046 valid 0.216991922464864
LOSS train 0.26927799491559046 valid 0.21601892163356146
LOSS train 0.26927799491559046 valid 0.21563110909154337
LOSS train 0.26927799491559046 valid 0.21622176887467504
LOSS train 0.26927799491559046 valid 0.21559868662646323
LOSS train 0.26927799491559046 valid 0.21505202835097031
LOSS train 0.26927799491559046 valid 0.2156399096761431
LOSS train 0.26927799491559046 valid 0.21642821985814306
LOSS train 0.26927799491559046 valid 0.21580690024672328
LOSS train 0.26927799491559046 valid 0.21543445987136742
LOSS train 0.26927799491559046 valid 0.2148818660240907
LOSS train 0.26927799491559046 valid 0.2152255740016699
LOSS train 0.26927799491559046 valid 0.2149368811671327
LOSS train 0.26927799491559046 valid 0.21660750997917994
LOSS train 0.26927799491559046 valid 0.21710914893205777
LOSS train 0.26927799491559046 valid 0.21637762168591673
LOSS train 0.26927799491559046 valid 0.2162821527984407
LOSS train 0.26927799491559046 valid 0.21582980972269308
LOSS train 0.26927799491559046 valid 0.2155987465001167
LOSS train 0.26927799491559046 valid 0.21761707123368979
LOSS train 0.26927799491559046 valid 0.21727823937425808
LOSS train 0.26927799491559046 valid 0.21828333407640457
LOSS train 0.26927799491559046 valid 0.21802069772692287
LOSS train 0.26927799491559046 valid 0.218180505415568
LOSS train 0.26927799491559046 valid 0.21914153588267993
LOSS train 0.26927799491559046 valid 0.2193085606451388
LOSS train 0.26927799491559046 valid 0.21897408935156734
LOSS train 0.26927799491559046 valid 0.2189796787819692
LOSS train 0.26927799491559046 valid 0.2181863097245233
LOSS train 0.26927799491559046 valid 0.21878647778568597
LOSS train 0.26927799491559046 valid 0.218773743106147
LOSS train 0.26927799491559046 valid 0.2189272604882717
LOSS train 0.26927799491559046 valid 0.21906779046918526
LOSS train 0.26927799491559046 valid 0.218781242687856
LOSS train 0.26927799491559046 valid 0.21855496366818747
LOSS train 0.26927799491559046 valid 0.21836501103825867
LOSS train 0.26927799491559046 valid 0.21722655502649454
LOSS train 0.26927799491559046 valid 0.2170739429015102
LOSS train 0.26927799491559046 valid 0.2179756513727245
LOSS train 0.26927799491559046 valid 0.21735944927615278
LOSS train 0.26927799491559046 valid 0.2178673996873524
LOSS train 0.26927799491559046 valid 0.2183015233704022
LOSS train 0.26927799491559046 valid 0.21878767412313274
LOSS train 0.26927799491559046 valid 0.21891549126141602
LOSS train 0.26927799491559046 valid 0.2195692921746267
LOSS train 0.26927799491559046 valid 0.21970293791712942
LOSS train 0.26927799491559046 valid 0.21947341084480285
LOSS train 0.26927799491559046 valid 0.21959400314249491
LOSS train 0.26927799491559046 valid 0.21987305388048098
LOSS train 0.26927799491559046 valid 0.21968213048500893
LOSS train 0.26927799491559046 valid 0.21961853266516818
LOSS train 0.26927799491559046 valid 0.2194557273760438
LOSS train 0.26927799491559046 valid 0.21944428961954
LOSS train 0.26927799491559046 valid 0.21933486112734166
LOSS train 0.26927799491559046 valid 0.21916852065597672
LOSS train 0.26927799491559046 valid 0.2189578218829064
LOSS train 0.26927799491559046 valid 0.21928524462615742
LOSS train 0.26927799491559046 valid 0.21924178926057594
LOSS train 0.26927799491559046 valid 0.21865996410106792
LOSS train 0.26927799491559046 valid 0.21856793693520807
LOSS train 0.26927799491559046 valid 0.21871921687983395
LOSS train 0.26927799491559046 valid 0.21877414501375622
LOSS train 0.26927799491559046 valid 0.21896026930311224
LOSS train 0.26927799491559046 valid 0.21881072935850723
LOSS train 0.26927799491559046 valid 0.21880393931942602
LOSS train 0.26927799491559046 valid 0.2190933386062054
LOSS train 0.26927799491559046 valid 0.21929323045830978
LOSS train 0.26927799491559046 valid 0.21979067598779997
LOSS train 0.26927799491559046 valid 0.21976529906705483
LOSS train 0.26927799491559046 valid 0.21995637018461617
LOSS train 0.26927799491559046 valid 0.22011873231391715
LOSS train 0.26927799491559046 valid 0.22005826398730277
LOSS train 0.26927799491559046 valid 0.22007301331746695
LOSS train 0.26927799491559046 valid 0.22041908052622103
LOSS train 0.26927799491559046 valid 0.2200474184982985
LOSS train 0.26927799491559046 valid 0.21981656422408727
LOSS train 0.26927799491559046 valid 0.22013226080508458
LOSS train 0.26927799491559046 valid 0.22027093523515845
LOSS train 0.26927799491559046 valid 0.2200098429049287
LOSS train 0.26927799491559046 valid 0.2198885986926379
LOSS train 0.26927799491559046 valid 0.21926447448380496
LOSS train 0.26927799491559046 valid 0.2190349971706217
LOSS train 0.26927799491559046 valid 0.21917089444022994
LOSS train 0.26927799491559046 valid 0.21921305411628314
LOSS train 0.26927799491559046 valid 0.21907489858897386
LOSS train 0.26927799491559046 valid 0.21900110704857007
LOSS train 0.26927799491559046 valid 0.21943393971609032
LOSS train 0.26927799491559046 valid 0.21919125408448023
LOSS train 0.26927799491559046 valid 0.21957541008790335
LOSS train 0.26927799491559046 valid 0.2197546465164524
LOSS train 0.26927799491559046 valid 0.21943140493220642
LOSS train 0.26927799491559046 valid 0.21918305680155753
LOSS train 0.26927799491559046 valid 0.21912950696038805
LOSS train 0.26927799491559046 valid 0.2192845571725095
LOSS train 0.26927799491559046 valid 0.2193035414306129
LOSS train 0.26927799491559046 valid 0.2192618343378267
LOSS train 0.26927799491559046 valid 0.21924612128734589
LOSS train 0.26927799491559046 valid 0.21943817867173088
LOSS train 0.26927799491559046 valid 0.21936125408007404
LOSS train 0.26927799491559046 valid 0.21926048514433205
LOSS train 0.26927799491559046 valid 0.219028069991474
LOSS train 0.26927799491559046 valid 0.21869867123090303
LOSS train 0.26927799491559046 valid 0.2186772260274596
LOSS train 0.26927799491559046 valid 0.21862472288987853
LOSS train 0.26927799491559046 valid 0.21857045120314547
LOSS train 0.26927799491559046 valid 0.21871898701386666
LOSS train 0.26927799491559046 valid 0.2188895097485295
LOSS train 0.26927799491559046 valid 0.21910002838601084
LOSS train 0.26927799491559046 valid 0.21908481614868136
LOSS train 0.26927799491559046 valid 0.21903278765039166
LOSS train 0.26927799491559046 valid 0.21886059481034176
LOSS train 0.26927799491559046 valid 0.2190475000866822
LOSS train 0.26927799491559046 valid 0.21910788995999816
LOSS train 0.26927799491559046 valid 0.21933149577866137
LOSS train 0.26927799491559046 valid 0.21939977326176383
LOSS train 0.26927799491559046 valid 0.21957763760454124
LOSS train 0.26927799491559046 valid 0.2194470572060552
LOSS train 0.26927799491559046 valid 0.2194225545207115
LOSS train 0.26927799491559046 valid 0.21934240711789552
LOSS train 0.26927799491559046 valid 0.21905438531492208
LOSS train 0.26927799491559046 valid 0.21924368787131854
LOSS train 0.26927799491559046 valid 0.2191695439815521
LOSS train 0.26927799491559046 valid 0.218981880423249
LOSS train 0.26927799491559046 valid 0.2190291229635477
LOSS train 0.26927799491559046 valid 0.21883070614992403
LOSS train 0.26927799491559046 valid 0.2188855263513404
LOSS train 0.26927799491559046 valid 0.21896215465761001
LOSS train 0.26927799491559046 valid 0.21929277460544538
LOSS train 0.26927799491559046 valid 0.21923329790306698
LOSS train 0.26927799491559046 valid 0.21920812195992168
LOSS train 0.26927799491559046 valid 0.21934492717374046
LOSS train 0.26927799491559046 valid 0.21937797022983432
LOSS train 0.26927799491559046 valid 0.21946705387245793
LOSS train 0.26927799491559046 valid 0.2195644595740754
LOSS train 0.26927799491559046 valid 0.21933314231641454
LOSS train 0.26927799491559046 valid 0.21938648311103262
LOSS train 0.26927799491559046 valid 0.2195209101294026
LOSS train 0.26927799491559046 valid 0.21942341264831014
LOSS train 0.26927799491559046 valid 0.21947374663310137
LOSS train 0.26927799491559046 valid 0.21927779283197152
LOSS train 0.26927799491559046 valid 0.21905326561109553
LOSS train 0.26927799491559046 valid 0.2191494610379724
LOSS train 0.26927799491559046 valid 0.21899606984609749
LOSS train 0.26927799491559046 valid 0.21887580365982168
LOSS train 0.26927799491559046 valid 0.21881004306622323
LOSS train 0.26927799491559046 valid 0.21889844159970337
LOSS train 0.26927799491559046 valid 0.21882489579064504
LOSS train 0.26927799491559046 valid 0.21874203101139178
LOSS train 0.26927799491559046 valid 0.21876606505132665
LOSS train 0.26927799491559046 valid 0.21889599270365212
LOSS train 0.26927799491559046 valid 0.21902891294250276
LOSS train 0.26927799491559046 valid 0.21903220829036502
LOSS train 0.26927799491559046 valid 0.2188647714603013
LOSS train 0.26927799491559046 valid 0.21876540133258798
LOSS train 0.26927799491559046 valid 0.21874671254327388
LOSS train 0.26927799491559046 valid 0.2187103152922962
LOSS train 0.26927799491559046 valid 0.21876822025389284
LOSS train 0.26927799491559046 valid 0.21890685979717522
LOSS train 0.26927799491559046 valid 0.21871371933801925
LOSS train 0.26927799491559046 valid 0.21867703384858497
LOSS train 0.26927799491559046 valid 0.2185763102517557
LOSS train 0.26927799491559046 valid 0.21851503409837422
LOSS train 0.26927799491559046 valid 0.21870484386439099
LOSS train 0.26927799491559046 valid 0.21877891849726439
LOSS train 0.26927799491559046 valid 0.21861358095018357
LOSS train 0.26927799491559046 valid 0.2186442309433652
LOSS train 0.26927799491559046 valid 0.21857927426313742
LOSS train 0.26927799491559046 valid 0.2188809060928773
LOSS train 0.26927799491559046 valid 0.21882376558889594
LOSS train 0.26927799491559046 valid 0.21902777450253266
LOSS train 0.26927799491559046 valid 0.21898235785002684
LOSS train 0.26927799491559046 valid 0.21899433568120003
LOSS train 0.26927799491559046 valid 0.2190573765567286
LOSS train 0.26927799491559046 valid 0.21918952509318249
LOSS train 0.26927799491559046 valid 0.21940159856392244
LOSS train 0.26927799491559046 valid 0.21932012733875536
LOSS train 0.26927799491559046 valid 0.21924143978735297
LOSS train 0.26927799491559046 valid 0.21929031183708061
LOSS train 0.26927799491559046 valid 0.21927838178648465
LOSS train 0.26927799491559046 valid 0.21911111989846596
LOSS train 0.26927799491559046 valid 0.21913056824195898
LOSS train 0.26927799491559046 valid 0.21916635632514953
LOSS train 0.26927799491559046 valid 0.2194012485409235
LOSS train 0.26927799491559046 valid 0.2193097285926342
LOSS train 0.26927799491559046 valid 0.2193968415680066
LOSS train 0.26927799491559046 valid 0.2193308321532802
LOSS train 0.26927799491559046 valid 0.21902623564697976
LOSS train 0.26927799491559046 valid 0.21880815523090186
LOSS train 0.26927799491559046 valid 0.21857175285915076
LOSS train 0.26927799491559046 valid 0.21854845017468164
LOSS train 0.26927799491559046 valid 0.21859606733060863
LOSS train 0.26927799491559046 valid 0.21853706667369063
LOSS train 0.26927799491559046 valid 0.21840076755344598
LOSS train 0.26927799491559046 valid 0.2185061676679431
LOSS train 0.26927799491559046 valid 0.2184164916987911
LOSS train 0.26927799491559046 valid 0.2182617232735668
LOSS train 0.26927799491559046 valid 0.21809291137589348
LOSS train 0.26927799491559046 valid 0.21800834982268577
LOSS train 0.26927799491559046 valid 0.21794466503653756
LOSS train 0.26927799491559046 valid 0.21785794433794523
LOSS train 0.26927799491559046 valid 0.2177563322683609
LOSS train 0.26927799491559046 valid 0.21777164074389832
LOSS train 0.26927799491559046 valid 0.2177994627973218
LOSS train 0.26927799491559046 valid 0.2178605597723147
LOSS train 0.26927799491559046 valid 0.2180988180407127
LOSS train 0.26927799491559046 valid 0.21800337356125188
LOSS train 0.26927799491559046 valid 0.2179639605131555
LOSS train 0.26927799491559046 valid 0.21780600032563938
LOSS train 0.26927799491559046 valid 0.21785822026588747
LOSS train 0.26927799491559046 valid 0.21796445348182647
LOSS train 0.26927799491559046 valid 0.21789480502635367
LOSS train 0.26927799491559046 valid 0.2175939771036307
LOSS train 0.26927799491559046 valid 0.21746212911803692
LOSS train 0.26927799491559046 valid 0.21752963387522817
LOSS train 0.26927799491559046 valid 0.21752346947850515
LOSS train 0.26927799491559046 valid 0.21751184511135835
LOSS train 0.26927799491559046 valid 0.2175436558163896
LOSS train 0.26927799491559046 valid 0.2175388940708424
LOSS train 0.26927799491559046 valid 0.21758986888868123
LOSS train 0.26927799491559046 valid 0.2176678976825168
LOSS train 0.26927799491559046 valid 0.2175207437880068
LOSS train 0.26927799491559046 valid 0.21769234746694566
LOSS train 0.26927799491559046 valid 0.21788754613988429
LOSS train 0.26927799491559046 valid 0.21792886382530605
LOSS train 0.26927799491559046 valid 0.2178582588556727
LOSS train 0.26927799491559046 valid 0.2179683799466749
LOSS train 0.26927799491559046 valid 0.21788717321321077
LOSS train 0.26927799491559046 valid 0.21781912411097437
LOSS train 0.26927799491559046 valid 0.21785280265001009
LOSS train 0.26927799491559046 valid 0.2178969294872395
LOSS train 0.26927799491559046 valid 0.21782525735242025
LOSS train 0.26927799491559046 valid 0.2176825429384525
LOSS train 0.26927799491559046 valid 0.21769475782739706
LOSS train 0.26927799491559046 valid 0.21790133069717246
LOSS train 0.26927799491559046 valid 0.21778699483708283
LOSS train 0.26927799491559046 valid 0.21785045223254146
LOSS train 0.26927799491559046 valid 0.21778354925929375
LOSS train 0.26927799491559046 valid 0.2177849099376148
LOSS train 0.26927799491559046 valid 0.21773397364419944
LOSS train 0.26927799491559046 valid 0.2178378921391359
LOSS train 0.26927799491559046 valid 0.21795998729737717
LOSS train 0.26927799491559046 valid 0.2178465434798488
LOSS train 0.26927799491559046 valid 0.217801330439279
LOSS train 0.26927799491559046 valid 0.21765573021462736
LOSS train 0.26927799491559046 valid 0.21761395653962215
LOSS train 0.26927799491559046 valid 0.21755392447005223
LOSS train 0.26927799491559046 valid 0.21765473371202296
LOSS train 0.26927799491559046 valid 0.21763220483410187
LOSS train 0.26927799491559046 valid 0.21763624509103893
LOSS train 0.26927799491559046 valid 0.21774802056791112
LOSS train 0.26927799491559046 valid 0.21776879015361963
LOSS train 0.26927799491559046 valid 0.2176165643015078
LOSS train 0.26927799491559046 valid 0.2175134926822262
LOSS train 0.26927799491559046 valid 0.2174670121348496
LOSS train 0.26927799491559046 valid 0.2175691988674575
LOSS train 0.26927799491559046 valid 0.21742064904578975
LOSS train 0.26927799491559046 valid 0.21744727000855563
LOSS train 0.26927799491559046 valid 0.21745644056505256
LOSS train 0.26927799491559046 valid 0.2174908337069721
LOSS train 0.26927799491559046 valid 0.21745681969655883
LOSS train 0.26927799491559046 valid 0.21745050500008475
LOSS train 0.26927799491559046 valid 0.2173872774017268
LOSS train 0.26927799491559046 valid 0.21742599757061792
LOSS train 0.26927799491559046 valid 0.2174043911573005
LOSS train 0.26927799491559046 valid 0.21738589760996782
LOSS train 0.26927799491559046 valid 0.2172771114475873
LOSS train 0.26927799491559046 valid 0.21732136920347053
LOSS train 0.26927799491559046 valid 0.21730786083719214
LOSS train 0.26927799491559046 valid 0.21735536450087423
LOSS train 0.26927799491559046 valid 0.21740824504186643
LOSS train 0.26927799491559046 valid 0.21734163479063423
LOSS train 0.26927799491559046 valid 0.21729343608021737
LOSS train 0.26927799491559046 valid 0.21725154427990961
LOSS train 0.26927799491559046 valid 0.2172898327475352
LOSS train 0.26927799491559046 valid 0.21720064187994098
LOSS train 0.26927799491559046 valid 0.21721585482162864
LOSS train 0.26927799491559046 valid 0.21718666900376804
LOSS train 0.26927799491559046 valid 0.21740592465875974
LOSS train 0.26927799491559046 valid 0.21736856843633062
LOSS train 0.26927799491559046 valid 0.21750129783501873
LOSS train 0.26927799491559046 valid 0.21755990014685783
LOSS train 0.26927799491559046 valid 0.21761923735180208
LOSS train 0.26927799491559046 valid 0.21767085507943315
LOSS train 0.26927799491559046 valid 0.2176529852530131
LOSS train 0.26927799491559046 valid 0.21778121604896583
LOSS train 0.26927799491559046 valid 0.2177772143748915
LOSS train 0.26927799491559046 valid 0.2177871087713847
LOSS train 0.26927799491559046 valid 0.21781408866938157
LOSS train 0.26927799491559046 valid 0.21778828704770808
LOSS train 0.26927799491559046 valid 0.21780373999532662
LOSS train 0.26927799491559046 valid 0.21778472384502148
LOSS train 0.26927799491559046 valid 0.21769904550164937
LOSS train 0.26927799491559046 valid 0.2177731595306753
LOSS train 0.26927799491559046 valid 0.21774124460560934
LOSS train 0.26927799491559046 valid 0.2176006234744016
LOSS train 0.26927799491559046 valid 0.21755537740242334
LOSS train 0.26927799491559046 valid 0.21740673028505766
LOSS train 0.26927799491559046 valid 0.21751088223940024
LOSS train 0.26927799491559046 valid 0.21750801286748425
LOSS train 0.26927799491559046 valid 0.21754798211339044
LOSS train 0.26927799491559046 valid 0.2174755816311097
LOSS train 0.26927799491559046 valid 0.21747708690888953
LOSS train 0.26927799491559046 valid 0.2174258506730244
LOSS train 0.26927799491559046 valid 0.21737768604274255
LOSS train 0.26927799491559046 valid 0.21746382135170716
LOSS train 0.26927799491559046 valid 0.2173946976304768
LOSS train 0.26927799491559046 valid 0.21735884151351986
LOSS train 0.26927799491559046 valid 0.2172873549252039
LOSS train 0.26927799491559046 valid 0.21742656017800113
LOSS train 0.26927799491559046 valid 0.21734946816277928
LOSS train 0.26927799491559046 valid 0.21733066140726254
LOSS train 0.26927799491559046 valid 0.21740819421761176
LOSS train 0.26927799491559046 valid 0.21740904363019725
LOSS train 0.26927799491559046 valid 0.21733979331819633
LOSS train 0.26927799491559046 valid 0.21728350313342348
LOSS train 0.26927799491559046 valid 0.2173175410649111
LOSS train 0.26927799491559046 valid 0.21735957826393237
LOSS train 0.26927799491559046 valid 0.21726430125663734
LOSS train 0.26927799491559046 valid 0.2171995500666264
LOSS train 0.26927799491559046 valid 0.21721803268481946
LOSS train 0.26927799491559046 valid 0.21712495800758844
LOSS train 0.26927799491559046 valid 0.21713401087692805
LOSS train 0.26927799491559046 valid 0.2171899483584271
LOSS train 0.26927799491559046 valid 0.21724518227644943
LOSS train 0.26927799491559046 valid 0.21733224873164558
LOSS train 0.26927799491559046 valid 0.21729036235371552
LOSS train 0.26927799491559046 valid 0.21721278424833862
LOSS train 0.26927799491559046 valid 0.21719672585303865
LOSS train 0.26927799491559046 valid 0.21728526510778262
LOSS train 0.26927799491559046 valid 0.2173180847361101
LOSS train 0.26927799491559046 valid 0.2173391963280011
LOSS train 0.26927799491559046 valid 0.2174808213280307
LOSS train 0.26927799491559046 valid 0.21741825466010709
LOSS train 0.26927799491559046 valid 0.21752362760061716
LOSS train 0.26927799491559046 valid 0.21756521938753523
LOSS train 0.26927799491559046 valid 0.21749220596073748
LOSS train 0.26927799491559046 valid 0.21760015997984639
LOSS train 0.26927799491559046 valid 0.21761736281908275
LOSS train 0.26927799491559046 valid 0.2175999713366298
LOSS train 0.26927799491559046 valid 0.2174988010412325
LOSS train 0.26927799491559046 valid 0.2175946254636537
EPOCH 9:
  batch 1 loss: 0.28542935848236084
  batch 2 loss: 0.3006274104118347
  batch 3 loss: 0.2874222795168559
  batch 4 loss: 0.288483127951622
  batch 5 loss: 0.2935454249382019
  batch 6 loss: 0.28620824466149014
  batch 7 loss: 0.28809043977941784
  batch 8 loss: 0.28366936929523945
  batch 9 loss: 0.2831551482280095
  batch 10 loss: 0.27748225778341296
  batch 11 loss: 0.2784237414598465
  batch 12 loss: 0.27700797592600185
  batch 13 loss: 0.275575476197096
  batch 14 loss: 0.27518443018198013
  batch 15 loss: 0.27456973294417064
  batch 16 loss: 0.2743111876770854
  batch 17 loss: 0.27352817181278677
  batch 18 loss: 0.27176647467745674
  batch 19 loss: 0.27264660047857386
  batch 20 loss: 0.26935217455029486
  batch 21 loss: 0.27223423903896693
  batch 22 loss: 0.2738599269227548
  batch 23 loss: 0.2709234376316485
  batch 24 loss: 0.2700080592185259
  batch 25 loss: 0.27053493201732637
  batch 26 loss: 0.27140039377487624
  batch 27 loss: 0.2712403762119788
  batch 28 loss: 0.2711350050355707
  batch 29 loss: 0.27064183919594204
  batch 30 loss: 0.27175443222125373
  batch 31 loss: 0.2720565377704559
  batch 32 loss: 0.27268345141783357
  batch 33 loss: 0.27375085471254407
  batch 34 loss: 0.27384420834919987
  batch 35 loss: 0.2752238303422928
  batch 36 loss: 0.2749680375887288
  batch 37 loss: 0.274756236253558
  batch 38 loss: 0.2758017774475248
  batch 39 loss: 0.27605864291007703
  batch 40 loss: 0.2759479697793722
  batch 41 loss: 0.27616690389993714
  batch 42 loss: 0.27631292066403795
  batch 43 loss: 0.27609502194925795
  batch 44 loss: 0.27573214640671556
  batch 45 loss: 0.2755519492758645
  batch 46 loss: 0.2744635521717694
  batch 47 loss: 0.27403130524970115
  batch 48 loss: 0.27354350841293734
  batch 49 loss: 0.27403801953306006
  batch 50 loss: 0.2738701769709587
  batch 51 loss: 0.273658114028912
  batch 52 loss: 0.27305458743984884
  batch 53 loss: 0.2728893028677635
  batch 54 loss: 0.2730123960861453
  batch 55 loss: 0.2729413002729416
  batch 56 loss: 0.2743311979408775
  batch 57 loss: 0.2741003520133203
  batch 58 loss: 0.2735595351149296
  batch 59 loss: 0.2745303817219653
  batch 60 loss: 0.27460396314660707
  batch 61 loss: 0.27522376692685924
  batch 62 loss: 0.2758002961354871
  batch 63 loss: 0.27602611765975044
  batch 64 loss: 0.2762427448760718
  batch 65 loss: 0.27582319172529074
  batch 66 loss: 0.275406152009964
  batch 67 loss: 0.27521254277941004
  batch 68 loss: 0.2754640460890882
  batch 69 loss: 0.2759580499884011
  batch 70 loss: 0.2752468764781952
  batch 71 loss: 0.2754372694122959
  batch 72 loss: 0.2754378670619594
  batch 73 loss: 0.2749707443665152
  batch 74 loss: 0.2744793382448119
  batch 75 loss: 0.27413174450397493
  batch 76 loss: 0.2741744343779589
  batch 77 loss: 0.2738006107218854
  batch 78 loss: 0.27352981727856857
  batch 79 loss: 0.2739256262024747
  batch 80 loss: 0.27330416925251483
  batch 81 loss: 0.27291589349876216
  batch 82 loss: 0.2729932778492207
  batch 83 loss: 0.272592472563307
  batch 84 loss: 0.2718569354287216
  batch 85 loss: 0.2716618308249642
  batch 86 loss: 0.2716070668295372
  batch 87 loss: 0.2712091822391269
  batch 88 loss: 0.27117210000076075
  batch 89 loss: 0.2708248629998625
  batch 90 loss: 0.27121425767739615
  batch 91 loss: 0.2709931908073006
  batch 92 loss: 0.2707780651424242
  batch 93 loss: 0.2705244627050174
  batch 94 loss: 0.27064809964058245
  batch 95 loss: 0.27087180426246243
  batch 96 loss: 0.2709485248972972
  batch 97 loss: 0.2713505091126432
  batch 98 loss: 0.27133645725493527
  batch 99 loss: 0.27114417035170274
  batch 100 loss: 0.2710585138201714
  batch 101 loss: 0.2708223341125073
  batch 102 loss: 0.2707724539088268
  batch 103 loss: 0.27059441660214395
  batch 104 loss: 0.27079536851782066
  batch 105 loss: 0.27056712508201597
  batch 106 loss: 0.2708330174099724
  batch 107 loss: 0.27094772903718684
  batch 108 loss: 0.27063940810384574
  batch 109 loss: 0.2707663401277787
  batch 110 loss: 0.27073565301570024
  batch 111 loss: 0.2705558180003553
  batch 112 loss: 0.2704363349559052
  batch 113 loss: 0.27010946002154224
  batch 114 loss: 0.2699925989696854
  batch 115 loss: 0.26944859377715896
  batch 116 loss: 0.2693528888811325
  batch 117 loss: 0.26903069898104054
  batch 118 loss: 0.2691086163207636
  batch 119 loss: 0.26917320101701914
  batch 120 loss: 0.26862553047637144
  batch 121 loss: 0.2682716478247288
  batch 122 loss: 0.26807459150670004
  batch 123 loss: 0.2677686929218168
  batch 124 loss: 0.26757147908210754
  batch 125 loss: 0.2675486216545105
  batch 126 loss: 0.2677703790721439
  batch 127 loss: 0.2676825450630639
  batch 128 loss: 0.2678193883039057
  batch 129 loss: 0.2681191424990809
  batch 130 loss: 0.2679445749292007
  batch 131 loss: 0.26825827485277454
  batch 132 loss: 0.26822815666144545
  batch 133 loss: 0.26806951734356416
  batch 134 loss: 0.26799204300588636
  batch 135 loss: 0.2681641591919793
  batch 136 loss: 0.26838642512174216
  batch 137 loss: 0.2684731677065801
  batch 138 loss: 0.26856281273606897
  batch 139 loss: 0.26903257631569455
  batch 140 loss: 0.2694430276751518
  batch 141 loss: 0.2691636911943449
  batch 142 loss: 0.2692129311847015
  batch 143 loss: 0.2696108299118656
  batch 144 loss: 0.2696013200200266
  batch 145 loss: 0.2695114715345975
  batch 146 loss: 0.26944509781386755
  batch 147 loss: 0.26935279450449
  batch 148 loss: 0.269338407427878
  batch 149 loss: 0.268875977036937
  batch 150 loss: 0.268595849275589
  batch 151 loss: 0.2680761008862628
  batch 152 loss: 0.26792855266677706
  batch 153 loss: 0.2680252104802848
  batch 154 loss: 0.2676051120672907
  batch 155 loss: 0.2677052066210778
  batch 156 loss: 0.2675335043324874
  batch 157 loss: 0.2674715041544787
  batch 158 loss: 0.2676183388014383
  batch 159 loss: 0.26784390839015915
  batch 160 loss: 0.2677389114163816
  batch 161 loss: 0.2676168366433671
  batch 162 loss: 0.2675721465989395
  batch 163 loss: 0.26735129157092674
  batch 164 loss: 0.26756143978819613
  batch 165 loss: 0.2678870675238696
  batch 166 loss: 0.26801592030798094
  batch 167 loss: 0.26789665204322266
  batch 168 loss: 0.26771035071994576
  batch 169 loss: 0.26788721661243214
  batch 170 loss: 0.2678792747504571
  batch 171 loss: 0.2679562710704859
  batch 172 loss: 0.26804156887323355
  batch 173 loss: 0.26812232221137583
  batch 174 loss: 0.2681017445250489
  batch 175 loss: 0.26799896878855567
  batch 176 loss: 0.2682255571708083
  batch 177 loss: 0.2684626674416375
  batch 178 loss: 0.2682366209586015
  batch 179 loss: 0.26848610111787996
  batch 180 loss: 0.26847781663139664
  batch 181 loss: 0.268266500771375
  batch 182 loss: 0.26822869389594256
  batch 183 loss: 0.26808712773961446
  batch 184 loss: 0.26787201971139596
  batch 185 loss: 0.26775073835978636
  batch 186 loss: 0.2676760361239474
  batch 187 loss: 0.2678566327228903
  batch 188 loss: 0.2678043243891381
  batch 189 loss: 0.2678403191307865
  batch 190 loss: 0.2679046630075103
  batch 191 loss: 0.268119902276868
  batch 192 loss: 0.26793660029458505
  batch 193 loss: 0.2679973347353812
  batch 194 loss: 0.2680526126137714
  batch 195 loss: 0.26825260909704063
  batch 196 loss: 0.26805686578154564
  batch 197 loss: 0.268133531896596
  batch 198 loss: 0.26827703333563274
  batch 199 loss: 0.2683704906522329
  batch 200 loss: 0.2683572099357843
  batch 201 loss: 0.26817722803917693
  batch 202 loss: 0.26820885943304196
  batch 203 loss: 0.2681219106530908
  batch 204 loss: 0.2679019186864881
  batch 205 loss: 0.2677451894777577
  batch 206 loss: 0.26784383346444196
  batch 207 loss: 0.26770405025009947
  batch 208 loss: 0.26756655002156127
  batch 209 loss: 0.26746102889473927
  batch 210 loss: 0.2674494042992592
  batch 211 loss: 0.2672797740211984
  batch 212 loss: 0.26727720722556114
  batch 213 loss: 0.26711032843925586
  batch 214 loss: 0.26697054491421884
  batch 215 loss: 0.26683259987553887
  batch 216 loss: 0.26696914689684353
  batch 217 loss: 0.2666867761980004
  batch 218 loss: 0.2665404185242609
  batch 219 loss: 0.2663033677426647
  batch 220 loss: 0.26644343036142265
  batch 221 loss: 0.2662427550797009
  batch 222 loss: 0.2662806866673736
  batch 223 loss: 0.26622193570628827
  batch 224 loss: 0.26626420619764496
  batch 225 loss: 0.2663832453886668
  batch 226 loss: 0.266571480220398
  batch 227 loss: 0.2665447823801755
  batch 228 loss: 0.266578876266354
  batch 229 loss: 0.26662824143488856
  batch 230 loss: 0.2668098051910815
  batch 231 loss: 0.26689380729869333
  batch 232 loss: 0.2667211591555127
  batch 233 loss: 0.2666794722300231
  batch 234 loss: 0.2666978147676867
  batch 235 loss: 0.2665545889037721
  batch 236 loss: 0.26650429548600973
  batch 237 loss: 0.26660092237880967
  batch 238 loss: 0.26649947383073197
  batch 239 loss: 0.2666263688184227
  batch 240 loss: 0.266582750591139
  batch 241 loss: 0.26651312466478944
  batch 242 loss: 0.2664475011185181
  batch 243 loss: 0.26641987785390375
  batch 244 loss: 0.2664024679631483
  batch 245 loss: 0.26637455468275106
  batch 246 loss: 0.26648435154096867
  batch 247 loss: 0.26643166947461333
  batch 248 loss: 0.26650627750542855
  batch 249 loss: 0.26635745012137785
  batch 250 loss: 0.26618978530168536
  batch 251 loss: 0.2660713191645079
  batch 252 loss: 0.26591441591107656
  batch 253 loss: 0.26586497937266534
  batch 254 loss: 0.26595155272896837
  batch 255 loss: 0.2658310044045542
  batch 256 loss: 0.26586448843590915
  batch 257 loss: 0.26593014795028747
  batch 258 loss: 0.26582680446232937
  batch 259 loss: 0.26566315738620905
  batch 260 loss: 0.26560351327061654
  batch 261 loss: 0.2656898295856527
  batch 262 loss: 0.2657315054808864
  batch 263 loss: 0.26575610953818707
  batch 264 loss: 0.26588602832546737
  batch 265 loss: 0.2659348208386943
  batch 266 loss: 0.2658363023310676
  batch 267 loss: 0.2657139554117503
  batch 268 loss: 0.26572100957160566
  batch 269 loss: 0.26573489229696834
  batch 270 loss: 0.265729047578794
  batch 271 loss: 0.265812933060076
  batch 272 loss: 0.2657282786882099
  batch 273 loss: 0.2656688547723896
  batch 274 loss: 0.26555218438815026
  batch 275 loss: 0.2655594459988854
  batch 276 loss: 0.2655276594062646
  batch 277 loss: 0.26557789355624023
  batch 278 loss: 0.26554015809469084
  batch 279 loss: 0.26548180934776117
  batch 280 loss: 0.2652975567217384
  batch 281 loss: 0.2652810182541715
  batch 282 loss: 0.26526725783627086
  batch 283 loss: 0.2652153545997168
  batch 284 loss: 0.26513652655650194
  batch 285 loss: 0.26505315418828995
  batch 286 loss: 0.26508394248418876
  batch 287 loss: 0.26504166886366204
  batch 288 loss: 0.2649956488360961
  batch 289 loss: 0.264931821493129
  batch 290 loss: 0.2648148221188578
  batch 291 loss: 0.26473520056078936
  batch 292 loss: 0.2646647725917705
  batch 293 loss: 0.26467697637683296
  batch 294 loss: 0.26457090756925594
  batch 295 loss: 0.26444643382298744
  batch 296 loss: 0.2645110262406839
  batch 297 loss: 0.2645108506133661
  batch 298 loss: 0.2644125920714148
  batch 299 loss: 0.26445333350661604
  batch 300 loss: 0.264383930315574
  batch 301 loss: 0.2643734479564369
  batch 302 loss: 0.26448440102749315
  batch 303 loss: 0.26439848714339065
  batch 304 loss: 0.26446309435720505
  batch 305 loss: 0.264452334554469
  batch 306 loss: 0.26460938384525134
  batch 307 loss: 0.2645799278339268
  batch 308 loss: 0.2646223362293336
  batch 309 loss: 0.2645662074432404
  batch 310 loss: 0.26472697195506867
  batch 311 loss: 0.2646240995915373
  batch 312 loss: 0.26454768391946953
  batch 313 loss: 0.2645946689211903
  batch 314 loss: 0.26459504089727526
  batch 315 loss: 0.26456013203613343
  batch 316 loss: 0.2644850986573515
  batch 317 loss: 0.26464258726650985
  batch 318 loss: 0.26453661609370754
  batch 319 loss: 0.2644348351932992
  batch 320 loss: 0.26460865288972857
  batch 321 loss: 0.26456440785592217
  batch 322 loss: 0.26447472166015495
  batch 323 loss: 0.26442893366141956
  batch 324 loss: 0.2644389871921804
  batch 325 loss: 0.26457838292305286
  batch 326 loss: 0.26477027364486566
  batch 327 loss: 0.26470803448913294
  batch 328 loss: 0.2648314014985794
  batch 329 loss: 0.26500440685582377
  batch 330 loss: 0.26501682572292556
  batch 331 loss: 0.2649573678336475
  batch 332 loss: 0.2651176396084119
  batch 333 loss: 0.26522458673597454
  batch 334 loss: 0.26536477334841996
  batch 335 loss: 0.265376619527589
  batch 336 loss: 0.2654088918297064
  batch 337 loss: 0.2654432094592369
  batch 338 loss: 0.26541747505495533
  batch 339 loss: 0.26536695566852536
  batch 340 loss: 0.26533236424712575
  batch 341 loss: 0.26529234971119164
  batch 342 loss: 0.2653855393853104
  batch 343 loss: 0.26544333824958466
  batch 344 loss: 0.2654015667043453
  batch 345 loss: 0.2654651001743648
  batch 346 loss: 0.26543523672688213
  batch 347 loss: 0.2652634727078831
  batch 348 loss: 0.26517020255841056
  batch 349 loss: 0.2653041897410991
  batch 350 loss: 0.26537567619766506
  batch 351 loss: 0.26541138896745153
  batch 352 loss: 0.2653839028101753
  batch 353 loss: 0.2654625365230922
  batch 354 loss: 0.26559628777753164
  batch 355 loss: 0.26549866963440266
  batch 356 loss: 0.2655010518900464
  batch 357 loss: 0.2653952673787162
  batch 358 loss: 0.2653096282865082
  batch 359 loss: 0.265311059721001
  batch 360 loss: 0.2652570527046919
  batch 361 loss: 0.265194111790023
  batch 362 loss: 0.26507697736031444
  batch 363 loss: 0.2650620903686387
  batch 364 loss: 0.26503028925303573
  batch 365 loss: 0.26500796520546693
  batch 366 loss: 0.2649799741682459
  batch 367 loss: 0.2650793713672285
  batch 368 loss: 0.2650144743206708
  batch 369 loss: 0.2649931881808976
  batch 370 loss: 0.26512432589724255
  batch 371 loss: 0.26520051901552555
  batch 372 loss: 0.2652457993357412
  batch 373 loss: 0.26530900813299596
  batch 374 loss: 0.26533381840124487
  batch 375 loss: 0.2654115672111511
  batch 376 loss: 0.2655449407373337
  batch 377 loss: 0.2655366859164099
  batch 378 loss: 0.2654824838357628
  batch 379 loss: 0.2655073828543082
  batch 380 loss: 0.26555512159278516
  batch 381 loss: 0.2654995142709552
  batch 382 loss: 0.2654655698628326
  batch 383 loss: 0.26534728616398245
  batch 384 loss: 0.2654128208135565
  batch 385 loss: 0.26541123676609685
  batch 386 loss: 0.26534282273270304
  batch 387 loss: 0.2653894104082763
  batch 388 loss: 0.265418960492021
  batch 389 loss: 0.26536292753213475
  batch 390 loss: 0.26537224715336777
  batch 391 loss: 0.26525421017576056
  batch 392 loss: 0.26536169351667777
  batch 393 loss: 0.26536474041356384
  batch 394 loss: 0.26539564329355503
  batch 395 loss: 0.26542036450361906
  batch 396 loss: 0.265466206979872
  batch 397 loss: 0.26544857550628237
  batch 398 loss: 0.26544753630556656
  batch 399 loss: 0.2654789711598466
  batch 400 loss: 0.2655924529582262
  batch 401 loss: 0.2656163539077873
  batch 402 loss: 0.2656716483445903
  batch 403 loss: 0.26563759299720785
  batch 404 loss: 0.26560616596500475
  batch 405 loss: 0.2656650614591292
  batch 406 loss: 0.26565107314163827
  batch 407 loss: 0.2656516792293849
  batch 408 loss: 0.2656329816170767
  batch 409 loss: 0.26569935234951214
  batch 410 loss: 0.26564109329043367
  batch 411 loss: 0.2656009863255378
  batch 412 loss: 0.2656552133701959
  batch 413 loss: 0.2656492280974515
  batch 414 loss: 0.26566716765867915
  batch 415 loss: 0.26559000977550645
  batch 416 loss: 0.26565194280388266
  batch 417 loss: 0.2655915971711385
  batch 418 loss: 0.26555912631550477
  batch 419 loss: 0.26555069647996127
  batch 420 loss: 0.2655787901509376
  batch 421 loss: 0.26563185779045995
  batch 422 loss: 0.2655584971416053
  batch 423 loss: 0.2655698894820315
  batch 424 loss: 0.265625105819331
  batch 425 loss: 0.2655130415804246
  batch 426 loss: 0.2654064605493501
  batch 427 loss: 0.2653618641284925
  batch 428 loss: 0.2652959654751782
  batch 429 loss: 0.2652113534909584
  batch 430 loss: 0.2652392945317335
  batch 431 loss: 0.2652264130226144
  batch 432 loss: 0.26520799121095073
  batch 433 loss: 0.2650997108730378
  batch 434 loss: 0.2649820223107316
  batch 435 loss: 0.265078753301467
  batch 436 loss: 0.2651137765667854
  batch 437 loss: 0.26503886722318093
  batch 438 loss: 0.265017624927438
  batch 439 loss: 0.26495160674994517
  batch 440 loss: 0.26497108502821487
  batch 441 loss: 0.2649774126995718
  batch 442 loss: 0.2649502333472757
  batch 443 loss: 0.26486813904334944
  batch 444 loss: 0.26499564237557016
  batch 445 loss: 0.2649209780639477
  batch 446 loss: 0.26497545517613535
  batch 447 loss: 0.26501156313040647
  batch 448 loss: 0.2650080492853054
  batch 449 loss: 0.265027472330361
  batch 450 loss: 0.2649498736527231
  batch 451 loss: 0.26489016450032954
  batch 452 loss: 0.2647859227340833
  batch 453 loss: 0.2647407732854616
  batch 454 loss: 0.26468728121681884
  batch 455 loss: 0.26465986633038785
  batch 456 loss: 0.2646179582204735
  batch 457 loss: 0.2646605176790016
  batch 458 loss: 0.2645951017216824
  batch 459 loss: 0.26454200496600866
  batch 460 loss: 0.26454938682525053
  batch 461 loss: 0.26452667089967047
  batch 462 loss: 0.2645195278125408
  batch 463 loss: 0.26448846507020957
  batch 464 loss: 0.2643595263683077
  batch 465 loss: 0.26436611167205276
  batch 466 loss: 0.26432651015949865
  batch 467 loss: 0.2643357954073769
  batch 468 loss: 0.26429846605811363
  batch 469 loss: 0.264214347452243
  batch 470 loss: 0.2643329107063882
  batch 471 loss: 0.26431625053254915
  batch 472 loss: 0.2641828300715503
LOSS train 0.2641828300715503 valid 0.18189428746700287
LOSS train 0.2641828300715503 valid 0.18456680327653885
LOSS train 0.2641828300715503 valid 0.20339451730251312
LOSS train 0.2641828300715503 valid 0.19489514827728271
LOSS train 0.2641828300715503 valid 0.2033923238515854
LOSS train 0.2641828300715503 valid 0.20490896701812744
LOSS train 0.2641828300715503 valid 0.19685539390359605
LOSS train 0.2641828300715503 valid 0.19760124757885933
LOSS train 0.2641828300715503 valid 0.19901434083779654
LOSS train 0.2641828300715503 valid 0.19663772583007813
LOSS train 0.2641828300715503 valid 0.19582034241069446
LOSS train 0.2641828300715503 valid 0.19829903915524483
LOSS train 0.2641828300715503 valid 0.1996910572052002
LOSS train 0.2641828300715503 valid 0.19785763429743902
LOSS train 0.2641828300715503 valid 0.19712161123752595
LOSS train 0.2641828300715503 valid 0.20054943952709436
LOSS train 0.2641828300715503 valid 0.19999582714894237
LOSS train 0.2641828300715503 valid 0.19984248446093666
LOSS train 0.2641828300715503 valid 0.20068127701157018
LOSS train 0.2641828300715503 valid 0.20108507350087165
LOSS train 0.2641828300715503 valid 0.2024403406041009
LOSS train 0.2641828300715503 valid 0.2007109665057876
LOSS train 0.2641828300715503 valid 0.19923805348251178
LOSS train 0.2641828300715503 valid 0.19905843089024225
LOSS train 0.2641828300715503 valid 0.19898479342460632
LOSS train 0.2641828300715503 valid 0.1981520618383701
LOSS train 0.2641828300715503 valid 0.1986633621984058
LOSS train 0.2641828300715503 valid 0.1989763996430806
LOSS train 0.2641828300715503 valid 0.19776727002242517
LOSS train 0.2641828300715503 valid 0.1967667172352473
LOSS train 0.2641828300715503 valid 0.19639363548448008
LOSS train 0.2641828300715503 valid 0.19707301631569862
LOSS train 0.2641828300715503 valid 0.1965188311808037
LOSS train 0.2641828300715503 valid 0.19601456163560643
LOSS train 0.2641828300715503 valid 0.19658453336783818
LOSS train 0.2641828300715503 valid 0.19745624024007055
LOSS train 0.2641828300715503 valid 0.19667194018492828
LOSS train 0.2641828300715503 valid 0.19635088584924998
LOSS train 0.2641828300715503 valid 0.1960139840077131
LOSS train 0.2641828300715503 valid 0.19637580066919327
LOSS train 0.2641828300715503 valid 0.19585959758700394
LOSS train 0.2641828300715503 valid 0.1977668564234461
LOSS train 0.2641828300715503 valid 0.19821442767631176
LOSS train 0.2641828300715503 valid 0.1975676447830417
LOSS train 0.2641828300715503 valid 0.19739462965064578
LOSS train 0.2641828300715503 valid 0.1970209161872449
LOSS train 0.2641828300715503 valid 0.19694555408142983
LOSS train 0.2641828300715503 valid 0.19888042068729797
LOSS train 0.2641828300715503 valid 0.19860694450991495
LOSS train 0.2641828300715503 valid 0.1995946940779686
LOSS train 0.2641828300715503 valid 0.19944899894443213
LOSS train 0.2641828300715503 valid 0.1994898425271878
LOSS train 0.2641828300715503 valid 0.20047332290208564
LOSS train 0.2641828300715503 valid 0.20050001944656726
LOSS train 0.2641828300715503 valid 0.20016345137899572
LOSS train 0.2641828300715503 valid 0.20006163897258894
LOSS train 0.2641828300715503 valid 0.19934002715244628
LOSS train 0.2641828300715503 valid 0.20004423245273786
LOSS train 0.2641828300715503 valid 0.19995910964779934
LOSS train 0.2641828300715503 valid 0.19990467503666878
LOSS train 0.2641828300715503 valid 0.2001560423217836
LOSS train 0.2641828300715503 valid 0.19992989734295877
LOSS train 0.2641828300715503 valid 0.19976739703662813
LOSS train 0.2641828300715503 valid 0.19969586725346744
LOSS train 0.2641828300715503 valid 0.1985390869470743
LOSS train 0.2641828300715503 valid 0.1984001330353997
LOSS train 0.2641828300715503 valid 0.19937545728327624
LOSS train 0.2641828300715503 valid 0.19869418157374158
LOSS train 0.2641828300715503 valid 0.19913712986137555
LOSS train 0.2641828300715503 valid 0.19961554280349186
LOSS train 0.2641828300715503 valid 0.19998701882194464
LOSS train 0.2641828300715503 valid 0.20017107017338276
LOSS train 0.2641828300715503 valid 0.20082324682033226
LOSS train 0.2641828300715503 valid 0.20093043088107496
LOSS train 0.2641828300715503 valid 0.2006527171532313
LOSS train 0.2641828300715503 valid 0.20069124726088425
LOSS train 0.2641828300715503 valid 0.20093560741319286
LOSS train 0.2641828300715503 valid 0.20080547626966086
LOSS train 0.2641828300715503 valid 0.20069656172130682
LOSS train 0.2641828300715503 valid 0.2004481667652726
LOSS train 0.2641828300715503 valid 0.20054146573867326
LOSS train 0.2641828300715503 valid 0.20041091013245466
LOSS train 0.2641828300715503 valid 0.20014779844197883
LOSS train 0.2641828300715503 valid 0.19989367858284995
LOSS train 0.2641828300715503 valid 0.2003028690814972
LOSS train 0.2641828300715503 valid 0.20023422376361005
LOSS train 0.2641828300715503 valid 0.1996833685485796
LOSS train 0.2641828300715503 valid 0.19955757852982392
LOSS train 0.2641828300715503 valid 0.19974605672145157
LOSS train 0.2641828300715503 valid 0.19996802359819413
LOSS train 0.2641828300715503 valid 0.2001598325076994
LOSS train 0.2641828300715503 valid 0.19994248174454854
LOSS train 0.2641828300715503 valid 0.1999855815402923
LOSS train 0.2641828300715503 valid 0.20022216138053447
LOSS train 0.2641828300715503 valid 0.20038543876848722
LOSS train 0.2641828300715503 valid 0.20085295289754868
LOSS train 0.2641828300715503 valid 0.20088082367611915
LOSS train 0.2641828300715503 valid 0.20104211325548133
LOSS train 0.2641828300715503 valid 0.2010867932830194
LOSS train 0.2641828300715503 valid 0.2010151517391205
LOSS train 0.2641828300715503 valid 0.20114766459653874
LOSS train 0.2641828300715503 valid 0.2015774702908946
LOSS train 0.2641828300715503 valid 0.20118482859389294
LOSS train 0.2641828300715503 valid 0.20098216803028032
LOSS train 0.2641828300715503 valid 0.20131206015745798
LOSS train 0.2641828300715503 valid 0.20148610126859737
LOSS train 0.2641828300715503 valid 0.2012077147994086
LOSS train 0.2641828300715503 valid 0.2011146788243894
LOSS train 0.2641828300715503 valid 0.20048560202121735
LOSS train 0.2641828300715503 valid 0.2002492449500344
LOSS train 0.2641828300715503 valid 0.200380708720233
LOSS train 0.2641828300715503 valid 0.2003410945513419
LOSS train 0.2641828300715503 valid 0.20018564837168804
LOSS train 0.2641828300715503 valid 0.20007197402025523
LOSS train 0.2641828300715503 valid 0.2005297585673954
LOSS train 0.2641828300715503 valid 0.2002627239401998
LOSS train 0.2641828300715503 valid 0.20062931926331967
LOSS train 0.2641828300715503 valid 0.20080642856783787
LOSS train 0.2641828300715503 valid 0.2005000785619271
LOSS train 0.2641828300715503 valid 0.20026669688522816
LOSS train 0.2641828300715503 valid 0.2001733044701174
LOSS train 0.2641828300715503 valid 0.2003774600195103
LOSS train 0.2641828300715503 valid 0.20041111329706704
LOSS train 0.2641828300715503 valid 0.20035310486151325
LOSS train 0.2641828300715503 valid 0.20032358574867248
LOSS train 0.2641828300715503 valid 0.2004900969683178
LOSS train 0.2641828300715503 valid 0.20039340384363188
LOSS train 0.2641828300715503 valid 0.20032398530747741
LOSS train 0.2641828300715503 valid 0.2000050994083863
LOSS train 0.2641828300715503 valid 0.1996665697831374
LOSS train 0.2641828300715503 valid 0.1996486268880713
LOSS train 0.2641828300715503 valid 0.19959819203976428
LOSS train 0.2641828300715503 valid 0.19948417407677585
LOSS train 0.2641828300715503 valid 0.1996055002977599
LOSS train 0.2641828300715503 valid 0.19984365591296444
LOSS train 0.2641828300715503 valid 0.20007427727036617
LOSS train 0.2641828300715503 valid 0.20005691192881034
LOSS train 0.2641828300715503 valid 0.20004843898873398
LOSS train 0.2641828300715503 valid 0.19992761120950575
LOSS train 0.2641828300715503 valid 0.20009540670684406
LOSS train 0.2641828300715503 valid 0.2001865044341865
LOSS train 0.2641828300715503 valid 0.2004581711871523
LOSS train 0.2641828300715503 valid 0.20049013155740458
LOSS train 0.2641828300715503 valid 0.2006220378809505
LOSS train 0.2641828300715503 valid 0.20055712214831647
LOSS train 0.2641828300715503 valid 0.20051812666327987
LOSS train 0.2641828300715503 valid 0.2004666291937536
LOSS train 0.2641828300715503 valid 0.2001838450496261
LOSS train 0.2641828300715503 valid 0.20033680342587848
LOSS train 0.2641828300715503 valid 0.20028881112734476
LOSS train 0.2641828300715503 valid 0.20005712011792012
LOSS train 0.2641828300715503 valid 0.2001328461460377
LOSS train 0.2641828300715503 valid 0.19990711625105415
LOSS train 0.2641828300715503 valid 0.19995735982408772
LOSS train 0.2641828300715503 valid 0.19998705887025403
LOSS train 0.2641828300715503 valid 0.20031791963638404
LOSS train 0.2641828300715503 valid 0.20026713087680234
LOSS train 0.2641828300715503 valid 0.2001943476026571
LOSS train 0.2641828300715503 valid 0.2003291610089488
LOSS train 0.2641828300715503 valid 0.20039392914623022
LOSS train 0.2641828300715503 valid 0.2004847038791787
LOSS train 0.2641828300715503 valid 0.2005741337751165
LOSS train 0.2641828300715503 valid 0.2003210515149532
LOSS train 0.2641828300715503 valid 0.20039192014714566
LOSS train 0.2641828300715503 valid 0.2005049899672017
LOSS train 0.2641828300715503 valid 0.20038269433271455
LOSS train 0.2641828300715503 valid 0.20044765848956422
LOSS train 0.2641828300715503 valid 0.20023933026407445
LOSS train 0.2641828300715503 valid 0.20001372086578573
LOSS train 0.2641828300715503 valid 0.20008691698312758
LOSS train 0.2641828300715503 valid 0.1998891951570734
LOSS train 0.2641828300715503 valid 0.19973131442485853
LOSS train 0.2641828300715503 valid 0.19972358876570112
LOSS train 0.2641828300715503 valid 0.1997768415630549
LOSS train 0.2641828300715503 valid 0.1997296918290002
LOSS train 0.2641828300715503 valid 0.19963776938278566
LOSS train 0.2641828300715503 valid 0.1996659836863394
LOSS train 0.2641828300715503 valid 0.19977356342787153
LOSS train 0.2641828300715503 valid 0.19996827524467553
LOSS train 0.2641828300715503 valid 0.1999737032585674
LOSS train 0.2641828300715503 valid 0.19985275736171237
LOSS train 0.2641828300715503 valid 0.19979830258167708
LOSS train 0.2641828300715503 valid 0.1997830269278073
LOSS train 0.2641828300715503 valid 0.19971899088958037
LOSS train 0.2641828300715503 valid 0.1997785299210935
LOSS train 0.2641828300715503 valid 0.19990286895985243
LOSS train 0.2641828300715503 valid 0.1997032030062242
LOSS train 0.2641828300715503 valid 0.19964301205695945
LOSS train 0.2641828300715503 valid 0.19956383637334935
LOSS train 0.2641828300715503 valid 0.19953546649531315
LOSS train 0.2641828300715503 valid 0.1997346649463264
LOSS train 0.2641828300715503 valid 0.19980586343444884
LOSS train 0.2641828300715503 valid 0.19962877629643277
LOSS train 0.2641828300715503 valid 0.199721771494015
LOSS train 0.2641828300715503 valid 0.19966735526537283
LOSS train 0.2641828300715503 valid 0.19994823398942851
LOSS train 0.2641828300715503 valid 0.19989583766097344
LOSS train 0.2641828300715503 valid 0.20015209484280963
LOSS train 0.2641828300715503 valid 0.20010325806823806
LOSS train 0.2641828300715503 valid 0.20009520709514617
LOSS train 0.2641828300715503 valid 0.2001383384661888
LOSS train 0.2641828300715503 valid 0.20027383763601284
LOSS train 0.2641828300715503 valid 0.2004577812596495
LOSS train 0.2641828300715503 valid 0.2003747839845863
LOSS train 0.2641828300715503 valid 0.2003312275177095
LOSS train 0.2641828300715503 valid 0.2003608408917501
LOSS train 0.2641828300715503 valid 0.2003182118065691
LOSS train 0.2641828300715503 valid 0.20017041416408923
LOSS train 0.2641828300715503 valid 0.20015907729641672
LOSS train 0.2641828300715503 valid 0.20017735965195158
LOSS train 0.2641828300715503 valid 0.20040911471391742
LOSS train 0.2641828300715503 valid 0.2002964974293169
LOSS train 0.2641828300715503 valid 0.2004430223378777
LOSS train 0.2641828300715503 valid 0.20036434904437198
LOSS train 0.2641828300715503 valid 0.20007862933846407
LOSS train 0.2641828300715503 valid 0.19987177904005404
LOSS train 0.2641828300715503 valid 0.19964954806362978
LOSS train 0.2641828300715503 valid 0.19962528969990004
LOSS train 0.2641828300715503 valid 0.19965477199314935
LOSS train 0.2641828300715503 valid 0.19956810196692293
LOSS train 0.2641828300715503 valid 0.19944876061575445
LOSS train 0.2641828300715503 valid 0.19955982838396555
LOSS train 0.2641828300715503 valid 0.19946800299289516
LOSS train 0.2641828300715503 valid 0.19934416748583317
LOSS train 0.2641828300715503 valid 0.19919382492701213
LOSS train 0.2641828300715503 valid 0.19905627387023606
LOSS train 0.2641828300715503 valid 0.1989789489607454
LOSS train 0.2641828300715503 valid 0.1988892779407794
LOSS train 0.2641828300715503 valid 0.19881165209816012
LOSS train 0.2641828300715503 valid 0.1988299460514732
LOSS train 0.2641828300715503 valid 0.1988646067453153
LOSS train 0.2641828300715503 valid 0.19890525776507526
LOSS train 0.2641828300715503 valid 0.19910585976209763
LOSS train 0.2641828300715503 valid 0.19898853992295062
LOSS train 0.2641828300715503 valid 0.198938210213438
LOSS train 0.2641828300715503 valid 0.19875517058170447
LOSS train 0.2641828300715503 valid 0.1988249007408126
LOSS train 0.2641828300715503 valid 0.1989244260833043
LOSS train 0.2641828300715503 valid 0.1988455595207015
LOSS train 0.2641828300715503 valid 0.19855455129096905
LOSS train 0.2641828300715503 valid 0.19841237087962044
LOSS train 0.2641828300715503 valid 0.19849588912873228
LOSS train 0.2641828300715503 valid 0.19847687674157413
LOSS train 0.2641828300715503 valid 0.1984765518029205
LOSS train 0.2641828300715503 valid 0.19849543881659604
LOSS train 0.2641828300715503 valid 0.19851338978463073
LOSS train 0.2641828300715503 valid 0.1985380541578478
LOSS train 0.2641828300715503 valid 0.19863175510639144
LOSS train 0.2641828300715503 valid 0.19847930584806037
LOSS train 0.2641828300715503 valid 0.1986414276957512
LOSS train 0.2641828300715503 valid 0.19886123676936465
LOSS train 0.2641828300715503 valid 0.1989207560462611
LOSS train 0.2641828300715503 valid 0.19886583119277426
LOSS train 0.2641828300715503 valid 0.19900123557941182
LOSS train 0.2641828300715503 valid 0.19891093916752758
LOSS train 0.2641828300715503 valid 0.19883650774136186
LOSS train 0.2641828300715503 valid 0.19884598829171074
LOSS train 0.2641828300715503 valid 0.19887455957111463
LOSS train 0.2641828300715503 valid 0.19875575314387392
LOSS train 0.2641828300715503 valid 0.19861046671867372
LOSS train 0.2641828300715503 valid 0.198630317132135
LOSS train 0.2641828300715503 valid 0.19886648672000143
LOSS train 0.2641828300715503 valid 0.1987582322762493
LOSS train 0.2641828300715503 valid 0.1987979623178641
LOSS train 0.2641828300715503 valid 0.19874487935372118
LOSS train 0.2641828300715503 valid 0.19875510787605344
LOSS train 0.2641828300715503 valid 0.1986782176329402
LOSS train 0.2641828300715503 valid 0.19879043880683273
LOSS train 0.2641828300715503 valid 0.19893632458045138
LOSS train 0.2641828300715503 valid 0.19886637881950095
LOSS train 0.2641828300715503 valid 0.19882124535931872
LOSS train 0.2641828300715503 valid 0.19869306349359891
LOSS train 0.2641828300715503 valid 0.19862898381856772
LOSS train 0.2641828300715503 valid 0.19856876042419977
LOSS train 0.2641828300715503 valid 0.19870302259922026
LOSS train 0.2641828300715503 valid 0.19865482754033545
LOSS train 0.2641828300715503 valid 0.1986444236354277
LOSS train 0.2641828300715503 valid 0.19877653783388274
LOSS train 0.2641828300715503 valid 0.19881087659080396
LOSS train 0.2641828300715503 valid 0.19866421797445843
LOSS train 0.2641828300715503 valid 0.19856869728963994
LOSS train 0.2641828300715503 valid 0.19852363460875572
LOSS train 0.2641828300715503 valid 0.1986070851871066
LOSS train 0.2641828300715503 valid 0.1984615764042861
LOSS train 0.2641828300715503 valid 0.19847330681064673
LOSS train 0.2641828300715503 valid 0.19847009763434217
LOSS train 0.2641828300715503 valid 0.19847346359429044
LOSS train 0.2641828300715503 valid 0.19845108770661885
LOSS train 0.2641828300715503 valid 0.19846662353066838
LOSS train 0.2641828300715503 valid 0.19841132446609694
LOSS train 0.2641828300715503 valid 0.19843514780817983
LOSS train 0.2641828300715503 valid 0.1984154773391273
LOSS train 0.2641828300715503 valid 0.1983745475474478
LOSS train 0.2641828300715503 valid 0.19827022499778643
LOSS train 0.2641828300715503 valid 0.19830060934616348
LOSS train 0.2641828300715503 valid 0.1983065398762355
LOSS train 0.2641828300715503 valid 0.19835037469663203
LOSS train 0.2641828300715503 valid 0.1984234800774779
LOSS train 0.2641828300715503 valid 0.1983552225756406
LOSS train 0.2641828300715503 valid 0.19830721139907836
LOSS train 0.2641828300715503 valid 0.19823074816073294
LOSS train 0.2641828300715503 valid 0.19826129666030012
LOSS train 0.2641828300715503 valid 0.19819861025896796
LOSS train 0.2641828300715503 valid 0.19822378201704277
LOSS train 0.2641828300715503 valid 0.1982092792382006
LOSS train 0.2641828300715503 valid 0.19844982607497108
LOSS train 0.2641828300715503 valid 0.19839504681503345
LOSS train 0.2641828300715503 valid 0.1985277897351748
LOSS train 0.2641828300715503 valid 0.19858957397899196
LOSS train 0.2641828300715503 valid 0.1986640964304247
LOSS train 0.2641828300715503 valid 0.1987079262637632
LOSS train 0.2641828300715503 valid 0.1986898170927396
LOSS train 0.2641828300715503 valid 0.19878007909550835
LOSS train 0.2641828300715503 valid 0.19873526474093176
LOSS train 0.2641828300715503 valid 0.19874442257578412
LOSS train 0.2641828300715503 valid 0.1987823213957533
LOSS train 0.2641828300715503 valid 0.19875287234031064
LOSS train 0.2641828300715503 valid 0.19874444327451898
LOSS train 0.2641828300715503 valid 0.19873461449594707
LOSS train 0.2641828300715503 valid 0.19864395703189075
LOSS train 0.2641828300715503 valid 0.19871089433398204
LOSS train 0.2641828300715503 valid 0.19868709734693077
LOSS train 0.2641828300715503 valid 0.19855393944509997
LOSS train 0.2641828300715503 valid 0.19853386558868266
LOSS train 0.2641828300715503 valid 0.1983612007361192
LOSS train 0.2641828300715503 valid 0.19847179940141782
LOSS train 0.2641828300715503 valid 0.19845834438224816
LOSS train 0.2641828300715503 valid 0.19850673917226674
LOSS train 0.2641828300715503 valid 0.19844884128737234
LOSS train 0.2641828300715503 valid 0.1984420006473859
LOSS train 0.2641828300715503 valid 0.19839141637176905
LOSS train 0.2641828300715503 valid 0.19834553921617656
LOSS train 0.2641828300715503 valid 0.19843853056967795
LOSS train 0.2641828300715503 valid 0.19837268490991192
LOSS train 0.2641828300715503 valid 0.19834631708130906
LOSS train 0.2641828300715503 valid 0.19830130448653585
LOSS train 0.2641828300715503 valid 0.19844784068070817
LOSS train 0.2641828300715503 valid 0.19838366683947264
LOSS train 0.2641828300715503 valid 0.1983612465911207
LOSS train 0.2641828300715503 valid 0.19844982970286817
LOSS train 0.2641828300715503 valid 0.19843378033805803
LOSS train 0.2641828300715503 valid 0.19837060091439743
LOSS train 0.2641828300715503 valid 0.19831689762130772
LOSS train 0.2641828300715503 valid 0.1983297533507264
LOSS train 0.2641828300715503 valid 0.198389455632887
LOSS train 0.2641828300715503 valid 0.19831144641761836
LOSS train 0.2641828300715503 valid 0.19824456412785335
LOSS train 0.2641828300715503 valid 0.19826671905044851
LOSS train 0.2641828300715503 valid 0.19817647683757084
LOSS train 0.2641828300715503 valid 0.1981951950277601
LOSS train 0.2641828300715503 valid 0.1982662332193804
LOSS train 0.2641828300715503 valid 0.19832287254658612
LOSS train 0.2641828300715503 valid 0.1984094792064121
LOSS train 0.2641828300715503 valid 0.19836954111601673
LOSS train 0.2641828300715503 valid 0.1982786795622866
LOSS train 0.2641828300715503 valid 0.19824862074148789
LOSS train 0.2641828300715503 valid 0.1983304631476309
LOSS train 0.2641828300715503 valid 0.19835713580666975
LOSS train 0.2641828300715503 valid 0.19835795042906632
LOSS train 0.2641828300715503 valid 0.1985174344645606
LOSS train 0.2641828300715503 valid 0.19847392371321648
LOSS train 0.2641828300715503 valid 0.1985810843620511
LOSS train 0.2641828300715503 valid 0.1986224240015361
LOSS train 0.2641828300715503 valid 0.19853493902873207
LOSS train 0.2641828300715503 valid 0.19863218673288005
LOSS train 0.2641828300715503 valid 0.19867921441467734
LOSS train 0.2641828300715503 valid 0.1986762585285899
LOSS train 0.2641828300715503 valid 0.19855257208742524
LOSS train 0.2641828300715503 valid 0.19862787954529448
EPOCH 10:
  batch 1 loss: 0.28858068585395813
  batch 2 loss: 0.3016519695520401
  batch 3 loss: 0.28570449352264404
  batch 4 loss: 0.28965459764003754
  batch 5 loss: 0.2930575251579285
  batch 6 loss: 0.2857924501101176
  batch 7 loss: 0.2847124380724771
  batch 8 loss: 0.2809191532433033
  batch 9 loss: 0.28101302186648053
  batch 10 loss: 0.2751946210861206
  batch 11 loss: 0.27585338733413
  batch 12 loss: 0.2730725482106209
  batch 13 loss: 0.27192829434688276
  batch 14 loss: 0.27217546318258556
  batch 15 loss: 0.2711611330509186
  batch 16 loss: 0.2716180142015219
  batch 17 loss: 0.27164558452718396
  batch 18 loss: 0.27017761684126324
  batch 19 loss: 0.2697178619472604
  batch 20 loss: 0.26643274798989297
  batch 21 loss: 0.2684166807503927
  batch 22 loss: 0.26894292303107004
  batch 23 loss: 0.2663062538789666
  batch 24 loss: 0.2660760146876176
  batch 25 loss: 0.26590786457061766
  batch 26 loss: 0.266266337954081
  batch 27 loss: 0.2670752063945488
  batch 28 loss: 0.26684131153992247
  batch 29 loss: 0.2659503612025031
  batch 30 loss: 0.2678939978281657
  batch 31 loss: 0.26845004770063585
  batch 32 loss: 0.2692171987146139
  batch 33 loss: 0.27067076256780914
  batch 34 loss: 0.27064769934205446
  batch 35 loss: 0.27163784759385246
  batch 36 loss: 0.2712695971131325
  batch 37 loss: 0.27099621054288503
  batch 38 loss: 0.2711098429403807
  batch 39 loss: 0.2712554274461208
  batch 40 loss: 0.270821488648653
  batch 41 loss: 0.2706429587631691
  batch 42 loss: 0.27050029521896723
  batch 43 loss: 0.2701616585254669
  batch 44 loss: 0.2699931433255022
  batch 45 loss: 0.26945659816265105
  batch 46 loss: 0.2681572187853896
  batch 47 loss: 0.2675684135644994
  batch 48 loss: 0.2669576893871029
  batch 49 loss: 0.26736526616982054
  batch 50 loss: 0.2672322836518288
  batch 51 loss: 0.26717038803240833
  batch 52 loss: 0.266453143209219
  batch 53 loss: 0.2657145506368493
  batch 54 loss: 0.26589203460348976
  batch 55 loss: 0.2653894448822195
  batch 56 loss: 0.266466944079314
  batch 57 loss: 0.2663352858079107
  batch 58 loss: 0.2654803937365269
  batch 59 loss: 0.26575055996240193
  batch 60 loss: 0.2655942258735498
  batch 61 loss: 0.26645611128846153
  batch 62 loss: 0.26689852221358207
  batch 63 loss: 0.2667509210961206
  batch 64 loss: 0.2668733161408454
  batch 65 loss: 0.2665869531723169
  batch 66 loss: 0.2663371122696183
  batch 67 loss: 0.2660684592243451
  batch 68 loss: 0.2662646312485723
  batch 69 loss: 0.26667835034322046
  batch 70 loss: 0.265849315055779
  batch 71 loss: 0.26596617929532496
  batch 72 loss: 0.26603239505655235
  batch 73 loss: 0.26565488560559
  batch 74 loss: 0.2652107003975559
  batch 75 loss: 0.2649539879957835
  batch 76 loss: 0.265136328966994
  batch 77 loss: 0.26488645219957674
  batch 78 loss: 0.2647964352598557
  batch 79 loss: 0.26482248853279067
  batch 80 loss: 0.2642049303278327
  batch 81 loss: 0.26398439098287513
  batch 82 loss: 0.2641568660009198
  batch 83 loss: 0.2638117495071457
  batch 84 loss: 0.2632052520556109
  batch 85 loss: 0.26315386733588053
  batch 86 loss: 0.26323691720879355
  batch 87 loss: 0.26284041332787483
  batch 88 loss: 0.26269809613850986
  batch 89 loss: 0.26244001224469604
  batch 90 loss: 0.2628892161779933
  batch 91 loss: 0.26260053440109715
  batch 92 loss: 0.26242083424459334
  batch 93 loss: 0.2621390527935438
  batch 94 loss: 0.26233209447657807
  batch 95 loss: 0.2624847233295441
  batch 96 loss: 0.26255637034773827
  batch 97 loss: 0.26300839081252975
  batch 98 loss: 0.26297157120947934
  batch 99 loss: 0.2628032766809367
  batch 100 loss: 0.26278538167476656
  batch 101 loss: 0.26261883339669445
  batch 102 loss: 0.26265746837153153
  batch 103 loss: 0.26250427512867935
  batch 104 loss: 0.2627518023722447
  batch 105 loss: 0.2624543314888364
  batch 106 loss: 0.26279226395319094
  batch 107 loss: 0.26294609402941765
  batch 108 loss: 0.2626147973868582
  batch 109 loss: 0.2626941190400255
  batch 110 loss: 0.26277568529952655
  batch 111 loss: 0.26260799773641536
  batch 112 loss: 0.26255284688834635
  batch 113 loss: 0.2622011947420846
  batch 114 loss: 0.26218942121455546
  batch 115 loss: 0.2616688638925552
  batch 116 loss: 0.2615808003164571
  batch 117 loss: 0.26133221451543337
  batch 118 loss: 0.2614948912444761
  batch 119 loss: 0.2614897413163626
  batch 120 loss: 0.2610438832392295
  batch 121 loss: 0.2607518700528736
  batch 122 loss: 0.26052202858397217
  batch 123 loss: 0.26041391891677207
  batch 124 loss: 0.26005102345539677
  batch 125 loss: 0.2599504479169846
  batch 126 loss: 0.2601660479392324
  batch 127 loss: 0.2600069645586915
  batch 128 loss: 0.26006715127732605
  batch 129 loss: 0.2604721912810969
  batch 130 loss: 0.2604019778279158
  batch 131 loss: 0.26060435915266283
  batch 132 loss: 0.2606456079943614
  batch 133 loss: 0.26056040240857836
  batch 134 loss: 0.26038101437821315
  batch 135 loss: 0.2604122677335033
  batch 136 loss: 0.26074466229799914
  batch 137 loss: 0.26109061939437894
  batch 138 loss: 0.26109544122996536
  batch 139 loss: 0.2611999141869785
  batch 140 loss: 0.2617465075637613
  batch 141 loss: 0.2616630086662076
  batch 142 loss: 0.26163069043360965
  batch 143 loss: 0.2619354041306289
  batch 144 loss: 0.2620912026613951
  batch 145 loss: 0.26211729892368973
  batch 146 loss: 0.2621680212347475
  batch 147 loss: 0.2621163421747636
  batch 148 loss: 0.2621253527499534
  batch 149 loss: 0.261721262395782
  batch 150 loss: 0.2614789303143819
  batch 151 loss: 0.26106359232340426
  batch 152 loss: 0.2609772550824441
  batch 153 loss: 0.2612701514187981
  batch 154 loss: 0.2609873534022988
  batch 155 loss: 0.2611030174839881
  batch 156 loss: 0.26113185439354336
  batch 157 loss: 0.2609108962640641
  batch 158 loss: 0.26104100190008744
  batch 159 loss: 0.26115481030641113
  batch 160 loss: 0.2609721305780113
  batch 161 loss: 0.26081231504863833
  batch 162 loss: 0.2607849476698004
  batch 163 loss: 0.2605899582428435
  batch 164 loss: 0.26074070092744944
  batch 165 loss: 0.26099743382497265
  batch 166 loss: 0.2610897051103144
  batch 167 loss: 0.26101300771721825
  batch 168 loss: 0.2607678663695142
  batch 169 loss: 0.2608672744600025
  batch 170 loss: 0.26093857034164314
  batch 171 loss: 0.2611305522814132
  batch 172 loss: 0.26115078071868697
  batch 173 loss: 0.261277785277091
  batch 174 loss: 0.261428559391663
  batch 175 loss: 0.2612886813708714
  batch 176 loss: 0.26150590367615223
  batch 177 loss: 0.26165585497678334
  batch 178 loss: 0.26151462954081844
  batch 179 loss: 0.2617232941049437
  batch 180 loss: 0.26173065669006773
  batch 181 loss: 0.2616653094963474
  batch 182 loss: 0.26157526491762517
  batch 183 loss: 0.26139266487679197
  batch 184 loss: 0.2612642081373412
  batch 185 loss: 0.2611683754502116
  batch 186 loss: 0.26110948566147074
  batch 187 loss: 0.2614411264657974
  batch 188 loss: 0.2613773080421255
  batch 189 loss: 0.2613601909270362
  batch 190 loss: 0.2613619336172154
  batch 191 loss: 0.26166590821992664
  batch 192 loss: 0.2614825456403196
  batch 193 loss: 0.26150364502106305
  batch 194 loss: 0.26149666186460513
  batch 195 loss: 0.2616834700107574
  batch 196 loss: 0.2614924877273793
  batch 197 loss: 0.26158534966144464
  batch 198 loss: 0.2616405119799604
  batch 199 loss: 0.2617054938071936
  batch 200 loss: 0.2616262324899435
  batch 201 loss: 0.2614618730337466
  batch 202 loss: 0.261436823939923
  batch 203 loss: 0.261332839653997
  batch 204 loss: 0.26126237418137344
  batch 205 loss: 0.26117340922355653
  batch 206 loss: 0.26118233988007294
  batch 207 loss: 0.2612684845636432
  batch 208 loss: 0.26120953247524226
  batch 209 loss: 0.26106147261327534
  batch 210 loss: 0.2610771050055822
  batch 211 loss: 0.26100036741997956
  batch 212 loss: 0.2609483995387014
  batch 213 loss: 0.2608042359352112
  batch 214 loss: 0.2607088767200987
  batch 215 loss: 0.26058482340601985
  batch 216 loss: 0.26064081731493827
  batch 217 loss: 0.2603734606696713
  batch 218 loss: 0.2602131728322134
  batch 219 loss: 0.2599460326781556
  batch 220 loss: 0.2599666598845612
  batch 221 loss: 0.2598404340884265
  batch 222 loss: 0.2598615792152044
  batch 223 loss: 0.25973389628489457
  batch 224 loss: 0.2597629173126604
  batch 225 loss: 0.2598453048202727
  batch 226 loss: 0.26007186239008356
  batch 227 loss: 0.2599907284397386
  batch 228 loss: 0.25997559654346686
  batch 229 loss: 0.2600543025929855
  batch 230 loss: 0.260258568175461
  batch 231 loss: 0.2603668676955359
  batch 232 loss: 0.26021759128519173
  batch 233 loss: 0.2600998465979048
  batch 234 loss: 0.26013053705294925
  batch 235 loss: 0.2599527303842788
  batch 236 loss: 0.2599008066295567
  batch 237 loss: 0.2598900803156543
  batch 238 loss: 0.2598088222767125
  batch 239 loss: 0.2598738506374
  batch 240 loss: 0.2598205040519436
  batch 241 loss: 0.2597564898462216
  batch 242 loss: 0.25963351740078494
  batch 243 loss: 0.25959897574819163
  batch 244 loss: 0.2596295336108716
  batch 245 loss: 0.25962728328850804
  batch 246 loss: 0.2597978207759741
  batch 247 loss: 0.2597786362475229
  batch 248 loss: 0.2598715929855262
  batch 249 loss: 0.2598681686274019
  batch 250 loss: 0.25966316324472427
  batch 251 loss: 0.25955625768914165
  batch 252 loss: 0.25938253642784226
  batch 253 loss: 0.25935089264226996
  batch 254 loss: 0.25947420269720195
  batch 255 loss: 0.2592919543093326
  batch 256 loss: 0.2592759641702287
  batch 257 loss: 0.2593591705486468
  batch 258 loss: 0.25922692255225294
  batch 259 loss: 0.2590781221863846
  batch 260 loss: 0.25901747706990974
  batch 261 loss: 0.2591597060476683
  batch 262 loss: 0.2591635006415935
  batch 263 loss: 0.25918519491251885
  batch 264 loss: 0.25935318879783154
  batch 265 loss: 0.2594586037802246
  batch 266 loss: 0.2593943314780866
  batch 267 loss: 0.25929499889134466
  batch 268 loss: 0.2592945784997584
  batch 269 loss: 0.2592902201259003
  batch 270 loss: 0.25924837009774315
  batch 271 loss: 0.25928688461710164
  batch 272 loss: 0.2591899492613533
  batch 273 loss: 0.25912713611518945
  batch 274 loss: 0.2590290725666241
  batch 275 loss: 0.2591144095767628
  batch 276 loss: 0.2590989881883497
  batch 277 loss: 0.2590908793335787
  batch 278 loss: 0.2590094705065377
  batch 279 loss: 0.25896279754177215
  batch 280 loss: 0.2587698255266462
  batch 281 loss: 0.2587824818078309
  batch 282 loss: 0.25883382484845235
  batch 283 loss: 0.2588336343478819
  batch 284 loss: 0.25874536741577403
  batch 285 loss: 0.25867912931400433
  batch 286 loss: 0.25875723367179193
  batch 287 loss: 0.2588072298191981
  batch 288 loss: 0.2588130930542118
  batch 289 loss: 0.2587712553030067
  batch 290 loss: 0.2587342832622857
  batch 291 loss: 0.25868781110675065
  batch 292 loss: 0.2586124374543967
  batch 293 loss: 0.2586477969998793
  batch 294 loss: 0.25856983666720035
  batch 295 loss: 0.2584856335894536
  batch 296 loss: 0.2585293460838698
  batch 297 loss: 0.2585649663450742
  batch 298 loss: 0.2585390451670493
  batch 299 loss: 0.2586089676140144
  batch 300 loss: 0.25855318292975427
  batch 301 loss: 0.2585400598884817
  batch 302 loss: 0.2586686780417202
  batch 303 loss: 0.25863191293607846
  batch 304 loss: 0.25866294421843794
  batch 305 loss: 0.25862716802808106
  batch 306 loss: 0.258844260345487
  batch 307 loss: 0.2588565307350811
  batch 308 loss: 0.25891601028186934
  batch 309 loss: 0.25888128433991403
  batch 310 loss: 0.25907470867518456
  batch 311 loss: 0.2589836880516776
  batch 312 loss: 0.25894856529358107
  batch 313 loss: 0.25895863266798635
  batch 314 loss: 0.25898873919893983
  batch 315 loss: 0.25897174553265645
  batch 316 loss: 0.2589134080217609
  batch 317 loss: 0.259056657317685
  batch 318 loss: 0.25897668625386255
  batch 319 loss: 0.2589815853624882
  batch 320 loss: 0.25912778326310215
  batch 321 loss: 0.25902087016269054
  batch 322 loss: 0.2590211063438321
  batch 323 loss: 0.2590030148486973
  batch 324 loss: 0.2589863585653128
  batch 325 loss: 0.2590697654394003
  batch 326 loss: 0.2592905645538693
  batch 327 loss: 0.2592847265234781
  batch 328 loss: 0.2593028103796447
  batch 329 loss: 0.2594862949884409
  batch 330 loss: 0.2595659330035701
  batch 331 loss: 0.2595324659455579
  batch 332 loss: 0.25967286688735686
  batch 333 loss: 0.2597333353023987
  batch 334 loss: 0.2598739329747811
  batch 335 loss: 0.259973308933315
  batch 336 loss: 0.26000934777160484
  batch 337 loss: 0.26001344625603195
  batch 338 loss: 0.26002600409928156
  batch 339 loss: 0.26010062543340134
  batch 340 loss: 0.2601671146119342
  batch 341 loss: 0.26010375751777826
  batch 342 loss: 0.2601780214330606
  batch 343 loss: 0.2603168608322088
  batch 344 loss: 0.26034681576975555
  batch 345 loss: 0.26036449955857316
  batch 346 loss: 0.2603307972753668
  batch 347 loss: 0.26023639356368555
  batch 348 loss: 0.26023281519782954
  batch 349 loss: 0.260307324308379
  batch 350 loss: 0.26035339968545096
  batch 351 loss: 0.2605351708044014
  batch 352 loss: 0.26052561597051943
  batch 353 loss: 0.2605393396389721
  batch 354 loss: 0.26070235156070043
  batch 355 loss: 0.2606629969368518
  batch 356 loss: 0.2606526788198546
  batch 357 loss: 0.26049038834598554
  batch 358 loss: 0.2604109917106575
  batch 359 loss: 0.2604979174548869
  batch 360 loss: 0.260447292899092
  batch 361 loss: 0.26038563329922526
  batch 362 loss: 0.26035623083621756
  batch 363 loss: 0.2603931906055813
  batch 364 loss: 0.26033537302698406
  batch 365 loss: 0.2603212655407109
  batch 366 loss: 0.26036196857527955
  batch 367 loss: 0.26034955457055925
  batch 368 loss: 0.26019971113166085
  batch 369 loss: 0.26022776282899746
  batch 370 loss: 0.26028285171534565
  batch 371 loss: 0.2602240263692774
  batch 372 loss: 0.260282245013983
  batch 373 loss: 0.2603925717020802
  batch 374 loss: 0.26036138430158084
  batch 375 loss: 0.26037976968288423
  batch 376 loss: 0.26048259171558186
  batch 377 loss: 0.26047436545160785
  batch 378 loss: 0.260393340357397
  batch 379 loss: 0.2603797209608838
  batch 380 loss: 0.2603358785180669
  batch 381 loss: 0.2602744903073223
  batch 382 loss: 0.2602448617132546
  batch 383 loss: 0.26012900568953384
  batch 384 loss: 0.26013913296628743
  batch 385 loss: 0.260105037689209
  batch 386 loss: 0.260042136969344
  batch 387 loss: 0.25998441682459467
  batch 388 loss: 0.25998649581037847
  batch 389 loss: 0.25995588482652043
  batch 390 loss: 0.25994543780883156
  batch 391 loss: 0.2598150374410707
  batch 392 loss: 0.2598909263175969
  batch 393 loss: 0.25986885269937926
  batch 394 loss: 0.2598646609538098
  batch 395 loss: 0.25986473850811587
  batch 396 loss: 0.259952012073211
  batch 397 loss: 0.2599616638884436
  batch 398 loss: 0.2599164789271115
  batch 399 loss: 0.25993128748316513
  batch 400 loss: 0.2600345033034682
  batch 401 loss: 0.2600269879114598
  batch 402 loss: 0.2600173275194951
  batch 403 loss: 0.2599332296005254
  batch 404 loss: 0.2599050023473135
  batch 405 loss: 0.25996071894963585
  batch 406 loss: 0.2599439360384871
  batch 407 loss: 0.259975808917451
  batch 408 loss: 0.2599467136988453
  batch 409 loss: 0.26000276805427663
  batch 410 loss: 0.2599218518268771
  batch 411 loss: 0.25987408580281146
  batch 412 loss: 0.2599337336797159
  batch 413 loss: 0.2599501804635831
  batch 414 loss: 0.25993426871184566
  batch 415 loss: 0.2598396898034107
  batch 416 loss: 0.2599052285345701
  batch 417 loss: 0.259831923053419
  batch 418 loss: 0.2598034969072022
  batch 419 loss: 0.25979948989815815
  batch 420 loss: 0.2598074679573377
  batch 421 loss: 0.25982907357804846
  batch 422 loss: 0.2597734491336402
  batch 423 loss: 0.2597713999925776
  batch 424 loss: 0.2598133262528001
  batch 425 loss: 0.259699265851694
  batch 426 loss: 0.2595796060044441
  batch 427 loss: 0.2595173972691529
  batch 428 loss: 0.25944935199674046
  batch 429 loss: 0.259368957617344
  batch 430 loss: 0.2594428526800732
  batch 431 loss: 0.25941120333572
  batch 432 loss: 0.25939727664269785
  batch 433 loss: 0.25927101072207887
  batch 434 loss: 0.2592022586528057
  batch 435 loss: 0.25930851259450804
  batch 436 loss: 0.25936954946966345
  batch 437 loss: 0.2592931095877283
  batch 438 loss: 0.2592856438328686
  batch 439 loss: 0.2592062107319169
  batch 440 loss: 0.25923137248239736
  batch 441 loss: 0.25920580847868846
  batch 442 loss: 0.25918392102103427
  batch 443 loss: 0.259094770720258
  batch 444 loss: 0.2592403516508974
  batch 445 loss: 0.2591586913620488
  batch 446 loss: 0.2591871390361422
  batch 447 loss: 0.25919945204684663
  batch 448 loss: 0.2592172849191619
  batch 449 loss: 0.25919190835713807
  batch 450 loss: 0.25907413678036795
  batch 451 loss: 0.25899676724178033
  batch 452 loss: 0.2588898688225092
  batch 453 loss: 0.2588372765721601
  batch 454 loss: 0.2587952951896558
  batch 455 loss: 0.258800508753284
  batch 456 loss: 0.25873546207552417
  batch 457 loss: 0.25876247742635156
  batch 458 loss: 0.2587154178054572
  batch 459 loss: 0.25867102309769274
  batch 460 loss: 0.25865201616416805
  batch 461 loss: 0.25866672076656605
  batch 462 loss: 0.25866414151795497
  batch 463 loss: 0.25864508229893174
  batch 464 loss: 0.25853033000924464
  batch 465 loss: 0.2584823469000478
  batch 466 loss: 0.25847157058925585
  batch 467 loss: 0.2584747748341754
  batch 468 loss: 0.25841951328847146
  batch 469 loss: 0.25832635415261235
  batch 470 loss: 0.2585065573136857
  batch 471 loss: 0.2585266225213964
  batch 472 loss: 0.25835801866221225
LOSS train 0.25835801866221225 valid 0.20188182592391968
LOSS train 0.25835801866221225 valid 0.20344701409339905
LOSS train 0.25835801866221225 valid 0.22159677743911743
LOSS train 0.25835801866221225 valid 0.21383439749479294
LOSS train 0.25835801866221225 valid 0.2211330771446228
LOSS train 0.25835801866221225 valid 0.2206709384918213
LOSS train 0.25835801866221225 valid 0.21116700981344497
LOSS train 0.25835801866221225 valid 0.2107203472405672
LOSS train 0.25835801866221225 valid 0.2114277266793781
LOSS train 0.25835801866221225 valid 0.20906001180410386
LOSS train 0.25835801866221225 valid 0.20792408016594974
LOSS train 0.25835801866221225 valid 0.21030850956837335
LOSS train 0.25835801866221225 valid 0.2121609907883864
LOSS train 0.25835801866221225 valid 0.21016097707407816
LOSS train 0.25835801866221225 valid 0.2094917853673299
LOSS train 0.25835801866221225 valid 0.21274281293153763
LOSS train 0.25835801866221225 valid 0.21243123096578262
LOSS train 0.25835801866221225 valid 0.21230941183037227
LOSS train 0.25835801866221225 valid 0.21289033089813433
LOSS train 0.25835801866221225 valid 0.21300457790493965
LOSS train 0.25835801866221225 valid 0.21454443676131113
LOSS train 0.25835801866221225 valid 0.2129125784743916
LOSS train 0.25835801866221225 valid 0.21144217447094296
LOSS train 0.25835801866221225 valid 0.21161308884620667
LOSS train 0.25835801866221225 valid 0.21167636454105376
LOSS train 0.25835801866221225 valid 0.21089719293209222
LOSS train 0.25835801866221225 valid 0.21142728074833197
LOSS train 0.25835801866221225 valid 0.2117256000638008
LOSS train 0.25835801866221225 valid 0.2104057309956386
LOSS train 0.25835801866221225 valid 0.20910258839527765
LOSS train 0.25835801866221225 valid 0.20886136014615336
LOSS train 0.25835801866221225 valid 0.20969761814922094
LOSS train 0.25835801866221225 valid 0.20904345629793225
LOSS train 0.25835801866221225 valid 0.20842817601035624
LOSS train 0.25835801866221225 valid 0.2094564859356199
LOSS train 0.25835801866221225 valid 0.21030491714676222
LOSS train 0.25835801866221225 valid 0.2094648605262911
LOSS train 0.25835801866221225 valid 0.20919647224639593
LOSS train 0.25835801866221225 valid 0.2088028582242819
LOSS train 0.25835801866221225 valid 0.2090987168252468
LOSS train 0.25835801866221225 valid 0.20862053625467347
LOSS train 0.25835801866221225 valid 0.210526409248511
LOSS train 0.25835801866221225 valid 0.21086801900420077
LOSS train 0.25835801866221225 valid 0.2103185226971453
LOSS train 0.25835801866221225 valid 0.21011398401525286
LOSS train 0.25835801866221225 valid 0.20972963616899823
LOSS train 0.25835801866221225 valid 0.20955250396373423
LOSS train 0.25835801866221225 valid 0.2113738882665833
LOSS train 0.25835801866221225 valid 0.21105494578273928
LOSS train 0.25835801866221225 valid 0.21189798086881637
LOSS train 0.25835801866221225 valid 0.21168607181193783
LOSS train 0.25835801866221225 valid 0.2117968316261585
LOSS train 0.25835801866221225 valid 0.21284274278946644
LOSS train 0.25835801866221225 valid 0.21283967655014108
LOSS train 0.25835801866221225 valid 0.2125729663805528
LOSS train 0.25835801866221225 valid 0.21260621132595198
LOSS train 0.25835801866221225 valid 0.2118749906096542
LOSS train 0.25835801866221225 valid 0.21264781417517825
LOSS train 0.25835801866221225 valid 0.21263002945204912
LOSS train 0.25835801866221225 valid 0.21255794142683346
LOSS train 0.25835801866221225 valid 0.21283094365088667
LOSS train 0.25835801866221225 valid 0.2125483549410297
LOSS train 0.25835801866221225 valid 0.21230398993643504
LOSS train 0.25835801866221225 valid 0.21238013496622443
LOSS train 0.25835801866221225 valid 0.2111770551938277
LOSS train 0.25835801866221225 valid 0.2108973212765925
LOSS train 0.25835801866221225 valid 0.2117155903755729
LOSS train 0.25835801866221225 valid 0.21101059633142807
LOSS train 0.25835801866221225 valid 0.2115086716586265
LOSS train 0.25835801866221225 valid 0.21195343966994967
LOSS train 0.25835801866221225 valid 0.21234868235991033
LOSS train 0.25835801866221225 valid 0.21241053442160288
LOSS train 0.25835801866221225 valid 0.21295821258466538
LOSS train 0.25835801866221225 valid 0.21307128506737785
LOSS train 0.25835801866221225 valid 0.21287701825300853
LOSS train 0.25835801866221225 valid 0.21301054287897914
LOSS train 0.25835801866221225 valid 0.2132491318436412
LOSS train 0.25835801866221225 valid 0.2130348093998738
LOSS train 0.25835801866221225 valid 0.21304968225805065
LOSS train 0.25835801866221225 valid 0.21281176675111055
LOSS train 0.25835801866221225 valid 0.21285127839188517
LOSS train 0.25835801866221225 valid 0.21261652013877544
LOSS train 0.25835801866221225 valid 0.21235618857016048
LOSS train 0.25835801866221225 valid 0.21212089168173925
LOSS train 0.25835801866221225 valid 0.21238103526480057
LOSS train 0.25835801866221225 valid 0.21216423012489496
LOSS train 0.25835801866221225 valid 0.21159319922156716
LOSS train 0.25835801866221225 valid 0.21154076606035233
LOSS train 0.25835801866221225 valid 0.21172340197509595
LOSS train 0.25835801866221225 valid 0.21185803049140506
LOSS train 0.25835801866221225 valid 0.21204155683517456
LOSS train 0.25835801866221225 valid 0.2118684338486713
LOSS train 0.25835801866221225 valid 0.21189912656943002
LOSS train 0.25835801866221225 valid 0.2121592878978303
LOSS train 0.25835801866221225 valid 0.21216654275593005
LOSS train 0.25835801866221225 valid 0.21268078653762737
LOSS train 0.25835801866221225 valid 0.2127401395556853
LOSS train 0.25835801866221225 valid 0.21294888519510932
LOSS train 0.25835801866221225 valid 0.2130159865124057
LOSS train 0.25835801866221225 valid 0.21293818727135658
LOSS train 0.25835801866221225 valid 0.21313715084354476
LOSS train 0.25835801866221225 valid 0.21359343257020502
LOSS train 0.25835801866221225 valid 0.21325078534269795
LOSS train 0.25835801866221225 valid 0.2130479570478201
LOSS train 0.25835801866221225 valid 0.21339279186157953
LOSS train 0.25835801866221225 valid 0.2135109564043441
LOSS train 0.25835801866221225 valid 0.21325662202924212
LOSS train 0.25835801866221225 valid 0.21319682675379295
LOSS train 0.25835801866221225 valid 0.21252790489874848
LOSS train 0.25835801866221225 valid 0.2122988759116693
LOSS train 0.25835801866221225 valid 0.21243837663719245
LOSS train 0.25835801866221225 valid 0.21245353828583444
LOSS train 0.25835801866221225 valid 0.2122366023538387
LOSS train 0.25835801866221225 valid 0.21209688236316046
LOSS train 0.25835801866221225 valid 0.2125381183365117
LOSS train 0.25835801866221225 valid 0.21223748834996387
LOSS train 0.25835801866221225 valid 0.21262195120509872
LOSS train 0.25835801866221225 valid 0.21281841435169768
LOSS train 0.25835801866221225 valid 0.21248316927617336
LOSS train 0.25835801866221225 valid 0.21225513592362405
LOSS train 0.25835801866221225 valid 0.21215706398664427
LOSS train 0.25835801866221225 valid 0.21241467372804393
LOSS train 0.25835801866221225 valid 0.21243577626177934
LOSS train 0.25835801866221225 valid 0.21239064729982807
LOSS train 0.25835801866221225 valid 0.2123864872455597
LOSS train 0.25835801866221225 valid 0.2125766258391123
LOSS train 0.25835801866221225 valid 0.21247181589678515
LOSS train 0.25835801866221225 valid 0.21234490361530334
LOSS train 0.25835801866221225 valid 0.2119689538728359
LOSS train 0.25835801866221225 valid 0.21164709535928872
LOSS train 0.25835801866221225 valid 0.21166643090830503
LOSS train 0.25835801866221225 valid 0.21161470889593614
LOSS train 0.25835801866221225 valid 0.21150082417000504
LOSS train 0.25835801866221225 valid 0.2116284646205048
LOSS train 0.25835801866221225 valid 0.21183745242931223
LOSS train 0.25835801866221225 valid 0.21206588517217076
LOSS train 0.25835801866221225 valid 0.21211514703548737
LOSS train 0.25835801866221225 valid 0.2120640830717225
LOSS train 0.25835801866221225 valid 0.21193950189103325
LOSS train 0.25835801866221225 valid 0.21213962009974888
LOSS train 0.25835801866221225 valid 0.2122625710693657
LOSS train 0.25835801866221225 valid 0.21256598368496962
LOSS train 0.25835801866221225 valid 0.21261981744449454
LOSS train 0.25835801866221225 valid 0.212714451054732
LOSS train 0.25835801866221225 valid 0.21262751375806743
LOSS train 0.25835801866221225 valid 0.2125546322905854
LOSS train 0.25835801866221225 valid 0.21252759409194089
LOSS train 0.25835801866221225 valid 0.2122412733733654
LOSS train 0.25835801866221225 valid 0.2123844726573701
LOSS train 0.25835801866221225 valid 0.21231585572163264
LOSS train 0.25835801866221225 valid 0.21206945407864275
LOSS train 0.25835801866221225 valid 0.21207757617690062
LOSS train 0.25835801866221225 valid 0.2118755597499461
LOSS train 0.25835801866221225 valid 0.21190164915540002
LOSS train 0.25835801866221225 valid 0.21194999487169328
LOSS train 0.25835801866221225 valid 0.2123170556166233
LOSS train 0.25835801866221225 valid 0.21225164232739976
LOSS train 0.25835801866221225 valid 0.21223997503896302
LOSS train 0.25835801866221225 valid 0.2124223122431797
LOSS train 0.25835801866221225 valid 0.21245440151542425
LOSS train 0.25835801866221225 valid 0.21260269763677017
LOSS train 0.25835801866221225 valid 0.21268844107786813
LOSS train 0.25835801866221225 valid 0.2124235904289901
LOSS train 0.25835801866221225 valid 0.2125069119217919
LOSS train 0.25835801866221225 valid 0.21267836166150642
LOSS train 0.25835801866221225 valid 0.21255603369819112
LOSS train 0.25835801866221225 valid 0.21263557681423462
LOSS train 0.25835801866221225 valid 0.21242754427450045
LOSS train 0.25835801866221225 valid 0.21223713589843207
LOSS train 0.25835801866221225 valid 0.21228014590109096
LOSS train 0.25835801866221225 valid 0.21214443488776336
LOSS train 0.25835801866221225 valid 0.21199035453935003
LOSS train 0.25835801866221225 valid 0.21197385896492554
LOSS train 0.25835801866221225 valid 0.2120677493940825
LOSS train 0.25835801866221225 valid 0.21199446158749716
LOSS train 0.25835801866221225 valid 0.2118819678540934
LOSS train 0.25835801866221225 valid 0.21190511582598173
LOSS train 0.25835801866221225 valid 0.2119902508814683
LOSS train 0.25835801866221225 valid 0.21218256469212432
LOSS train 0.25835801866221225 valid 0.21217095280687015
LOSS train 0.25835801866221225 valid 0.21205473390732021
LOSS train 0.25835801866221225 valid 0.21198211667629388
LOSS train 0.25835801866221225 valid 0.21202086091367273
LOSS train 0.25835801866221225 valid 0.21192692147324915
LOSS train 0.25835801866221225 valid 0.2119459615365879
LOSS train 0.25835801866221225 valid 0.21200876955383569
LOSS train 0.25835801866221225 valid 0.21178596080305742
LOSS train 0.25835801866221225 valid 0.2117301698060746
LOSS train 0.25835801866221225 valid 0.21168028527782076
LOSS train 0.25835801866221225 valid 0.2116890729257935
LOSS train 0.25835801866221225 valid 0.21196518385909616
LOSS train 0.25835801866221225 valid 0.21200589983103177
LOSS train 0.25835801866221225 valid 0.21183133573112092
LOSS train 0.25835801866221225 valid 0.21187694040463143
LOSS train 0.25835801866221225 valid 0.211813856393863
LOSS train 0.25835801866221225 valid 0.21210356436821878
LOSS train 0.25835801866221225 valid 0.2120052965613186
LOSS train 0.25835801866221225 valid 0.21227282820024876
LOSS train 0.25835801866221225 valid 0.212241130707851
LOSS train 0.25835801866221225 valid 0.21223303236067295
LOSS train 0.25835801866221225 valid 0.2122732268933633
LOSS train 0.25835801866221225 valid 0.21242355326614756
LOSS train 0.25835801866221225 valid 0.21258874266899278
LOSS train 0.25835801866221225 valid 0.21247166899197242
LOSS train 0.25835801866221225 valid 0.21242029361608553
LOSS train 0.25835801866221225 valid 0.21243255592665625
LOSS train 0.25835801866221225 valid 0.21237969549669736
LOSS train 0.25835801866221225 valid 0.21215566247701645
LOSS train 0.25835801866221225 valid 0.21213968319185614
LOSS train 0.25835801866221225 valid 0.21219648931707655
LOSS train 0.25835801866221225 valid 0.2123914472582216
LOSS train 0.25835801866221225 valid 0.2122741682490088
LOSS train 0.25835801866221225 valid 0.2123931030012632
LOSS train 0.25835801866221225 valid 0.212320501489617
LOSS train 0.25835801866221225 valid 0.21203666474930075
LOSS train 0.25835801866221225 valid 0.21183273216916454
LOSS train 0.25835801866221225 valid 0.21157469830480038
LOSS train 0.25835801866221225 valid 0.2115312419626691
LOSS train 0.25835801866221225 valid 0.2115485711048727
LOSS train 0.25835801866221225 valid 0.21149684488773346
LOSS train 0.25835801866221225 valid 0.2113660795116856
LOSS train 0.25835801866221225 valid 0.21149670017195177
LOSS train 0.25835801866221225 valid 0.21138417627244788
LOSS train 0.25835801866221225 valid 0.2112841972682093
LOSS train 0.25835801866221225 valid 0.21115554458565183
LOSS train 0.25835801866221225 valid 0.210983680874373
LOSS train 0.25835801866221225 valid 0.21090315374246252
LOSS train 0.25835801866221225 valid 0.21081317536402167
LOSS train 0.25835801866221225 valid 0.21071584392583007
LOSS train 0.25835801866221225 valid 0.2107382771761521
LOSS train 0.25835801866221225 valid 0.2107745778638047
LOSS train 0.25835801866221225 valid 0.21081976239280453
LOSS train 0.25835801866221225 valid 0.2110371757858301
LOSS train 0.25835801866221225 valid 0.2109281239219201
LOSS train 0.25835801866221225 valid 0.21092982653607714
LOSS train 0.25835801866221225 valid 0.21076346624453188
LOSS train 0.25835801866221225 valid 0.21079766794347563
LOSS train 0.25835801866221225 valid 0.21088355115982665
LOSS train 0.25835801866221225 valid 0.2108105921720361
LOSS train 0.25835801866221225 valid 0.2105018636211753
LOSS train 0.25835801866221225 valid 0.21033343004982502
LOSS train 0.25835801866221225 valid 0.2104115558803574
LOSS train 0.25835801866221225 valid 0.21039387614393432
LOSS train 0.25835801866221225 valid 0.21038707638861703
LOSS train 0.25835801866221225 valid 0.21039555632338233
LOSS train 0.25835801866221225 valid 0.21042086838222132
LOSS train 0.25835801866221225 valid 0.21043649128815423
LOSS train 0.25835801866221225 valid 0.21052526628538484
LOSS train 0.25835801866221225 valid 0.21038932906815325
LOSS train 0.25835801866221225 valid 0.21057332211732865
LOSS train 0.25835801866221225 valid 0.21082622958606934
LOSS train 0.25835801866221225 valid 0.21085039378395157
LOSS train 0.25835801866221225 valid 0.21079762356554566
LOSS train 0.25835801866221225 valid 0.21095104550752114
LOSS train 0.25835801866221225 valid 0.21088066989300297
LOSS train 0.25835801866221225 valid 0.21081992250401527
LOSS train 0.25835801866221225 valid 0.21080438873879176
LOSS train 0.25835801866221225 valid 0.21083315409893214
LOSS train 0.25835801866221225 valid 0.21072420247733362
LOSS train 0.25835801866221225 valid 0.21058139560314326
LOSS train 0.25835801866221225 valid 0.210561186417766
LOSS train 0.25835801866221225 valid 0.21079898199052302
LOSS train 0.25835801866221225 valid 0.21066655059957687
LOSS train 0.25835801866221225 valid 0.21071762382758386
LOSS train 0.25835801866221225 valid 0.21063259362049822
LOSS train 0.25835801866221225 valid 0.21063823208100813
LOSS train 0.25835801866221225 valid 0.210565718428026
LOSS train 0.25835801866221225 valid 0.21066984295177815
LOSS train 0.25835801866221225 valid 0.21077940826300795
LOSS train 0.25835801866221225 valid 0.210685965805142
LOSS train 0.25835801866221225 valid 0.2106245538955245
LOSS train 0.25835801866221225 valid 0.21047328100265825
LOSS train 0.25835801866221225 valid 0.21042023850229633
LOSS train 0.25835801866221225 valid 0.2103588601751049
LOSS train 0.25835801866221225 valid 0.21049233165654269
LOSS train 0.25835801866221225 valid 0.2104521957428559
LOSS train 0.25835801866221225 valid 0.21044946572195322
LOSS train 0.25835801866221225 valid 0.21055048318217984
LOSS train 0.25835801866221225 valid 0.21057712278699361
LOSS train 0.25835801866221225 valid 0.2104035386549575
LOSS train 0.25835801866221225 valid 0.21031248596003047
LOSS train 0.25835801866221225 valid 0.21024809164781097
LOSS train 0.25835801866221225 valid 0.2103391909978415
LOSS train 0.25835801866221225 valid 0.21017262995452948
LOSS train 0.25835801866221225 valid 0.21024066591471957
LOSS train 0.25835801866221225 valid 0.2102503241463141
LOSS train 0.25835801866221225 valid 0.21028753834734396
LOSS train 0.25835801866221225 valid 0.21028925333586004
LOSS train 0.25835801866221225 valid 0.21029258068251364
LOSS train 0.25835801866221225 valid 0.2102229406607562
LOSS train 0.25835801866221225 valid 0.21022408924151942
LOSS train 0.25835801866221225 valid 0.2102348496754692
LOSS train 0.25835801866221225 valid 0.21021764163474582
LOSS train 0.25835801866221225 valid 0.2101147351723139
LOSS train 0.25835801866221225 valid 0.21016658362695725
LOSS train 0.25835801866221225 valid 0.2101658144714059
LOSS train 0.25835801866221225 valid 0.21020675468123723
LOSS train 0.25835801866221225 valid 0.2103044048651753
LOSS train 0.25835801866221225 valid 0.21023668239148563
LOSS train 0.25835801866221225 valid 0.21020575175682704
LOSS train 0.25835801866221225 valid 0.2101268935639201
LOSS train 0.25835801866221225 valid 0.2101511320234924
LOSS train 0.25835801866221225 valid 0.21007563311471403
LOSS train 0.25835801866221225 valid 0.21010593123930066
LOSS train 0.25835801866221225 valid 0.21007089253331795
LOSS train 0.25835801866221225 valid 0.21030744101876528
LOSS train 0.25835801866221225 valid 0.2102333305324716
LOSS train 0.25835801866221225 valid 0.2103653017963682
LOSS train 0.25835801866221225 valid 0.21042600611652756
LOSS train 0.25835801866221225 valid 0.2104772140422175
LOSS train 0.25835801866221225 valid 0.2105482604917606
LOSS train 0.25835801866221225 valid 0.2105348538607359
LOSS train 0.25835801866221225 valid 0.21065075743122222
LOSS train 0.25835801866221225 valid 0.21060728030219958
LOSS train 0.25835801866221225 valid 0.21059805646775262
LOSS train 0.25835801866221225 valid 0.2106384701932533
LOSS train 0.25835801866221225 valid 0.21060256838610494
LOSS train 0.25835801866221225 valid 0.2106158209484328
LOSS train 0.25835801866221225 valid 0.2105987799765548
LOSS train 0.25835801866221225 valid 0.21050935215316713
LOSS train 0.25835801866221225 valid 0.2105632457313508
LOSS train 0.25835801866221225 valid 0.21053289668345304
LOSS train 0.25835801866221225 valid 0.21038188418552234
LOSS train 0.25835801866221225 valid 0.21035924066364028
LOSS train 0.25835801866221225 valid 0.2101833922129411
LOSS train 0.25835801866221225 valid 0.21032625500775554
LOSS train 0.25835801866221225 valid 0.21031230865815365
LOSS train 0.25835801866221225 valid 0.210367445584114
LOSS train 0.25835801866221225 valid 0.2103091991056425
LOSS train 0.25835801866221225 valid 0.21031924124920007
LOSS train 0.25835801866221225 valid 0.21026665188394644
LOSS train 0.25835801866221225 valid 0.21020215103425174
LOSS train 0.25835801866221225 valid 0.21031372629486406
LOSS train 0.25835801866221225 valid 0.2102657542257252
LOSS train 0.25835801866221225 valid 0.21023163670924172
LOSS train 0.25835801866221225 valid 0.21018970726678768
LOSS train 0.25835801866221225 valid 0.21036633526359186
LOSS train 0.25835801866221225 valid 0.21029476041095496
LOSS train 0.25835801866221225 valid 0.21026567921350137
LOSS train 0.25835801866221225 valid 0.21035267469637534
LOSS train 0.25835801866221225 valid 0.21033299458691102
LOSS train 0.25835801866221225 valid 0.21025496682054118
LOSS train 0.25835801866221225 valid 0.21019362621335178
LOSS train 0.25835801866221225 valid 0.2102224485472191
LOSS train 0.25835801866221225 valid 0.2102914681469185
LOSS train 0.25835801866221225 valid 0.21021345899456498
LOSS train 0.25835801866221225 valid 0.2101641354883782
LOSS train 0.25835801866221225 valid 0.21020827129825778
LOSS train 0.25835801866221225 valid 0.21012526379102964
LOSS train 0.25835801866221225 valid 0.2101664655974933
LOSS train 0.25835801866221225 valid 0.2102570201766457
LOSS train 0.25835801866221225 valid 0.2103231352838603
LOSS train 0.25835801866221225 valid 0.2104162213738174
LOSS train 0.25835801866221225 valid 0.21039152717859733
LOSS train 0.25835801866221225 valid 0.21029028640666478
LOSS train 0.25835801866221225 valid 0.21026274879996695
LOSS train 0.25835801866221225 valid 0.21034021729848631
LOSS train 0.25835801866221225 valid 0.21035825352928492
LOSS train 0.25835801866221225 valid 0.21034076959309803
LOSS train 0.25835801866221225 valid 0.21048351162009768
LOSS train 0.25835801866221225 valid 0.2104381444216435
LOSS train 0.25835801866221225 valid 0.21053955242943367
LOSS train 0.25835801866221225 valid 0.21057635020290197
LOSS train 0.25835801866221225 valid 0.21047417451064665
LOSS train 0.25835801866221225 valid 0.21058512060609583
LOSS train 0.25835801866221225 valid 0.21063655887633725
LOSS train 0.25835801866221225 valid 0.21062077151494715
LOSS train 0.25835801866221225 valid 0.21050078778163248
LOSS train 0.25835801866221225 valid 0.21058183379451112
EPOCH 11:
  batch 1 loss: 0.2752944529056549
  batch 2 loss: 0.29242923855781555
  batch 3 loss: 0.2742881377538045
  batch 4 loss: 0.2800031080842018
  batch 5 loss: 0.28503175973892214
  batch 6 loss: 0.27735284964243573
  batch 7 loss: 0.2782153274331774
  batch 8 loss: 0.27446714602410793
  batch 9 loss: 0.27313171989387935
  batch 10 loss: 0.26787897646427156
  batch 11 loss: 0.26918371699073096
  batch 12 loss: 0.26686208446820575
  batch 13 loss: 0.2653759030195383
  batch 14 loss: 0.2646429432289941
  batch 15 loss: 0.2644001066684723
  batch 16 loss: 0.26456571742892265
  batch 17 loss: 0.2650349631029017
  batch 18 loss: 0.26303452170557445
  batch 19 loss: 0.2630586584931926
  batch 20 loss: 0.2606112830340862
  batch 21 loss: 0.2631585874727794
  batch 22 loss: 0.2635331851514903
  batch 23 loss: 0.26124719109224237
  batch 24 loss: 0.26226533638934296
  batch 25 loss: 0.26208376705646513
  batch 26 loss: 0.2620857684658124
  batch 27 loss: 0.26392689954351495
  batch 28 loss: 0.2645838404340403
  batch 29 loss: 0.2633860188311544
  batch 30 loss: 0.2652428552508354
  batch 31 loss: 0.2663707862938604
  batch 32 loss: 0.267528326716274
  batch 33 loss: 0.26851763102141296
  batch 34 loss: 0.2684730453526272
  batch 35 loss: 0.27014114899294717
  batch 36 loss: 0.2703085843887594
  batch 37 loss: 0.2700816606347625
  batch 38 loss: 0.27017926816877563
  batch 39 loss: 0.27089727574434036
  batch 40 loss: 0.2705587472766638
  batch 41 loss: 0.2705841882199776
  batch 42 loss: 0.2701674753001758
  batch 43 loss: 0.26957435039586797
  batch 44 loss: 0.26917384497144003
  batch 45 loss: 0.26888108054796855
  batch 46 loss: 0.2674346394512964
  batch 47 loss: 0.26669689981227224
  batch 48 loss: 0.26597241560618085
  batch 49 loss: 0.26635683191065884
  batch 50 loss: 0.26602716267108917
  batch 51 loss: 0.26581979908195197
  batch 52 loss: 0.26504817547706455
  batch 53 loss: 0.2642997222126655
  batch 54 loss: 0.26441523929437
  batch 55 loss: 0.26397755416956814
  batch 56 loss: 0.2646357385175569
  batch 57 loss: 0.26402630251750614
  batch 58 loss: 0.2631268984284894
  batch 59 loss: 0.2635126497785924
  batch 60 loss: 0.2636095732450485
  batch 61 loss: 0.26455887028428376
  batch 62 loss: 0.265066042061775
  batch 63 loss: 0.26480791729594033
  batch 64 loss: 0.26492764288559556
  batch 65 loss: 0.2646918319738828
  batch 66 loss: 0.2643863102703383
  batch 67 loss: 0.26410054092976587
  batch 68 loss: 0.2640688073109178
  batch 69 loss: 0.2645960000978
  batch 70 loss: 0.2639279772128378
  batch 71 loss: 0.26382381164691815
  batch 72 loss: 0.26367668993771076
  batch 73 loss: 0.26335013484301634
  batch 74 loss: 0.262979082561828
  batch 75 loss: 0.26264586925506594
  batch 76 loss: 0.26268706745222997
  batch 77 loss: 0.2623030357546621
  batch 78 loss: 0.2619974555877539
  batch 79 loss: 0.2618002929264986
  batch 80 loss: 0.26115305721759796
  batch 81 loss: 0.2607854818120415
  batch 82 loss: 0.26098046651700646
  batch 83 loss: 0.2606956486601427
  batch 84 loss: 0.26006968195239705
  batch 85 loss: 0.2601109608131297
  batch 86 loss: 0.2602146189919738
  batch 87 loss: 0.25990763186723337
  batch 88 loss: 0.25985372828489
  batch 89 loss: 0.25962407635838797
  batch 90 loss: 0.26007766524950665
  batch 91 loss: 0.2598114508193928
  batch 92 loss: 0.25968952007267787
  batch 93 loss: 0.2595632331025216
  batch 94 loss: 0.25989506171738846
  batch 95 loss: 0.2601386203577644
  batch 96 loss: 0.26034054237728316
  batch 97 loss: 0.2608312572707835
  batch 98 loss: 0.2608844262300705
  batch 99 loss: 0.2607691286489217
  batch 100 loss: 0.2608208133280277
  batch 101 loss: 0.26078006079291355
  batch 102 loss: 0.26068416018696394
  batch 103 loss: 0.2606140255060011
  batch 104 loss: 0.2610158735456375
  batch 105 loss: 0.2608152674777167
  batch 106 loss: 0.2611542879691664
  batch 107 loss: 0.2613541061911627
  batch 108 loss: 0.2610362824742441
  batch 109 loss: 0.2611579765147025
  batch 110 loss: 0.26120115261186255
  batch 111 loss: 0.26107974484697116
  batch 112 loss: 0.2610067210293242
  batch 113 loss: 0.26060323522681683
  batch 114 loss: 0.26050530321765364
  batch 115 loss: 0.26003122938715895
  batch 116 loss: 0.2599264642801778
  batch 117 loss: 0.2596935689704031
  batch 118 loss: 0.259724507266182
  batch 119 loss: 0.2597483099759126
  batch 120 loss: 0.2593704840789239
  batch 121 loss: 0.2591136623758915
  batch 122 loss: 0.25883451446157985
  batch 123 loss: 0.25879372741148726
  batch 124 loss: 0.25846205062923894
  batch 125 loss: 0.25819655191898344
  batch 126 loss: 0.25848779543524697
  batch 127 loss: 0.25824716159208555
  batch 128 loss: 0.25817966600880027
  batch 129 loss: 0.2585310875907425
  batch 130 loss: 0.2585451671710381
  batch 131 loss: 0.25863353464439626
  batch 132 loss: 0.25857361654440564
  batch 133 loss: 0.25851500616934064
  batch 134 loss: 0.258335003101114
  batch 135 loss: 0.25832788624145364
  batch 136 loss: 0.2585436229539268
  batch 137 loss: 0.2589206248521805
  batch 138 loss: 0.2589756240663321
  batch 139 loss: 0.25898031633106067
  batch 140 loss: 0.25941982216068676
  batch 141 loss: 0.25927474756612845
  batch 142 loss: 0.2592084085647489
  batch 143 loss: 0.2594903341748498
  batch 144 loss: 0.25951591020243037
  batch 145 loss: 0.2594848921586727
  batch 146 loss: 0.2595385269147076
  batch 147 loss: 0.25944900502558466
  batch 148 loss: 0.2594609452864608
  batch 149 loss: 0.2590381453101267
  batch 150 loss: 0.25900792102018994
  batch 151 loss: 0.25854447345859954
  batch 152 loss: 0.2583681870447962
  batch 153 loss: 0.25875356890796836
  batch 154 loss: 0.2585891628420198
  batch 155 loss: 0.2585617119266141
  batch 156 loss: 0.2584661233883638
  batch 157 loss: 0.25839466653811705
  batch 158 loss: 0.2584523207779172
  batch 159 loss: 0.2585331364622656
  batch 160 loss: 0.2583768586628139
  batch 161 loss: 0.2581392277656875
  batch 162 loss: 0.25802093505123513
  batch 163 loss: 0.2577622134261336
  batch 164 loss: 0.25795926299037003
  batch 165 loss: 0.25811634244340836
  batch 166 loss: 0.2582724631191736
  batch 167 loss: 0.2580967183241587
  batch 168 loss: 0.25781808899981634
  batch 169 loss: 0.25795337152198927
  batch 170 loss: 0.2580063004704083
  batch 171 loss: 0.25806588335343966
  batch 172 loss: 0.2580679939236752
  batch 173 loss: 0.25826129472324616
  batch 174 loss: 0.2582651712428564
  batch 175 loss: 0.2580574962922505
  batch 176 loss: 0.25822575602003117
  batch 177 loss: 0.25844855471856176
  batch 178 loss: 0.25810536164581105
  batch 179 loss: 0.2583458186361377
  batch 180 loss: 0.2582632825606399
  batch 181 loss: 0.2581267141176192
  batch 182 loss: 0.258043905208399
  batch 183 loss: 0.25771429499641796
  batch 184 loss: 0.25748620060798916
  batch 185 loss: 0.2573397319059114
  batch 186 loss: 0.25720530516037377
  batch 187 loss: 0.25741954450620047
  batch 188 loss: 0.25727041358960434
  batch 189 loss: 0.25726430780357784
  batch 190 loss: 0.25726363556949716
  batch 191 loss: 0.2574497435573508
  batch 192 loss: 0.2573043835970263
  batch 193 loss: 0.25734007806357945
  batch 194 loss: 0.2572992053843036
  batch 195 loss: 0.2574641099342933
  batch 196 loss: 0.2573209334240884
  batch 197 loss: 0.25745704580987167
  batch 198 loss: 0.25749657799800235
  batch 199 loss: 0.257577694615527
  batch 200 loss: 0.2574715843051672
  batch 201 loss: 0.2572984364050538
  batch 202 loss: 0.25722331693857026
  batch 203 loss: 0.257108102951731
  batch 204 loss: 0.25700704483132736
  batch 205 loss: 0.25681089380892314
  batch 206 loss: 0.25675966236197834
  batch 207 loss: 0.25672859500571726
  batch 208 loss: 0.25667519559367347
  batch 209 loss: 0.2565035309517783
  batch 210 loss: 0.2565700020108904
  batch 211 loss: 0.256569834823292
  batch 212 loss: 0.2566032869354734
  batch 213 loss: 0.25655820706920446
  batch 214 loss: 0.25655454543427886
  batch 215 loss: 0.25654995462229085
  batch 216 loss: 0.25657728404082636
  batch 217 loss: 0.25637905409533857
  batch 218 loss: 0.25627944912385503
  batch 219 loss: 0.25604914298884945
  batch 220 loss: 0.2561888734048063
  batch 221 loss: 0.2560723099638434
  batch 222 loss: 0.2560530163280599
  batch 223 loss: 0.25602791374841616
  batch 224 loss: 0.2560741413783814
  batch 225 loss: 0.2561171916458342
  batch 226 loss: 0.2563146659362633
  batch 227 loss: 0.2562310693022438
  batch 228 loss: 0.2562730486194293
  batch 229 loss: 0.25633572087537776
  batch 230 loss: 0.25652299691801483
  batch 231 loss: 0.2566114633114307
  batch 232 loss: 0.2564225397233305
  batch 233 loss: 0.2564294148924013
  batch 234 loss: 0.25643174413941866
  batch 235 loss: 0.2562721904922039
  batch 236 loss: 0.25625946075987005
  batch 237 loss: 0.2562938518036267
  batch 238 loss: 0.2562188664529504
  batch 239 loss: 0.2562788979902427
  batch 240 loss: 0.25628195150444905
  batch 241 loss: 0.25622512313587537
  batch 242 loss: 0.2562035203349492
  batch 243 loss: 0.25620829893483055
  batch 244 loss: 0.2561952202412926
  batch 245 loss: 0.2562435227389238
  batch 246 loss: 0.2562975500051568
  batch 247 loss: 0.25631023834832767
  batch 248 loss: 0.25654087486045973
  batch 249 loss: 0.2565014861434339
  batch 250 loss: 0.25629858994483945
  batch 251 loss: 0.25627071389639045
  batch 252 loss: 0.25620376224082614
  batch 253 loss: 0.2561680556168198
  batch 254 loss: 0.25624422124755664
  batch 255 loss: 0.2562411007343554
  batch 256 loss: 0.2562755190883763
  batch 257 loss: 0.2563075048102479
  batch 258 loss: 0.25623579334843066
  batch 259 loss: 0.25624663477698806
  batch 260 loss: 0.2562610395825826
  batch 261 loss: 0.25643713092895304
  batch 262 loss: 0.25646259459375426
  batch 263 loss: 0.2565561037779761
  batch 264 loss: 0.256788471205668
  batch 265 loss: 0.25696290888876283
  batch 266 loss: 0.2568839018730293
  batch 267 loss: 0.25677875019191354
  batch 268 loss: 0.25693062859684673
  batch 269 loss: 0.2569824088240202
  batch 270 loss: 0.25697172317239975
  batch 271 loss: 0.257045756626833
  batch 272 loss: 0.25699751698138085
  batch 273 loss: 0.2570287587987634
  batch 274 loss: 0.25697914097648467
  batch 275 loss: 0.2569751478867097
  batch 276 loss: 0.2569180724305519
  batch 277 loss: 0.2569820663020929
  batch 278 loss: 0.25695292876778747
  batch 279 loss: 0.2569341435227343
  batch 280 loss: 0.2567481760467802
  batch 281 loss: 0.25690295305964783
  batch 282 loss: 0.25683529347392686
  batch 283 loss: 0.2567830476449151
  batch 284 loss: 0.25671307952471184
  batch 285 loss: 0.25670430367453057
  batch 286 loss: 0.25670156372593833
  batch 287 loss: 0.2566651734637051
  batch 288 loss: 0.2566827161548038
  batch 289 loss: 0.256542147082441
  batch 290 loss: 0.25644831343971447
  batch 291 loss: 0.2563895948768891
  batch 292 loss: 0.25629622167716287
  batch 293 loss: 0.2562004225551065
  batch 294 loss: 0.2561206227662612
  batch 295 loss: 0.256068989280927
  batch 296 loss: 0.25611295067780726
  batch 297 loss: 0.2561182899507208
  batch 298 loss: 0.2561045832081929
  batch 299 loss: 0.2562074118035294
  batch 300 loss: 0.2562052655220032
  batch 301 loss: 0.25615797422058956
  batch 302 loss: 0.2562801538417671
  batch 303 loss: 0.25636111190020056
  batch 304 loss: 0.2564148214204531
  batch 305 loss: 0.256436878393908
  batch 306 loss: 0.2566640548943694
  batch 307 loss: 0.25679044066306433
  batch 308 loss: 0.25689106942577794
  batch 309 loss: 0.25685153684569795
  batch 310 loss: 0.25697023560923915
  batch 311 loss: 0.2569453708132747
  batch 312 loss: 0.25701407414789385
  batch 313 loss: 0.25705065671056987
  batch 314 loss: 0.2570133604061831
  batch 315 loss: 0.2570306655906496
  batch 316 loss: 0.2571108258223232
  batch 317 loss: 0.25719506451008073
  batch 318 loss: 0.25711399251185124
  batch 319 loss: 0.2571395527232777
  batch 320 loss: 0.2572792164050043
  batch 321 loss: 0.25716092801911067
  batch 322 loss: 0.25706994399335814
  batch 323 loss: 0.2570338658873142
  batch 324 loss: 0.2570369067015471
  batch 325 loss: 0.25701238283744227
  batch 326 loss: 0.25716232531275485
  batch 327 loss: 0.25718273354597415
  batch 328 loss: 0.2572162378488517
  batch 329 loss: 0.2573294534509305
  batch 330 loss: 0.25735645763801807
  batch 331 loss: 0.2573640703074522
  batch 332 loss: 0.257508463349687
  batch 333 loss: 0.2574960064959598
  batch 334 loss: 0.25764489423728987
  batch 335 loss: 0.2576797317213087
  batch 336 loss: 0.2577409515423434
  batch 337 loss: 0.25775509692441106
  batch 338 loss: 0.25771919946522404
  batch 339 loss: 0.2577146150633297
  batch 340 loss: 0.25781269875519414
  batch 341 loss: 0.2578086796656382
  batch 342 loss: 0.25781184535103235
  batch 343 loss: 0.25787253921129266
  batch 344 loss: 0.25790008395736996
  batch 345 loss: 0.25789301529310754
  batch 346 loss: 0.25786796479211377
  batch 347 loss: 0.25771123346918257
  batch 348 loss: 0.25769847327436524
  batch 349 loss: 0.25777889438037543
  batch 350 loss: 0.25779668829270774
  batch 351 loss: 0.2578071073504255
  batch 352 loss: 0.2577504276467318
  batch 353 loss: 0.2577415645206954
  batch 354 loss: 0.2578631468159331
  batch 355 loss: 0.25779632023522553
  batch 356 loss: 0.2578418352379558
  batch 357 loss: 0.25770680802542956
  batch 358 loss: 0.2575895667242604
  batch 359 loss: 0.2575712895492989
  batch 360 loss: 0.25750139935149086
  batch 361 loss: 0.25742375611268253
  batch 362 loss: 0.25734303204229525
  batch 363 loss: 0.25733885501534487
  batch 364 loss: 0.257248819815916
  batch 365 loss: 0.2572469610465716
  batch 366 loss: 0.25726095220593154
  batch 367 loss: 0.25725889096305543
  batch 368 loss: 0.257103887953512
  batch 369 loss: 0.2570549504059117
  batch 370 loss: 0.25714541689769643
  batch 371 loss: 0.25711280778251244
  batch 372 loss: 0.25716555907681427
  batch 373 loss: 0.25723786971684115
  batch 374 loss: 0.25722977618642034
  batch 375 loss: 0.25720679835478466
  batch 376 loss: 0.25731132896815206
  batch 377 loss: 0.25727133990440826
  batch 378 loss: 0.25717883541312797
  batch 379 loss: 0.25719099430893216
  batch 380 loss: 0.25711030524812245
  batch 381 loss: 0.25703372851287914
  batch 382 loss: 0.2569516305055918
  batch 383 loss: 0.25680723934198485
  batch 384 loss: 0.2567858330439776
  batch 385 loss: 0.25669716956553523
  batch 386 loss: 0.2566153146519562
  batch 387 loss: 0.25656767235707867
  batch 388 loss: 0.2565290069533992
  batch 389 loss: 0.2564918227244772
  batch 390 loss: 0.2564701680953686
  batch 391 loss: 0.256303971304613
  batch 392 loss: 0.2563665779877682
  batch 393 loss: 0.2563431843849842
  batch 394 loss: 0.25637033415324795
  batch 395 loss: 0.25634240202511416
  batch 396 loss: 0.2564043293170857
  batch 397 loss: 0.25643884531346617
  batch 398 loss: 0.25642481667162786
  batch 399 loss: 0.2564152796241574
  batch 400 loss: 0.25648051876574757
  batch 401 loss: 0.2564654491860373
  batch 402 loss: 0.25643650175475363
  batch 403 loss: 0.25635413148858704
  batch 404 loss: 0.2563087425314554
  batch 405 loss: 0.25637959256584264
  batch 406 loss: 0.25633330676209165
  batch 407 loss: 0.2563657137114527
  batch 408 loss: 0.25633343214205667
  batch 409 loss: 0.2563643235449103
  batch 410 loss: 0.2563028924348878
  batch 411 loss: 0.2562671458199076
  batch 412 loss: 0.2563276796520335
  batch 413 loss: 0.2562956312811115
  batch 414 loss: 0.25626350827695094
  batch 415 loss: 0.2561729689678514
  batch 416 loss: 0.25618944517694986
  batch 417 loss: 0.2560654082458368
  batch 418 loss: 0.2560121050220357
  batch 419 loss: 0.2559188804578098
  batch 420 loss: 0.25589972486098606
  batch 421 loss: 0.25594002971173463
  batch 422 loss: 0.25589572203950295
  batch 423 loss: 0.25588166868714857
  batch 424 loss: 0.2559247463238689
  batch 425 loss: 0.25580595949116874
  batch 426 loss: 0.2556725396586696
  batch 427 loss: 0.25564243216983606
  batch 428 loss: 0.25554737420839685
  batch 429 loss: 0.2554678160758007
  batch 430 loss: 0.2555111893041189
  batch 431 loss: 0.2554556491272632
  batch 432 loss: 0.25542980763647294
  batch 433 loss: 0.2553043748159607
  batch 434 loss: 0.2552024198841939
  batch 435 loss: 0.25527908055261633
  batch 436 loss: 0.2553454577375989
  batch 437 loss: 0.25526828907038035
  batch 438 loss: 0.25526023812626053
  batch 439 loss: 0.25518226219608464
  batch 440 loss: 0.2552004875445908
  batch 441 loss: 0.2551773475681573
  batch 442 loss: 0.2551471979707075
  batch 443 loss: 0.2550709176951553
  batch 444 loss: 0.25528236975272495
  batch 445 loss: 0.2551790151368366
  batch 446 loss: 0.2552091543784056
  batch 447 loss: 0.2552777341081525
  batch 448 loss: 0.25531716275561067
  batch 449 loss: 0.25529013806435474
  batch 450 loss: 0.2551705378625128
  batch 451 loss: 0.2551771115239074
  batch 452 loss: 0.2551239158265886
  batch 453 loss: 0.2550859997011968
  batch 454 loss: 0.2550836625931547
  batch 455 loss: 0.2551313904288051
  batch 456 loss: 0.2550741880805346
  batch 457 loss: 0.25511361208603733
  batch 458 loss: 0.2550564838418794
  batch 459 loss: 0.2550532876276502
  batch 460 loss: 0.25506665395653766
  batch 461 loss: 0.25507060165260464
  batch 462 loss: 0.255071984805586
  batch 463 loss: 0.2550602245987363
  batch 464 loss: 0.25499172235742723
  batch 465 loss: 0.25492334647845194
  batch 466 loss: 0.25492767580333187
  batch 467 loss: 0.2549929503327519
  batch 468 loss: 0.25493750577935803
  batch 469 loss: 0.2548776375078189
  batch 470 loss: 0.2551303240212988
  batch 471 loss: 0.2551985916438376
  batch 472 loss: 0.2550016778000331
LOSS train 0.2550016778000331 valid 0.2015659660100937
LOSS train 0.2550016778000331 valid 0.20172804594039917
LOSS train 0.2550016778000331 valid 0.21987159053484598
LOSS train 0.2550016778000331 valid 0.21070756390690804
LOSS train 0.2550016778000331 valid 0.22031024992465972
LOSS train 0.2550016778000331 valid 0.22092410176992416
LOSS train 0.2550016778000331 valid 0.21269236717905318
LOSS train 0.2550016778000331 valid 0.2118234969675541
LOSS train 0.2550016778000331 valid 0.21260647310150993
LOSS train 0.2550016778000331 valid 0.20945716202259063
LOSS train 0.2550016778000331 valid 0.20903433182022788
LOSS train 0.2550016778000331 valid 0.2108575515449047
LOSS train 0.2550016778000331 valid 0.2123026756139902
LOSS train 0.2550016778000331 valid 0.21066360707793916
LOSS train 0.2550016778000331 valid 0.20964059631029766
LOSS train 0.2550016778000331 valid 0.21288220398128033
LOSS train 0.2550016778000331 valid 0.21252910442212047
LOSS train 0.2550016778000331 valid 0.21252019620603985
LOSS train 0.2550016778000331 valid 0.21352002573640724
LOSS train 0.2550016778000331 valid 0.21320997402071953
LOSS train 0.2550016778000331 valid 0.2147583287386667
LOSS train 0.2550016778000331 valid 0.21316574852574954
LOSS train 0.2550016778000331 valid 0.21157315762146658
LOSS train 0.2550016778000331 valid 0.21148688221971193
LOSS train 0.2550016778000331 valid 0.21211576461791992
LOSS train 0.2550016778000331 valid 0.2112463042140007
LOSS train 0.2550016778000331 valid 0.21168130746594183
LOSS train 0.2550016778000331 valid 0.2120979598590306
LOSS train 0.2550016778000331 valid 0.21064635391893058
LOSS train 0.2550016778000331 valid 0.20940347015857697
LOSS train 0.2550016778000331 valid 0.20908922533835134
LOSS train 0.2550016778000331 valid 0.2098953239619732
LOSS train 0.2550016778000331 valid 0.20923660695552826
LOSS train 0.2550016778000331 valid 0.20861065913649166
LOSS train 0.2550016778000331 valid 0.20930053293704987
LOSS train 0.2550016778000331 valid 0.2101449796722995
LOSS train 0.2550016778000331 valid 0.20918133613225576
LOSS train 0.2550016778000331 valid 0.20895613141750036
LOSS train 0.2550016778000331 valid 0.2085941418623313
LOSS train 0.2550016778000331 valid 0.20903520546853543
LOSS train 0.2550016778000331 valid 0.20843821291516468
LOSS train 0.2550016778000331 valid 0.2105182632803917
LOSS train 0.2550016778000331 valid 0.21076641110486763
LOSS train 0.2550016778000331 valid 0.2101633914492347
LOSS train 0.2550016778000331 valid 0.20995424853430855
LOSS train 0.2550016778000331 valid 0.20951012262831564
LOSS train 0.2550016778000331 valid 0.2092035967618861
LOSS train 0.2550016778000331 valid 0.21092973556369543
LOSS train 0.2550016778000331 valid 0.21072031101401972
LOSS train 0.2550016778000331 valid 0.2113354581594467
LOSS train 0.2550016778000331 valid 0.21100997544971167
LOSS train 0.2550016778000331 valid 0.21112358656067115
LOSS train 0.2550016778000331 valid 0.21205712854862213
LOSS train 0.2550016778000331 valid 0.21204171771252597
LOSS train 0.2550016778000331 valid 0.21187664053656838
LOSS train 0.2550016778000331 valid 0.21184739390654223
LOSS train 0.2550016778000331 valid 0.21128466202501664
LOSS train 0.2550016778000331 valid 0.21204582165027486
LOSS train 0.2550016778000331 valid 0.21199187888937482
LOSS train 0.2550016778000331 valid 0.21197830960154534
LOSS train 0.2550016778000331 valid 0.21223750881484296
LOSS train 0.2550016778000331 valid 0.21211457180400048
LOSS train 0.2550016778000331 valid 0.21209196888265155
LOSS train 0.2550016778000331 valid 0.2120494453702122
LOSS train 0.2550016778000331 valid 0.21084396976691025
LOSS train 0.2550016778000331 valid 0.21059579582828464
LOSS train 0.2550016778000331 valid 0.2114394687449754
LOSS train 0.2550016778000331 valid 0.21071653966518009
LOSS train 0.2550016778000331 valid 0.21121976211451102
LOSS train 0.2550016778000331 valid 0.2117172279528209
LOSS train 0.2550016778000331 valid 0.21206862364016787
LOSS train 0.2550016778000331 valid 0.21210730365580982
LOSS train 0.2550016778000331 valid 0.21256279904548436
LOSS train 0.2550016778000331 valid 0.21268637317257957
LOSS train 0.2550016778000331 valid 0.21254075169563294
LOSS train 0.2550016778000331 valid 0.21273113375431613
LOSS train 0.2550016778000331 valid 0.21294612389106254
LOSS train 0.2550016778000331 valid 0.2127064565817515
LOSS train 0.2550016778000331 valid 0.21278162225137784
LOSS train 0.2550016778000331 valid 0.21256580259650945
LOSS train 0.2550016778000331 valid 0.21247052631260435
LOSS train 0.2550016778000331 valid 0.2122193891100767
LOSS train 0.2550016778000331 valid 0.21200258993958854
LOSS train 0.2550016778000331 valid 0.21184901307736123
LOSS train 0.2550016778000331 valid 0.2121046392356648
LOSS train 0.2550016778000331 valid 0.21187737445498622
LOSS train 0.2550016778000331 valid 0.21141328808219953
LOSS train 0.2550016778000331 valid 0.21128396825356918
LOSS train 0.2550016778000331 valid 0.21143953184063516
LOSS train 0.2550016778000331 valid 0.21159177240398194
LOSS train 0.2550016778000331 valid 0.2117070781660604
LOSS train 0.2550016778000331 valid 0.21143895241877306
LOSS train 0.2550016778000331 valid 0.21147967234093656
LOSS train 0.2550016778000331 valid 0.21177773970238706
LOSS train 0.2550016778000331 valid 0.21176261478348782
LOSS train 0.2550016778000331 valid 0.21220464181775847
LOSS train 0.2550016778000331 valid 0.21225993949727914
LOSS train 0.2550016778000331 valid 0.2124896257811663
LOSS train 0.2550016778000331 valid 0.21258265216543218
LOSS train 0.2550016778000331 valid 0.21249744459986686
LOSS train 0.2550016778000331 valid 0.21267565155383383
LOSS train 0.2550016778000331 valid 0.21313610190854354
LOSS train 0.2550016778000331 valid 0.21278950789021056
LOSS train 0.2550016778000331 valid 0.2126175117893861
LOSS train 0.2550016778000331 valid 0.21290742456912995
LOSS train 0.2550016778000331 valid 0.21309294270456963
LOSS train 0.2550016778000331 valid 0.21278266761904566
LOSS train 0.2550016778000331 valid 0.21272545238887822
LOSS train 0.2550016778000331 valid 0.21205787511046872
LOSS train 0.2550016778000331 valid 0.2118544113906947
LOSS train 0.2550016778000331 valid 0.21198465171697978
LOSS train 0.2550016778000331 valid 0.21193829444902285
LOSS train 0.2550016778000331 valid 0.21170438892018478
LOSS train 0.2550016778000331 valid 0.21153542883040613
LOSS train 0.2550016778000331 valid 0.21203476073949235
LOSS train 0.2550016778000331 valid 0.21175062772015046
LOSS train 0.2550016778000331 valid 0.21220285502763894
LOSS train 0.2550016778000331 valid 0.21239759861412694
LOSS train 0.2550016778000331 valid 0.21207702710848897
LOSS train 0.2550016778000331 valid 0.2118507970124483
LOSS train 0.2550016778000331 valid 0.21177737180851708
LOSS train 0.2550016778000331 valid 0.2120050480131243
LOSS train 0.2550016778000331 valid 0.21199053974170995
LOSS train 0.2550016778000331 valid 0.21201772290852763
LOSS train 0.2550016778000331 valid 0.2120755842924118
LOSS train 0.2550016778000331 valid 0.2122637207309405
LOSS train 0.2550016778000331 valid 0.21216908648727448
LOSS train 0.2550016778000331 valid 0.21203808498103172
LOSS train 0.2550016778000331 valid 0.2116938009280567
LOSS train 0.2550016778000331 valid 0.21135466087322968
LOSS train 0.2550016778000331 valid 0.21136161400616624
LOSS train 0.2550016778000331 valid 0.2113114164182634
LOSS train 0.2550016778000331 valid 0.2112233587225577
LOSS train 0.2550016778000331 valid 0.21137598245891173
LOSS train 0.2550016778000331 valid 0.21159295152734828
LOSS train 0.2550016778000331 valid 0.2117922492325306
LOSS train 0.2550016778000331 valid 0.21185818151400906
LOSS train 0.2550016778000331 valid 0.21178784016249835
LOSS train 0.2550016778000331 valid 0.211690811373347
LOSS train 0.2550016778000331 valid 0.211962710746697
LOSS train 0.2550016778000331 valid 0.2121281678794969
LOSS train 0.2550016778000331 valid 0.21243138506378925
LOSS train 0.2550016778000331 valid 0.21248071197863225
LOSS train 0.2550016778000331 valid 0.21258634401278365
LOSS train 0.2550016778000331 valid 0.2124423889250591
LOSS train 0.2550016778000331 valid 0.2123433840397286
LOSS train 0.2550016778000331 valid 0.21231937925426328
LOSS train 0.2550016778000331 valid 0.21206323832676216
LOSS train 0.2550016778000331 valid 0.2121379281450438
LOSS train 0.2550016778000331 valid 0.21210921794176102
LOSS train 0.2550016778000331 valid 0.21190258386908778
LOSS train 0.2550016778000331 valid 0.21185493959408058
LOSS train 0.2550016778000331 valid 0.2116915559262232
LOSS train 0.2550016778000331 valid 0.21170213983043448
LOSS train 0.2550016778000331 valid 0.21170274672969694
LOSS train 0.2550016778000331 valid 0.211988997956117
LOSS train 0.2550016778000331 valid 0.2119120544498893
LOSS train 0.2550016778000331 valid 0.21186173499762256
LOSS train 0.2550016778000331 valid 0.21207044812493353
LOSS train 0.2550016778000331 valid 0.2121189053170383
LOSS train 0.2550016778000331 valid 0.21224479728974172
LOSS train 0.2550016778000331 valid 0.21229981005559734
LOSS train 0.2550016778000331 valid 0.21200553167817052
LOSS train 0.2550016778000331 valid 0.21212501533147765
LOSS train 0.2550016778000331 valid 0.21225200485099446
LOSS train 0.2550016778000331 valid 0.21213668468127767
LOSS train 0.2550016778000331 valid 0.21224150186527274
LOSS train 0.2550016778000331 valid 0.2119998519441911
LOSS train 0.2550016778000331 valid 0.21177760877552823
LOSS train 0.2550016778000331 valid 0.21187087279908798
LOSS train 0.2550016778000331 valid 0.211689850914548
LOSS train 0.2550016778000331 valid 0.21154508549113607
LOSS train 0.2550016778000331 valid 0.21152879685335765
LOSS train 0.2550016778000331 valid 0.21160559458979245
LOSS train 0.2550016778000331 valid 0.2115050986834935
LOSS train 0.2550016778000331 valid 0.21145053986798634
LOSS train 0.2550016778000331 valid 0.21151008065474236
LOSS train 0.2550016778000331 valid 0.21153647435849973
LOSS train 0.2550016778000331 valid 0.21168080063838532
LOSS train 0.2550016778000331 valid 0.21173334990938505
LOSS train 0.2550016778000331 valid 0.21162149301879313
LOSS train 0.2550016778000331 valid 0.21160258852191025
LOSS train 0.2550016778000331 valid 0.21160574249231098
LOSS train 0.2550016778000331 valid 0.21150361778943436
LOSS train 0.2550016778000331 valid 0.21153005345447642
LOSS train 0.2550016778000331 valid 0.21158003238260106
LOSS train 0.2550016778000331 valid 0.21135309545751563
LOSS train 0.2550016778000331 valid 0.21134474619906
LOSS train 0.2550016778000331 valid 0.21131340481301464
LOSS train 0.2550016778000331 valid 0.21129425230779145
LOSS train 0.2550016778000331 valid 0.21153912244666934
LOSS train 0.2550016778000331 valid 0.21161733893677592
LOSS train 0.2550016778000331 valid 0.21141768760322907
LOSS train 0.2550016778000331 valid 0.21146542440677427
LOSS train 0.2550016778000331 valid 0.21136137468692584
LOSS train 0.2550016778000331 valid 0.2116105470578281
LOSS train 0.2550016778000331 valid 0.21153741908557525
LOSS train 0.2550016778000331 valid 0.21182462091397758
LOSS train 0.2550016778000331 valid 0.21178799628013342
LOSS train 0.2550016778000331 valid 0.2117421567440033
LOSS train 0.2550016778000331 valid 0.2117204298427449
LOSS train 0.2550016778000331 valid 0.21182897104190127
LOSS train 0.2550016778000331 valid 0.21198491062143165
LOSS train 0.2550016778000331 valid 0.21188126212241604
LOSS train 0.2550016778000331 valid 0.211831893426616
LOSS train 0.2550016778000331 valid 0.2118275582935046
LOSS train 0.2550016778000331 valid 0.2118053173529353
LOSS train 0.2550016778000331 valid 0.2115922924131155
LOSS train 0.2550016778000331 valid 0.21159982852388226
LOSS train 0.2550016778000331 valid 0.21163720055705026
LOSS train 0.2550016778000331 valid 0.21183404630111857
LOSS train 0.2550016778000331 valid 0.21172984450493218
LOSS train 0.2550016778000331 valid 0.21183798437667006
LOSS train 0.2550016778000331 valid 0.21176915316381187
LOSS train 0.2550016778000331 valid 0.21147217778272406
LOSS train 0.2550016778000331 valid 0.21125710162299652
LOSS train 0.2550016778000331 valid 0.21103109268274176
LOSS train 0.2550016778000331 valid 0.21098995270258789
LOSS train 0.2550016778000331 valid 0.21098418561018764
LOSS train 0.2550016778000331 valid 0.21093953963030468
LOSS train 0.2550016778000331 valid 0.2108357408467461
LOSS train 0.2550016778000331 valid 0.21096445029383307
LOSS train 0.2550016778000331 valid 0.21087755236123173
LOSS train 0.2550016778000331 valid 0.21075718523934484
LOSS train 0.2550016778000331 valid 0.2106103984514872
LOSS train 0.2550016778000331 valid 0.21045720142073335
LOSS train 0.2550016778000331 valid 0.21036101982719574
LOSS train 0.2550016778000331 valid 0.21027005757940442
LOSS train 0.2550016778000331 valid 0.21012546102552956
LOSS train 0.2550016778000331 valid 0.21012399961119113
LOSS train 0.2550016778000331 valid 0.2101325582374226
LOSS train 0.2550016778000331 valid 0.2101772143023795
LOSS train 0.2550016778000331 valid 0.2103720102059483
LOSS train 0.2550016778000331 valid 0.21025032830289286
LOSS train 0.2550016778000331 valid 0.2102123720214722
LOSS train 0.2550016778000331 valid 0.2100274490855508
LOSS train 0.2550016778000331 valid 0.2100655470467821
LOSS train 0.2550016778000331 valid 0.21016892227305084
LOSS train 0.2550016778000331 valid 0.21008556457742986
LOSS train 0.2550016778000331 valid 0.20976792561511198
LOSS train 0.2550016778000331 valid 0.20962581341444705
LOSS train 0.2550016778000331 valid 0.20967150300987497
LOSS train 0.2550016778000331 valid 0.20962117471322111
LOSS train 0.2550016778000331 valid 0.20960125876743285
LOSS train 0.2550016778000331 valid 0.20961855370171217
LOSS train 0.2550016778000331 valid 0.2096446699243251
LOSS train 0.2550016778000331 valid 0.20966007412686521
LOSS train 0.2550016778000331 valid 0.20974805157992146
LOSS train 0.2550016778000331 valid 0.20956925180063668
LOSS train 0.2550016778000331 valid 0.20975287902355194
LOSS train 0.2550016778000331 valid 0.20998072054281652
LOSS train 0.2550016778000331 valid 0.21002949928007428
LOSS train 0.2550016778000331 valid 0.2099362794354028
LOSS train 0.2550016778000331 valid 0.2100734218485712
LOSS train 0.2550016778000331 valid 0.21003579503180933
LOSS train 0.2550016778000331 valid 0.20994747133227065
LOSS train 0.2550016778000331 valid 0.20993297022372367
LOSS train 0.2550016778000331 valid 0.20994187706662704
LOSS train 0.2550016778000331 valid 0.20984582159970258
LOSS train 0.2550016778000331 valid 0.20970516972816908
LOSS train 0.2550016778000331 valid 0.20964076652609068
LOSS train 0.2550016778000331 valid 0.20983371531007855
LOSS train 0.2550016778000331 valid 0.20972181516681787
LOSS train 0.2550016778000331 valid 0.20977082007536382
LOSS train 0.2550016778000331 valid 0.20968183464599105
LOSS train 0.2550016778000331 valid 0.20965734734795147
LOSS train 0.2550016778000331 valid 0.20956045612413784
LOSS train 0.2550016778000331 valid 0.20966591085515804
LOSS train 0.2550016778000331 valid 0.20977575135275334
LOSS train 0.2550016778000331 valid 0.20966844067529397
LOSS train 0.2550016778000331 valid 0.2096051288376875
LOSS train 0.2550016778000331 valid 0.2094817509436432
LOSS train 0.2550016778000331 valid 0.20941530222639496
LOSS train 0.2550016778000331 valid 0.20935921082748984
LOSS train 0.2550016778000331 valid 0.2095046159354123
LOSS train 0.2550016778000331 valid 0.20946545753142107
LOSS train 0.2550016778000331 valid 0.2094530735420406
LOSS train 0.2550016778000331 valid 0.20958553029478882
LOSS train 0.2550016778000331 valid 0.20961514131356312
LOSS train 0.2550016778000331 valid 0.20945349410176278
LOSS train 0.2550016778000331 valid 0.20936082420187913
LOSS train 0.2550016778000331 valid 0.20933865951308123
LOSS train 0.2550016778000331 valid 0.20941857540775946
LOSS train 0.2550016778000331 valid 0.2092658664873788
LOSS train 0.2550016778000331 valid 0.2093685422550168
LOSS train 0.2550016778000331 valid 0.2093611782143166
LOSS train 0.2550016778000331 valid 0.2093852241799391
LOSS train 0.2550016778000331 valid 0.2093718419265416
LOSS train 0.2550016778000331 valid 0.20937014868721418
LOSS train 0.2550016778000331 valid 0.20929744361803448
LOSS train 0.2550016778000331 valid 0.20929418862685306
LOSS train 0.2550016778000331 valid 0.20930033728276212
LOSS train 0.2550016778000331 valid 0.20928253208004166
LOSS train 0.2550016778000331 valid 0.20916359978062765
LOSS train 0.2550016778000331 valid 0.20920575129783758
LOSS train 0.2550016778000331 valid 0.2091888054399877
LOSS train 0.2550016778000331 valid 0.2092223027940551
LOSS train 0.2550016778000331 valid 0.2092912138108439
LOSS train 0.2550016778000331 valid 0.20924681719130894
LOSS train 0.2550016778000331 valid 0.20920592531561852
LOSS train 0.2550016778000331 valid 0.2091388453874873
LOSS train 0.2550016778000331 valid 0.20915838469140577
LOSS train 0.2550016778000331 valid 0.20907983955967152
LOSS train 0.2550016778000331 valid 0.20910146582479539
LOSS train 0.2550016778000331 valid 0.20906300539853143
LOSS train 0.2550016778000331 valid 0.20927613900572645
LOSS train 0.2550016778000331 valid 0.2092126708368525
LOSS train 0.2550016778000331 valid 0.20937012692357038
LOSS train 0.2550016778000331 valid 0.20942560354289885
LOSS train 0.2550016778000331 valid 0.20946881194268505
LOSS train 0.2550016778000331 valid 0.2095209248962893
LOSS train 0.2550016778000331 valid 0.20950665143437874
LOSS train 0.2550016778000331 valid 0.20965756299777533
LOSS train 0.2550016778000331 valid 0.20959523566968882
LOSS train 0.2550016778000331 valid 0.2095793638910566
LOSS train 0.2550016778000331 valid 0.20962963984171046
LOSS train 0.2550016778000331 valid 0.20959012901369328
LOSS train 0.2550016778000331 valid 0.20961754478561054
LOSS train 0.2550016778000331 valid 0.20961127469905866
LOSS train 0.2550016778000331 valid 0.20951086641289293
LOSS train 0.2550016778000331 valid 0.2095320625375736
LOSS train 0.2550016778000331 valid 0.2095084470343886
LOSS train 0.2550016778000331 valid 0.20936128898486264
LOSS train 0.2550016778000331 valid 0.20934270271732483
LOSS train 0.2550016778000331 valid 0.20916364339681773
LOSS train 0.2550016778000331 valid 0.20927532113220063
LOSS train 0.2550016778000331 valid 0.2092641159573097
LOSS train 0.2550016778000331 valid 0.20930984289180943
LOSS train 0.2550016778000331 valid 0.20927891909713803
LOSS train 0.2550016778000331 valid 0.20929543001182152
LOSS train 0.2550016778000331 valid 0.20925966936115772
LOSS train 0.2550016778000331 valid 0.20919903794146447
LOSS train 0.2550016778000331 valid 0.20929260892016036
LOSS train 0.2550016778000331 valid 0.2092633705563888
LOSS train 0.2550016778000331 valid 0.20923605854831526
LOSS train 0.2550016778000331 valid 0.20919010726114115
LOSS train 0.2550016778000331 valid 0.20936332735887975
LOSS train 0.2550016778000331 valid 0.2092862263233704
LOSS train 0.2550016778000331 valid 0.20924073110463698
LOSS train 0.2550016778000331 valid 0.20934522392995217
LOSS train 0.2550016778000331 valid 0.2093348608425985
LOSS train 0.2550016778000331 valid 0.2092621525564389
LOSS train 0.2550016778000331 valid 0.2092020240973453
LOSS train 0.2550016778000331 valid 0.20923480491132238
LOSS train 0.2550016778000331 valid 0.20929869793463443
LOSS train 0.2550016778000331 valid 0.209221812083542
LOSS train 0.2550016778000331 valid 0.2091652549137643
LOSS train 0.2550016778000331 valid 0.20921075181371865
LOSS train 0.2550016778000331 valid 0.2091457574791075
LOSS train 0.2550016778000331 valid 0.2091755764399256
LOSS train 0.2550016778000331 valid 0.20927847662882249
LOSS train 0.2550016778000331 valid 0.20935675197026946
LOSS train 0.2550016778000331 valid 0.20945417429333707
LOSS train 0.2550016778000331 valid 0.20945105146048432
LOSS train 0.2550016778000331 valid 0.20936128039595106
LOSS train 0.2550016778000331 valid 0.20934215430798156
LOSS train 0.2550016778000331 valid 0.209412856387491
LOSS train 0.2550016778000331 valid 0.20939806403394517
LOSS train 0.2550016778000331 valid 0.2093654217245187
LOSS train 0.2550016778000331 valid 0.20951350434786745
LOSS train 0.2550016778000331 valid 0.2094670314125077
LOSS train 0.2550016778000331 valid 0.20956903568453553
LOSS train 0.2550016778000331 valid 0.20960521615897984
LOSS train 0.2550016778000331 valid 0.20949168200348758
LOSS train 0.2550016778000331 valid 0.2095905932253354
LOSS train 0.2550016778000331 valid 0.20965135288857373
LOSS train 0.2550016778000331 valid 0.2096326936249512
LOSS train 0.2550016778000331 valid 0.2095232084798424
LOSS train 0.2550016778000331 valid 0.2096064646392657
EPOCH 12:
  batch 1 loss: 0.26517003774642944
  batch 2 loss: 0.2873245179653168
  batch 3 loss: 0.2740405301253001
  batch 4 loss: 0.2781538665294647
  batch 5 loss: 0.28511842489242556
  batch 6 loss: 0.27621973554293316
  batch 7 loss: 0.27947903105190824
  batch 8 loss: 0.27822257950901985
  batch 9 loss: 0.27808430790901184
  batch 10 loss: 0.2738057762384415
  batch 11 loss: 0.2744157233021476
  batch 12 loss: 0.27335479607184726
  batch 13 loss: 0.27210492812670195
  batch 14 loss: 0.27097312254565104
  batch 15 loss: 0.26964881221453346
  batch 16 loss: 0.26957077719271183
  batch 17 loss: 0.27053701702286215
  batch 18 loss: 0.27002998027536607
  batch 19 loss: 0.26844104575483424
  batch 20 loss: 0.2651595562696457
  batch 21 loss: 0.2679242222082047
  batch 22 loss: 0.26812161776152527
  batch 23 loss: 0.2655183802480283
  batch 24 loss: 0.26620567838350934
  batch 25 loss: 0.26634199619293214
  batch 26 loss: 0.26703683458841765
  batch 27 loss: 0.26637598209910923
  batch 28 loss: 0.26664658742291586
  batch 29 loss: 0.2662250070736326
  batch 30 loss: 0.2662234624226888
  batch 31 loss: 0.266348437916848
  batch 32 loss: 0.26768782269209623
  batch 33 loss: 0.26974844661625946
  batch 34 loss: 0.27018927826600914
  batch 35 loss: 0.27094133240836005
  batch 36 loss: 0.2709907367825508
  batch 37 loss: 0.2709112296233306
  batch 38 loss: 0.2722403783547251
  batch 39 loss: 0.27266102417921406
  batch 40 loss: 0.2721277318894863
  batch 41 loss: 0.2725944053835985
  batch 42 loss: 0.2730175356070201
  batch 43 loss: 0.2730465122433596
  batch 44 loss: 0.27277073196389456
  batch 45 loss: 0.2723670224348704
  batch 46 loss: 0.27111769370410754
  batch 47 loss: 0.27134282665049775
  batch 48 loss: 0.2706068729360898
  batch 49 loss: 0.2705888857646864
  batch 50 loss: 0.2702158254384994
  batch 51 loss: 0.27058715738502204
  batch 52 loss: 0.26954370421858936
  batch 53 loss: 0.26834833790671153
  batch 54 loss: 0.2681419054667155
  batch 55 loss: 0.26753514246507126
  batch 56 loss: 0.2671358920633793
  batch 57 loss: 0.2663299318468362
  batch 58 loss: 0.26543116466752414
  batch 59 loss: 0.26571618399377595
  batch 60 loss: 0.2657437205314636
  batch 61 loss: 0.26637084708839165
  batch 62 loss: 0.26711593472188516
  batch 63 loss: 0.26674555691461715
  batch 64 loss: 0.2663896046578884
  batch 65 loss: 0.2659340697985429
  batch 66 loss: 0.2657299484267379
  batch 67 loss: 0.2656485847572782
  batch 68 loss: 0.2653824976261924
  batch 69 loss: 0.2654311825399813
  batch 70 loss: 0.2647311542715345
  batch 71 loss: 0.26478766849343205
  batch 72 loss: 0.2648908831179142
  batch 73 loss: 0.2643246615994467
  batch 74 loss: 0.2639942982712308
  batch 75 loss: 0.2637966271241506
  batch 76 loss: 0.2634129375219345
  batch 77 loss: 0.2630368120871581
  batch 78 loss: 0.2628797711088107
  batch 79 loss: 0.26270354341102553
  batch 80 loss: 0.26208676490932703
  batch 81 loss: 0.2615859013648681
  batch 82 loss: 0.2618224092009591
  batch 83 loss: 0.2616292121539633
  batch 84 loss: 0.26102742180228233
  batch 85 loss: 0.26110822716180015
  batch 86 loss: 0.2611709869531698
  batch 87 loss: 0.2609253839172166
  batch 88 loss: 0.26096915131942794
  batch 89 loss: 0.2606428779577941
  batch 90 loss: 0.2608965355488989
  batch 91 loss: 0.260765247128822
  batch 92 loss: 0.2607904845929664
  batch 93 loss: 0.2606851271083278
  batch 94 loss: 0.2609864741246751
  batch 95 loss: 0.2612609400560981
  batch 96 loss: 0.26146513829007745
  batch 97 loss: 0.2618731519917852
  batch 98 loss: 0.2617734608297445
  batch 99 loss: 0.26173506225600385
  batch 100 loss: 0.261891301125288
  batch 101 loss: 0.2618653052809215
  batch 102 loss: 0.2618998398675638
  batch 103 loss: 0.26175279770661325
  batch 104 loss: 0.26207792429396737
  batch 105 loss: 0.2620622375181743
  batch 106 loss: 0.26206384022843165
  batch 107 loss: 0.26193683336828355
  batch 108 loss: 0.26158872757244994
  batch 109 loss: 0.2616873751266287
  batch 110 loss: 0.2613999804312533
  batch 111 loss: 0.26129094034701855
  batch 112 loss: 0.26112961316747324
  batch 113 loss: 0.2607585700739801
  batch 114 loss: 0.2606029390243062
  batch 115 loss: 0.26006253465362217
  batch 116 loss: 0.25979710678602086
  batch 117 loss: 0.2594042051042247
  batch 118 loss: 0.2594029751874633
  batch 119 loss: 0.25944203989846365
  batch 120 loss: 0.2590260150531928
  batch 121 loss: 0.25900376705098743
  batch 122 loss: 0.25877609482554137
  batch 123 loss: 0.2587279166147961
  batch 124 loss: 0.2584756970886261
  batch 125 loss: 0.25815056133270264
  batch 126 loss: 0.25829293945479015
  batch 127 loss: 0.25808563969266696
  batch 128 loss: 0.258087009890005
  batch 129 loss: 0.25821164920348527
  batch 130 loss: 0.25811006186100155
  batch 131 loss: 0.2580627365649201
  batch 132 loss: 0.25797919872583763
  batch 133 loss: 0.25772694347048164
  batch 134 loss: 0.25757684469667835
  batch 135 loss: 0.257433118753963
  batch 136 loss: 0.25756382054704074
  batch 137 loss: 0.25779256431290704
  batch 138 loss: 0.25785659350778745
  batch 139 loss: 0.25785379797863445
  batch 140 loss: 0.2580256297120026
  batch 141 loss: 0.257677194602946
  batch 142 loss: 0.2575147045959889
  batch 143 loss: 0.25763789350753064
  batch 144 loss: 0.2576083270832896
  batch 145 loss: 0.25750739502495734
  batch 146 loss: 0.2574928385550029
  batch 147 loss: 0.25739811208783364
  batch 148 loss: 0.2574410462701643
  batch 149 loss: 0.2568944504597043
  batch 150 loss: 0.2568056853612264
  batch 151 loss: 0.256387504915528
  batch 152 loss: 0.2561601337633635
  batch 153 loss: 0.2562754518455929
  batch 154 loss: 0.25610994547605515
  batch 155 loss: 0.25606794847596076
  batch 156 loss: 0.25594481529715735
  batch 157 loss: 0.2558744071395534
  batch 158 loss: 0.25611419824859766
  batch 159 loss: 0.2562043550629286
  batch 160 loss: 0.25604338962584733
  batch 161 loss: 0.25580299159754877
  batch 162 loss: 0.25568551551780583
  batch 163 loss: 0.25549917983496845
  batch 164 loss: 0.2556520227070262
  batch 165 loss: 0.25574628123731324
  batch 166 loss: 0.2558247842882053
  batch 167 loss: 0.2557012855471251
  batch 168 loss: 0.2554363901061671
  batch 169 loss: 0.25562768451560886
  batch 170 loss: 0.2556648824144812
  batch 171 loss: 0.25570066247070045
  batch 172 loss: 0.25570878438478295
  batch 173 loss: 0.2558201334380001
  batch 174 loss: 0.25592391210040827
  batch 175 loss: 0.25570590734481813
  batch 176 loss: 0.2558359916914593
  batch 177 loss: 0.2560234987466349
  batch 178 loss: 0.25562517512380406
  batch 179 loss: 0.2559134758717521
  batch 180 loss: 0.2557958806554476
  batch 181 loss: 0.25560330249657287
  batch 182 loss: 0.2554783995498668
  batch 183 loss: 0.2551447909549286
  batch 184 loss: 0.2548742602862742
  batch 185 loss: 0.2547088029416832
  batch 186 loss: 0.25459187998566574
  batch 187 loss: 0.25482328339694016
  batch 188 loss: 0.2547203423653511
  batch 189 loss: 0.25468583933260075
  batch 190 loss: 0.2546405492644561
  batch 191 loss: 0.25488366100800597
  batch 192 loss: 0.2547141508354495
  batch 193 loss: 0.25468858806271627
  batch 194 loss: 0.25460534458307876
  batch 195 loss: 0.2547464720713787
  batch 196 loss: 0.25455426728847075
  batch 197 loss: 0.25459589752448997
  batch 198 loss: 0.25462030807528835
  batch 199 loss: 0.2546634417981958
  batch 200 loss: 0.25461407512426376
  batch 201 loss: 0.25443136462228216
  batch 202 loss: 0.25436989918793784
  batch 203 loss: 0.25423095246841165
  batch 204 loss: 0.25411563541959314
  batch 205 loss: 0.2539878718009809
  batch 206 loss: 0.25396260322587005
  batch 207 loss: 0.2538808039996935
  batch 208 loss: 0.2537842109226264
  batch 209 loss: 0.2536805638571105
  batch 210 loss: 0.2536904987834749
  batch 211 loss: 0.2536502827965253
  batch 212 loss: 0.25370854139328003
  batch 213 loss: 0.25358990989100766
  batch 214 loss: 0.2535341346514559
  batch 215 loss: 0.25348493484563606
  batch 216 loss: 0.25363238624952456
  batch 217 loss: 0.25346342649327996
  batch 218 loss: 0.25339829668812797
  batch 219 loss: 0.253168359739051
  batch 220 loss: 0.2533141145651991
  batch 221 loss: 0.25314533993669225
  batch 222 loss: 0.25305148234238495
  batch 223 loss: 0.2530681240986281
  batch 224 loss: 0.2530845993065408
  batch 225 loss: 0.25307429843478735
  batch 226 loss: 0.25322518398803945
  batch 227 loss: 0.25316658491342603
  batch 228 loss: 0.25323203447879405
  batch 229 loss: 0.25327671869092633
  batch 230 loss: 0.2534279182553291
  batch 231 loss: 0.25348422298957773
  batch 232 loss: 0.25331785549120656
  batch 233 loss: 0.25331285858103136
  batch 234 loss: 0.25325710549313796
  batch 235 loss: 0.25307334464915254
  batch 236 loss: 0.25306768166059157
  batch 237 loss: 0.2531431578256913
  batch 238 loss: 0.25311252888010327
  batch 239 loss: 0.2531200280748152
  batch 240 loss: 0.25308812217166027
  batch 241 loss: 0.25312399177877737
  batch 242 loss: 0.25308683711635177
  batch 243 loss: 0.2530094031934385
  batch 244 loss: 0.2529971966000854
  batch 245 loss: 0.25312390449095745
  batch 246 loss: 0.2531073672500083
  batch 247 loss: 0.2530674230955873
  batch 248 loss: 0.253357055086282
  batch 249 loss: 0.2534967087598211
  batch 250 loss: 0.25335810524225233
  batch 251 loss: 0.2532948586214111
  batch 252 loss: 0.2533453515479489
  batch 253 loss: 0.2534024606462524
  batch 254 loss: 0.2534390253227527
  batch 255 loss: 0.253357311557321
  batch 256 loss: 0.2534969806438312
  batch 257 loss: 0.2535823926851443
  batch 258 loss: 0.25345026401355286
  batch 259 loss: 0.25337269896475967
  batch 260 loss: 0.2534414665630231
  batch 261 loss: 0.25377729940460103
  batch 262 loss: 0.2537445857097174
  batch 263 loss: 0.2537135075027045
  batch 264 loss: 0.2539534224479487
  batch 265 loss: 0.2542211670920534
  batch 266 loss: 0.2542489484736794
  batch 267 loss: 0.2542147689432687
  batch 268 loss: 0.25421875331606436
  batch 269 loss: 0.25424001867221635
  batch 270 loss: 0.2543157759088057
  batch 271 loss: 0.2544922677463271
  batch 272 loss: 0.25442787122857924
  batch 273 loss: 0.2543387458328799
  batch 274 loss: 0.2543264649754023
  batch 275 loss: 0.25432313989509236
  batch 276 loss: 0.2542958838892156
  batch 277 loss: 0.25427657317383623
  batch 278 loss: 0.25424353065464994
  batch 279 loss: 0.25426169957524986
  batch 280 loss: 0.2540460460420166
  batch 281 loss: 0.2540408647039183
  batch 282 loss: 0.25399364673710884
  batch 283 loss: 0.2539130908440364
  batch 284 loss: 0.253859174083656
  batch 285 loss: 0.25378173025030837
  batch 286 loss: 0.25374843409428227
  batch 287 loss: 0.2536347303864017
  batch 288 loss: 0.253617731285178
  batch 289 loss: 0.2534739218884273
  batch 290 loss: 0.2533225426899976
  batch 291 loss: 0.25324734436072843
  batch 292 loss: 0.25317445616811923
  batch 293 loss: 0.2531112811366039
  batch 294 loss: 0.2530095529596822
  batch 295 loss: 0.2529332953994557
  batch 296 loss: 0.2529400084671137
  batch 297 loss: 0.2529025597102714
  batch 298 loss: 0.2528388837239886
  batch 299 loss: 0.25282519921211893
  batch 300 loss: 0.2528023112316926
  batch 301 loss: 0.2527832231450319
  batch 302 loss: 0.2528865057111576
  batch 303 loss: 0.25282295549860095
  batch 304 loss: 0.2529117673341381
  batch 305 loss: 0.2529037812205612
  batch 306 loss: 0.2530701932272101
  batch 307 loss: 0.2531265954726682
  batch 308 loss: 0.253222616413584
  batch 309 loss: 0.25317108361080626
  batch 310 loss: 0.2533038225866133
  batch 311 loss: 0.253223446021126
  batch 312 loss: 0.253258077475505
  batch 313 loss: 0.25326139953570626
  batch 314 loss: 0.25322608308048006
  batch 315 loss: 0.25316570462688565
  batch 316 loss: 0.2531368653702585
  batch 317 loss: 0.2532192401423439
  batch 318 loss: 0.2531270773718192
  batch 319 loss: 0.2530371589253315
  batch 320 loss: 0.253138188412413
  batch 321 loss: 0.2530462815661297
  batch 322 loss: 0.2529336957735305
  batch 323 loss: 0.25288853427573993
  batch 324 loss: 0.2529160807162155
  batch 325 loss: 0.25288219846211946
  batch 326 loss: 0.25299618297193677
  batch 327 loss: 0.25299333168096866
  batch 328 loss: 0.25298031228708057
  batch 329 loss: 0.25305971231504054
  batch 330 loss: 0.25306134512930206
  batch 331 loss: 0.2530370802587613
  batch 332 loss: 0.2531664533518165
  batch 333 loss: 0.2531572946825543
  batch 334 loss: 0.2532669343723508
  batch 335 loss: 0.2532806366682053
  batch 336 loss: 0.25328759463237865
  batch 337 loss: 0.253247355804599
  batch 338 loss: 0.25318349514310884
  batch 339 loss: 0.2531734369145734
  batch 340 loss: 0.25323975769912493
  batch 341 loss: 0.2532462322117646
  batch 342 loss: 0.2532919971217886
  batch 343 loss: 0.2533573959207396
  batch 344 loss: 0.25335551893641783
  batch 345 loss: 0.2533230338839517
  batch 346 loss: 0.2532699752835869
  batch 347 loss: 0.25309898053706553
  batch 348 loss: 0.25304173845155487
  batch 349 loss: 0.2531349027481325
  batch 350 loss: 0.2531475028395653
  batch 351 loss: 0.25317181230780067
  batch 352 loss: 0.2531001695685766
  batch 353 loss: 0.25310819032509674
  batch 354 loss: 0.2532005571881257
  batch 355 loss: 0.25315130084333287
  batch 356 loss: 0.25314708991666857
  batch 357 loss: 0.2530152574807656
  batch 358 loss: 0.2528888272673058
  batch 359 loss: 0.25287768543596717
  batch 360 loss: 0.25277634842528235
  batch 361 loss: 0.25269723929196514
  batch 362 loss: 0.25260535977492676
  batch 363 loss: 0.2526793753312639
  batch 364 loss: 0.252579198802238
  batch 365 loss: 0.25257388022664473
  batch 366 loss: 0.2525432652544454
  batch 367 loss: 0.2524957866369866
  batch 368 loss: 0.2523294824539967
  batch 369 loss: 0.25228674350392205
  batch 370 loss: 0.2523761222491393
  batch 371 loss: 0.2523181240031661
  batch 372 loss: 0.2523278378510988
  batch 373 loss: 0.25242888935449614
  batch 374 loss: 0.2523960428521595
  batch 375 loss: 0.25236212130387625
  batch 376 loss: 0.2525079693011147
  batch 377 loss: 0.2524670417413155
  batch 378 loss: 0.25239036322901487
  batch 379 loss: 0.2523850984183339
  batch 380 loss: 0.2523292656007566
  batch 381 loss: 0.2522751687392788
  batch 382 loss: 0.2522352205209083
  batch 383 loss: 0.25209597146075324
  batch 384 loss: 0.25208509360284853
  batch 385 loss: 0.2520012546669353
  batch 386 loss: 0.2519093461218893
  batch 387 loss: 0.25183815079758026
  batch 388 loss: 0.2517996442302601
  batch 389 loss: 0.2517465029323622
  batch 390 loss: 0.25167240378184197
  batch 391 loss: 0.2514824792552177
  batch 392 loss: 0.2515168060757676
  batch 393 loss: 0.2515068663895585
  batch 394 loss: 0.25151540709631093
  batch 395 loss: 0.2514923311109784
  batch 396 loss: 0.25155443882550854
  batch 397 loss: 0.2516203637162144
  batch 398 loss: 0.25160377562495334
  batch 399 loss: 0.25166259788182144
  batch 400 loss: 0.2517567748203874
  batch 401 loss: 0.25174448514668424
  batch 402 loss: 0.25176137452250097
  batch 403 loss: 0.2517205682345123
  batch 404 loss: 0.2517199454331162
  batch 405 loss: 0.2517736809489168
  batch 406 loss: 0.2517717038028933
  batch 407 loss: 0.2518224738153837
  batch 408 loss: 0.25179919658922684
  batch 409 loss: 0.25186117937046043
  batch 410 loss: 0.2517909724901362
  batch 411 loss: 0.25175596396563404
  batch 412 loss: 0.2518335849553057
  batch 413 loss: 0.25186653796033187
  batch 414 loss: 0.25191140092081493
  batch 415 loss: 0.25181691876376966
  batch 416 loss: 0.25189150404185057
  batch 417 loss: 0.2518640660124717
  batch 418 loss: 0.25181529168306926
  batch 419 loss: 0.2517748932423057
  batch 420 loss: 0.2517891771736599
  batch 421 loss: 0.2518130389358539
  batch 422 loss: 0.25174521337047007
  batch 423 loss: 0.2517451125749741
  batch 424 loss: 0.2518413822113905
  batch 425 loss: 0.2517211214584463
  batch 426 loss: 0.25159129031667127
  batch 427 loss: 0.25154316774296814
  batch 428 loss: 0.2514772760185683
  batch 429 loss: 0.25138154111819944
  batch 430 loss: 0.2514209108297215
  batch 431 loss: 0.25135137880622926
  batch 432 loss: 0.25131774600595236
  batch 433 loss: 0.25120299108171573
  batch 434 loss: 0.2511274479844603
  batch 435 loss: 0.25118447463402804
  batch 436 loss: 0.25124861775051566
  batch 437 loss: 0.25121245569031625
  batch 438 loss: 0.25118152041957803
  batch 439 loss: 0.2510750190543694
  batch 440 loss: 0.2511408190835606
  batch 441 loss: 0.2511481644074663
  batch 442 loss: 0.2511358617792302
  batch 443 loss: 0.2510439248978419
  batch 444 loss: 0.2512150724326168
  batch 445 loss: 0.25113966156927386
  batch 446 loss: 0.25113932935379013
  batch 447 loss: 0.25115811718123604
  batch 448 loss: 0.2511923860625497
  batch 449 loss: 0.2511787796538232
  batch 450 loss: 0.25108154584964115
  batch 451 loss: 0.25101559181303246
  batch 452 loss: 0.2509487850212418
  batch 453 loss: 0.2509417024669268
  batch 454 loss: 0.25085681447111036
  batch 455 loss: 0.25092256252582257
  batch 456 loss: 0.25092376690161855
  batch 457 loss: 0.2509841120738691
  batch 458 loss: 0.2509243664002314
  batch 459 loss: 0.2508624744051682
  batch 460 loss: 0.25086264435363853
  batch 461 loss: 0.2509043752629948
  batch 462 loss: 0.25089090717432305
  batch 463 loss: 0.2508943329040227
  batch 464 loss: 0.25084815993262777
  batch 465 loss: 0.250794633069346
  batch 466 loss: 0.2507894537991209
  batch 467 loss: 0.25083990372733334
  batch 468 loss: 0.2508221081752553
  batch 469 loss: 0.25070529636035344
  batch 470 loss: 0.25085072174985357
  batch 471 loss: 0.25091516845038997
  batch 472 loss: 0.25072099413659615
LOSS train 0.25072099413659615 valid 0.20105095207691193
LOSS train 0.25072099413659615 valid 0.20619796216487885
LOSS train 0.25072099413659615 valid 0.22784586747487387
LOSS train 0.25072099413659615 valid 0.21721139922738075
LOSS train 0.25072099413659615 valid 0.22667567431926727
LOSS train 0.25072099413659615 valid 0.22820298373699188
LOSS train 0.25072099413659615 valid 0.22008264916283743
LOSS train 0.25072099413659615 valid 0.2191516850143671
LOSS train 0.25072099413659615 valid 0.21971468130747476
LOSS train 0.25072099413659615 valid 0.21616996973752975
LOSS train 0.25072099413659615 valid 0.21476022086360239
LOSS train 0.25072099413659615 valid 0.21562912811835608
LOSS train 0.25072099413659615 valid 0.2179160863161087
LOSS train 0.25072099413659615 valid 0.2161850013903209
LOSS train 0.25072099413659615 valid 0.2150548199812571
LOSS train 0.25072099413659615 valid 0.21810184232890606
LOSS train 0.25072099413659615 valid 0.2175787257797578
LOSS train 0.25072099413659615 valid 0.21701625237862268
LOSS train 0.25072099413659615 valid 0.21803835034370422
LOSS train 0.25072099413659615 valid 0.21781146973371507
LOSS train 0.25072099413659615 valid 0.2193801019872938
LOSS train 0.25072099413659615 valid 0.21777725219726562
LOSS train 0.25072099413659615 valid 0.21608018292033154
LOSS train 0.25072099413659615 valid 0.21596086459855238
LOSS train 0.25072099413659615 valid 0.2162310403585434
LOSS train 0.25072099413659615 valid 0.21538575624044126
LOSS train 0.25072099413659615 valid 0.21586932131537684
LOSS train 0.25072099413659615 valid 0.21615903505257197
LOSS train 0.25072099413659615 valid 0.21477506849272499
LOSS train 0.25072099413659615 valid 0.21336419532696407
LOSS train 0.25072099413659615 valid 0.2130481182567535
LOSS train 0.25072099413659615 valid 0.21388390148058534
LOSS train 0.25072099413659615 valid 0.21340440665230606
LOSS train 0.25072099413659615 valid 0.21276495211264668
LOSS train 0.25072099413659615 valid 0.2135689573628562
LOSS train 0.25072099413659615 valid 0.2144835910035504
LOSS train 0.25072099413659615 valid 0.21348102431039553
LOSS train 0.25072099413659615 valid 0.21333807314697065
LOSS train 0.25072099413659615 valid 0.2128409131979331
LOSS train 0.25072099413659615 valid 0.21338978186249732
LOSS train 0.25072099413659615 valid 0.21278786586552131
LOSS train 0.25072099413659615 valid 0.21486873498984746
LOSS train 0.25072099413659615 valid 0.2151579700930174
LOSS train 0.25072099413659615 valid 0.21449259668588638
LOSS train 0.25072099413659615 valid 0.21435148980882432
LOSS train 0.25072099413659615 valid 0.2138524207731952
LOSS train 0.25072099413659615 valid 0.21352391991209477
LOSS train 0.25072099413659615 valid 0.2152564606318871
LOSS train 0.25072099413659615 valid 0.21484343099350833
LOSS train 0.25072099413659615 valid 0.2155851259827614
LOSS train 0.25072099413659615 valid 0.21526862476386277
LOSS train 0.25072099413659615 valid 0.21525341702195314
LOSS train 0.25072099413659615 valid 0.216347540043435
LOSS train 0.25072099413659615 valid 0.21635086724051722
LOSS train 0.25072099413659615 valid 0.21626157002015547
LOSS train 0.25072099413659615 valid 0.21624483726918697
LOSS train 0.25072099413659615 valid 0.21553368134456768
LOSS train 0.25072099413659615 valid 0.21635602029233142
LOSS train 0.25072099413659615 valid 0.21619680296566526
LOSS train 0.25072099413659615 valid 0.21619001179933547
LOSS train 0.25072099413659615 valid 0.21652450131588294
LOSS train 0.25072099413659615 valid 0.216265412347932
LOSS train 0.25072099413659615 valid 0.21626140579344735
LOSS train 0.25072099413659615 valid 0.2163881475571543
LOSS train 0.25072099413659615 valid 0.21511108462627118
LOSS train 0.25072099413659615 valid 0.21495016370758865
LOSS train 0.25072099413659615 valid 0.21575338849380835
LOSS train 0.25072099413659615 valid 0.21509668318664327
LOSS train 0.25072099413659615 valid 0.21551763082759967
LOSS train 0.25072099413659615 valid 0.21606826164892742
LOSS train 0.25072099413659615 valid 0.21650924250273637
LOSS train 0.25072099413659615 valid 0.21653030667867926
LOSS train 0.25072099413659615 valid 0.21712674731261108
LOSS train 0.25072099413659615 valid 0.21723748824080905
LOSS train 0.25072099413659615 valid 0.21712304095427196
LOSS train 0.25072099413659615 valid 0.21724952992640043
LOSS train 0.25072099413659615 valid 0.21752141396720687
LOSS train 0.25072099413659615 valid 0.21730534273844498
LOSS train 0.25072099413659615 valid 0.21739497837386554
LOSS train 0.25072099413659615 valid 0.21700208764523268
LOSS train 0.25072099413659615 valid 0.21702966664308382
LOSS train 0.25072099413659615 valid 0.21665823296075915
LOSS train 0.25072099413659615 valid 0.2163688781031643
LOSS train 0.25072099413659615 valid 0.2162041092912356
LOSS train 0.25072099413659615 valid 0.21646166776909548
LOSS train 0.25072099413659615 valid 0.21621696443058724
LOSS train 0.25072099413659615 valid 0.21572000370628533
LOSS train 0.25072099413659615 valid 0.21567829681391065
LOSS train 0.25072099413659615 valid 0.21575470051068937
LOSS train 0.25072099413659615 valid 0.21588899377319548
LOSS train 0.25072099413659615 valid 0.2159677336176673
LOSS train 0.25072099413659615 valid 0.21567404253975206
LOSS train 0.25072099413659615 valid 0.21571403052858126
LOSS train 0.25072099413659615 valid 0.2159799487983927
LOSS train 0.25072099413659615 valid 0.21596544808463047
LOSS train 0.25072099413659615 valid 0.2164617977105081
LOSS train 0.25072099413659615 valid 0.2164933555519458
LOSS train 0.25072099413659615 valid 0.2167065678810587
LOSS train 0.25072099413659615 valid 0.216822524413918
LOSS train 0.25072099413659615 valid 0.2167268967628479
LOSS train 0.25072099413659615 valid 0.21701120607333607
LOSS train 0.25072099413659615 valid 0.2175462638922766
LOSS train 0.25072099413659615 valid 0.21726786540549936
LOSS train 0.25072099413659615 valid 0.2170496782144675
LOSS train 0.25072099413659615 valid 0.21737146931035178
LOSS train 0.25072099413659615 valid 0.2174462877073378
LOSS train 0.25072099413659615 valid 0.2172065866049205
LOSS train 0.25072099413659615 valid 0.21718618425506134
LOSS train 0.25072099413659615 valid 0.2165292021604853
LOSS train 0.25072099413659615 valid 0.21628286770798943
LOSS train 0.25072099413659615 valid 0.21641499475315884
LOSS train 0.25072099413659615 valid 0.21642215297158277
LOSS train 0.25072099413659615 valid 0.2161913258312023
LOSS train 0.25072099413659615 valid 0.21605474645631356
LOSS train 0.25072099413659615 valid 0.21655945492827375
LOSS train 0.25072099413659615 valid 0.21627705672691608
LOSS train 0.25072099413659615 valid 0.21670939957993662
LOSS train 0.25072099413659615 valid 0.21695957257080886
LOSS train 0.25072099413659615 valid 0.2166320277111871
LOSS train 0.25072099413659615 valid 0.2164087572445472
LOSS train 0.25072099413659615 valid 0.21640236589534223
LOSS train 0.25072099413659615 valid 0.21659465866987823
LOSS train 0.25072099413659615 valid 0.21662802303709636
LOSS train 0.25072099413659615 valid 0.2166112818785252
LOSS train 0.25072099413659615 valid 0.21664687836170196
LOSS train 0.25072099413659615 valid 0.21688379690287604
LOSS train 0.25072099413659615 valid 0.21680305651792392
LOSS train 0.25072099413659615 valid 0.21672050922643393
LOSS train 0.25072099413659615 valid 0.21630558891351834
LOSS train 0.25072099413659615 valid 0.21596178630223642
LOSS train 0.25072099413659615 valid 0.21600392563197449
LOSS train 0.25072099413659615 valid 0.21593634242361243
LOSS train 0.25072099413659615 valid 0.21580397220034347
LOSS train 0.25072099413659615 valid 0.21595316958516392
LOSS train 0.25072099413659615 valid 0.2161355491037722
LOSS train 0.25072099413659615 valid 0.21638117095126824
LOSS train 0.25072099413659615 valid 0.21646908099633932
LOSS train 0.25072099413659615 valid 0.21641987648563107
LOSS train 0.25072099413659615 valid 0.21627832595392954
LOSS train 0.25072099413659615 valid 0.21661875822714396
LOSS train 0.25072099413659615 valid 0.21680946806643872
LOSS train 0.25072099413659615 valid 0.2170731568000686
LOSS train 0.25072099413659615 valid 0.2171572080025306
LOSS train 0.25072099413659615 valid 0.21727786471860278
LOSS train 0.25072099413659615 valid 0.21710157343025865
LOSS train 0.25072099413659615 valid 0.21699961754557204
LOSS train 0.25072099413659615 valid 0.21700006733540775
LOSS train 0.25072099413659615 valid 0.21670326439512744
LOSS train 0.25072099413659615 valid 0.2168261454809432
LOSS train 0.25072099413659615 valid 0.21677588760852814
LOSS train 0.25072099413659615 valid 0.21654027019510205
LOSS train 0.25072099413659615 valid 0.21655461966599288
LOSS train 0.25072099413659615 valid 0.21633993089199066
LOSS train 0.25072099413659615 valid 0.2164194549058939
LOSS train 0.25072099413659615 valid 0.2163909863079748
LOSS train 0.25072099413659615 valid 0.2167030050395391
LOSS train 0.25072099413659615 valid 0.21663312129913623
LOSS train 0.25072099413659615 valid 0.21653081570999533
LOSS train 0.25072099413659615 valid 0.21673563394531514
LOSS train 0.25072099413659615 valid 0.21677223946899177
LOSS train 0.25072099413659615 valid 0.2169007218402365
LOSS train 0.25072099413659615 valid 0.2169817649287942
LOSS train 0.25072099413659615 valid 0.21666565912267183
LOSS train 0.25072099413659615 valid 0.2167428976333723
LOSS train 0.25072099413659615 valid 0.216897929798473
LOSS train 0.25072099413659615 valid 0.2168188624712358
LOSS train 0.25072099413659615 valid 0.2169172888744377
LOSS train 0.25072099413659615 valid 0.216635645677646
LOSS train 0.25072099413659615 valid 0.21644181401066526
LOSS train 0.25072099413659615 valid 0.21650772462872897
LOSS train 0.25072099413659615 valid 0.21634822229892886
LOSS train 0.25072099413659615 valid 0.21622431555459665
LOSS train 0.25072099413659615 valid 0.21620407356003116
LOSS train 0.25072099413659615 valid 0.2162409075375261
LOSS train 0.25072099413659615 valid 0.2161244909252439
LOSS train 0.25072099413659615 valid 0.21605819565328685
LOSS train 0.25072099413659615 valid 0.21612043182055155
LOSS train 0.25072099413659615 valid 0.21616484124339028
LOSS train 0.25072099413659615 valid 0.21630085089020223
LOSS train 0.25072099413659615 valid 0.21637295501099693
LOSS train 0.25072099413659615 valid 0.21622335186320774
LOSS train 0.25072099413659615 valid 0.2161762516249667
LOSS train 0.25072099413659615 valid 0.21619794073977758
LOSS train 0.25072099413659615 valid 0.2161011756600245
LOSS train 0.25072099413659615 valid 0.21609242542369947
LOSS train 0.25072099413659615 valid 0.21614129745191143
LOSS train 0.25072099413659615 valid 0.21593334059664274
LOSS train 0.25072099413659615 valid 0.2159598225608785
LOSS train 0.25072099413659615 valid 0.21593477494186825
LOSS train 0.25072099413659615 valid 0.21592787261071958
LOSS train 0.25072099413659615 valid 0.21617704481666625
LOSS train 0.25072099413659615 valid 0.21622507545786598
LOSS train 0.25072099413659615 valid 0.2160253874532917
LOSS train 0.25072099413659615 valid 0.2160787067001628
LOSS train 0.25072099413659615 valid 0.21599695323369442
LOSS train 0.25072099413659615 valid 0.21629075333476067
LOSS train 0.25072099413659615 valid 0.21619560679203362
LOSS train 0.25072099413659615 valid 0.21650258865621355
LOSS train 0.25072099413659615 valid 0.21646524157056857
LOSS train 0.25072099413659615 valid 0.21642905570566653
LOSS train 0.25072099413659615 valid 0.21645689566633594
LOSS train 0.25072099413659615 valid 0.21661258215951448
LOSS train 0.25072099413659615 valid 0.21675466067098045
LOSS train 0.25072099413659615 valid 0.21665987372398376
LOSS train 0.25072099413659615 valid 0.21657352796415003
LOSS train 0.25072099413659615 valid 0.2165564644394569
LOSS train 0.25072099413659615 valid 0.2165589862399631
LOSS train 0.25072099413659615 valid 0.2163763686011617
LOSS train 0.25072099413659615 valid 0.2163859918784867
LOSS train 0.25072099413659615 valid 0.21643547940821875
LOSS train 0.25072099413659615 valid 0.21665716849232172
LOSS train 0.25072099413659615 valid 0.2165245589501453
LOSS train 0.25072099413659615 valid 0.21667233004536426
LOSS train 0.25072099413659615 valid 0.21657748179179487
LOSS train 0.25072099413659615 valid 0.21623658253703007
LOSS train 0.25072099413659615 valid 0.21601206760991504
LOSS train 0.25072099413659615 valid 0.21575216358433122
LOSS train 0.25072099413659615 valid 0.215716984909062
LOSS train 0.25072099413659615 valid 0.21567217726685686
LOSS train 0.25072099413659615 valid 0.21561885753815824
LOSS train 0.25072099413659615 valid 0.21551880759621098
LOSS train 0.25072099413659615 valid 0.2156868329322016
LOSS train 0.25072099413659615 valid 0.21561351418495178
LOSS train 0.25072099413659615 valid 0.21551428415945598
LOSS train 0.25072099413659615 valid 0.2153839323255751
LOSS train 0.25072099413659615 valid 0.2152262903024665
LOSS train 0.25072099413659615 valid 0.21517007164493007
LOSS train 0.25072099413659615 valid 0.2150580447755362
LOSS train 0.25072099413659615 valid 0.21489205689669696
LOSS train 0.25072099413659615 valid 0.21488222408553828
LOSS train 0.25072099413659615 valid 0.21491291570715057
LOSS train 0.25072099413659615 valid 0.2149629794575017
LOSS train 0.25072099413659615 valid 0.21516379624477272
LOSS train 0.25072099413659615 valid 0.21500858798241004
LOSS train 0.25072099413659615 valid 0.215004455916425
LOSS train 0.25072099413659615 valid 0.21482634998984257
LOSS train 0.25072099413659615 valid 0.21487499880388317
LOSS train 0.25072099413659615 valid 0.21499748214954087
LOSS train 0.25072099413659615 valid 0.2149014272824491
LOSS train 0.25072099413659615 valid 0.21456759528567393
LOSS train 0.25072099413659615 valid 0.21442441425877487
LOSS train 0.25072099413659615 valid 0.21450132008426445
LOSS train 0.25072099413659615 valid 0.21442481654661674
LOSS train 0.25072099413659615 valid 0.21437558452369737
LOSS train 0.25072099413659615 valid 0.21441275957895786
LOSS train 0.25072099413659615 valid 0.21444913847901956
LOSS train 0.25072099413659615 valid 0.21448695955247532
LOSS train 0.25072099413659615 valid 0.21459762469655083
LOSS train 0.25072099413659615 valid 0.2144180080857143
LOSS train 0.25072099413659615 valid 0.21466559332609175
LOSS train 0.25072099413659615 valid 0.21491546845768553
LOSS train 0.25072099413659615 valid 0.21497750755340334
LOSS train 0.25072099413659615 valid 0.2148873513866319
LOSS train 0.25072099413659615 valid 0.21500661130261234
LOSS train 0.25072099413659615 valid 0.21494299407098807
LOSS train 0.25072099413659615 valid 0.21487946453271434
LOSS train 0.25072099413659615 valid 0.21486766491418682
LOSS train 0.25072099413659615 valid 0.21486939212610556
LOSS train 0.25072099413659615 valid 0.21477188244749681
LOSS train 0.25072099413659615 valid 0.21465091997614275
LOSS train 0.25072099413659615 valid 0.2145921482893699
LOSS train 0.25072099413659615 valid 0.21479078751938943
LOSS train 0.25072099413659615 valid 0.2146782395730907
LOSS train 0.25072099413659615 valid 0.21471399311540704
LOSS train 0.25072099413659615 valid 0.214613428486968
LOSS train 0.25072099413659615 valid 0.21459866830504926
LOSS train 0.25072099413659615 valid 0.21452385505263724
LOSS train 0.25072099413659615 valid 0.21463637954708356
LOSS train 0.25072099413659615 valid 0.21476161851094114
LOSS train 0.25072099413659615 valid 0.21466041610196784
LOSS train 0.25072099413659615 valid 0.21458914628547937
LOSS train 0.25072099413659615 valid 0.21446708551443674
LOSS train 0.25072099413659615 valid 0.21439845838171223
LOSS train 0.25072099413659615 valid 0.21434280085955223
LOSS train 0.25072099413659615 valid 0.21448180821808902
LOSS train 0.25072099413659615 valid 0.21444350653800412
LOSS train 0.25072099413659615 valid 0.2143931553467086
LOSS train 0.25072099413659615 valid 0.21448812510469834
LOSS train 0.25072099413659615 valid 0.2145259128249247
LOSS train 0.25072099413659615 valid 0.21435526880834785
LOSS train 0.25072099413659615 valid 0.21425491253252132
LOSS train 0.25072099413659615 valid 0.21417561617303402
LOSS train 0.25072099413659615 valid 0.21426092614765302
LOSS train 0.25072099413659615 valid 0.21409059022094162
LOSS train 0.25072099413659615 valid 0.21415804248107106
LOSS train 0.25072099413659615 valid 0.21418410018607453
LOSS train 0.25072099413659615 valid 0.21422457165419018
LOSS train 0.25072099413659615 valid 0.2141843390547567
LOSS train 0.25072099413659615 valid 0.21420570450051846
LOSS train 0.25072099413659615 valid 0.21411399938936893
LOSS train 0.25072099413659615 valid 0.21408138677631458
LOSS train 0.25072099413659615 valid 0.21405794435780343
LOSS train 0.25072099413659615 valid 0.21404495850561422
LOSS train 0.25072099413659615 valid 0.21393868808640915
LOSS train 0.25072099413659615 valid 0.2139872272640972
LOSS train 0.25072099413659615 valid 0.2139712645596749
LOSS train 0.25072099413659615 valid 0.21401142823174346
LOSS train 0.25072099413659615 valid 0.21406620280854655
LOSS train 0.25072099413659615 valid 0.2140069630831779
LOSS train 0.25072099413659615 valid 0.21397820134957632
LOSS train 0.25072099413659615 valid 0.21388679188351298
LOSS train 0.25072099413659615 valid 0.21393640462728525
LOSS train 0.25072099413659615 valid 0.2138724835103888
LOSS train 0.25072099413659615 valid 0.2138971542253306
LOSS train 0.25072099413659615 valid 0.21382044460929808
LOSS train 0.25072099413659615 valid 0.21406388336342144
LOSS train 0.25072099413659615 valid 0.21399799149471307
LOSS train 0.25072099413659615 valid 0.21416354397094095
LOSS train 0.25072099413659615 valid 0.21420700385153874
LOSS train 0.25072099413659615 valid 0.21423654945627335
LOSS train 0.25072099413659615 valid 0.21427895876180705
LOSS train 0.25072099413659615 valid 0.21427225546003917
LOSS train 0.25072099413659615 valid 0.21443454559428243
LOSS train 0.25072099413659615 valid 0.21435209544031483
LOSS train 0.25072099413659615 valid 0.2143665959437688
LOSS train 0.25072099413659615 valid 0.21442378920656216
LOSS train 0.25072099413659615 valid 0.21439907236806224
LOSS train 0.25072099413659615 valid 0.21441485906172097
LOSS train 0.25072099413659615 valid 0.21441377512526735
LOSS train 0.25072099413659615 valid 0.21429867283441126
LOSS train 0.25072099413659615 valid 0.2143148933522798
LOSS train 0.25072099413659615 valid 0.2142941829117929
LOSS train 0.25072099413659615 valid 0.2141376266446276
LOSS train 0.25072099413659615 valid 0.21413208990369315
LOSS train 0.25072099413659615 valid 0.21394638130298027
LOSS train 0.25072099413659615 valid 0.21406186134354468
LOSS train 0.25072099413659615 valid 0.21407691824509098
LOSS train 0.25072099413659615 valid 0.21415456684260833
LOSS train 0.25072099413659615 valid 0.2141110144759384
LOSS train 0.25072099413659615 valid 0.21413138544920718
LOSS train 0.25072099413659615 valid 0.21408902347267936
LOSS train 0.25072099413659615 valid 0.21399837220648685
LOSS train 0.25072099413659615 valid 0.21409145820964207
LOSS train 0.25072099413659615 valid 0.21405035351029414
LOSS train 0.25072099413659615 valid 0.214023791676137
LOSS train 0.25072099413659615 valid 0.21398647355714015
LOSS train 0.25072099413659615 valid 0.2141290831017565
LOSS train 0.25072099413659615 valid 0.21402986215416497
LOSS train 0.25072099413659615 valid 0.21399274688203074
LOSS train 0.25072099413659615 valid 0.21408883723265984
LOSS train 0.25072099413659615 valid 0.21407927419782732
LOSS train 0.25072099413659615 valid 0.21401692408393
LOSS train 0.25072099413659615 valid 0.21395705831467932
LOSS train 0.25072099413659615 valid 0.21398284823395486
LOSS train 0.25072099413659615 valid 0.21404708701631298
LOSS train 0.25072099413659615 valid 0.21396877823365218
LOSS train 0.25072099413659615 valid 0.2139167682581745
LOSS train 0.25072099413659615 valid 0.21395932771693701
LOSS train 0.25072099413659615 valid 0.21390559637444068
LOSS train 0.25072099413659615 valid 0.21394348783152445
LOSS train 0.25072099413659615 valid 0.21403701013309664
LOSS train 0.25072099413659615 valid 0.21413278025151652
LOSS train 0.25072099413659615 valid 0.21424545609410695
LOSS train 0.25072099413659615 valid 0.21419813934355805
LOSS train 0.25072099413659615 valid 0.2141122066219088
LOSS train 0.25072099413659615 valid 0.21409640599335178
LOSS train 0.25072099413659615 valid 0.21416327912433475
LOSS train 0.25072099413659615 valid 0.21416040435017153
LOSS train 0.25072099413659615 valid 0.2141192175634724
LOSS train 0.25072099413659615 valid 0.21425941209826205
LOSS train 0.25072099413659615 valid 0.21421886320094322
LOSS train 0.25072099413659615 valid 0.21433910295284914
LOSS train 0.25072099413659615 valid 0.2143984351194266
LOSS train 0.25072099413659615 valid 0.2142874163749454
LOSS train 0.25072099413659615 valid 0.21440296205755782
LOSS train 0.25072099413659615 valid 0.21443534499960518
LOSS train 0.25072099413659615 valid 0.2144473521521046
LOSS train 0.25072099413659615 valid 0.21435218486610963
LOSS train 0.25072099413659615 valid 0.21442334382191583
EPOCH 13:
  batch 1 loss: 0.2595877945423126
  batch 2 loss: 0.2733857035636902
  batch 3 loss: 0.2585923671722412
  batch 4 loss: 0.2650401070713997
  batch 5 loss: 0.2742584764957428
  batch 6 loss: 0.26577415068944293
  batch 7 loss: 0.2705283377851759
  batch 8 loss: 0.26846445351839066
  batch 9 loss: 0.2693148652712504
  batch 10 loss: 0.26518040895462036
  batch 11 loss: 0.26512079618193884
  batch 12 loss: 0.26356108362476033
  batch 13 loss: 0.26322295459417194
  batch 14 loss: 0.2633678476725306
  batch 15 loss: 0.262771533926328
  batch 16 loss: 0.26212041173130274
  batch 17 loss: 0.2630564158453661
  batch 18 loss: 0.2627413777841462
  batch 19 loss: 0.2616961551340003
  batch 20 loss: 0.25856973230838776
  batch 21 loss: 0.261167205515362
  batch 22 loss: 0.26157942143353546
  batch 23 loss: 0.2592462754767874
  batch 24 loss: 0.25943609947959584
  batch 25 loss: 0.2594724977016449
  batch 26 loss: 0.2602063383047397
  batch 27 loss: 0.259437033423671
  batch 28 loss: 0.2598871663212776
  batch 29 loss: 0.2592220136831547
  batch 30 loss: 0.25966667185227077
  batch 31 loss: 0.2598998378361425
  batch 32 loss: 0.26086045848205686
  batch 33 loss: 0.26244753644321905
  batch 34 loss: 0.2627146835712826
  batch 35 loss: 0.26382603687899453
  batch 36 loss: 0.263986561447382
  batch 37 loss: 0.26379846278074626
  batch 38 loss: 0.265092487005811
  batch 39 loss: 0.2657274256149928
  batch 40 loss: 0.26532679311931134
  batch 41 loss: 0.26590231678834775
  batch 42 loss: 0.2660383550184114
  batch 43 loss: 0.2659448437219442
  batch 44 loss: 0.2657150101255287
  batch 45 loss: 0.265456203950776
  batch 46 loss: 0.264252261299154
  batch 47 loss: 0.2646040786454018
  batch 48 loss: 0.26383573561906815
  batch 49 loss: 0.26393574962810595
  batch 50 loss: 0.26344105005264284
  batch 51 loss: 0.2633424944737378
  batch 52 loss: 0.2623434407779804
  batch 53 loss: 0.26124253784710505
  batch 54 loss: 0.2609450789513411
  batch 55 loss: 0.2603926864537326
  batch 56 loss: 0.2601095533796719
  batch 57 loss: 0.2590783222725517
  batch 58 loss: 0.2580511408119366
  batch 59 loss: 0.2585114238120742
  batch 60 loss: 0.2589334872861703
  batch 61 loss: 0.2594238629595178
  batch 62 loss: 0.25980643663675557
  batch 63 loss: 0.2592906774509521
  batch 64 loss: 0.2592162780929357
  batch 65 loss: 0.25879172132565426
  batch 66 loss: 0.2585726786743511
  batch 67 loss: 0.2582181714363952
  batch 68 loss: 0.25807620234349193
  batch 69 loss: 0.25822798842969147
  batch 70 loss: 0.2575406312942505
  batch 71 loss: 0.2573654437568826
  batch 72 loss: 0.257368524869283
  batch 73 loss: 0.2569991056233236
  batch 74 loss: 0.25669300354815816
  batch 75 loss: 0.2564919630686442
  batch 76 loss: 0.25638990417907115
  batch 77 loss: 0.2561745736506078
  batch 78 loss: 0.2560447087654701
  batch 79 loss: 0.2556972654559944
  batch 80 loss: 0.25508160293102267
  batch 81 loss: 0.2545216405465279
  batch 82 loss: 0.2546706248710795
  batch 83 loss: 0.2544387862624892
  batch 84 loss: 0.2538795054313682
  batch 85 loss: 0.25398818236940046
  batch 86 loss: 0.2541982349614764
  batch 87 loss: 0.25378795362066947
  batch 88 loss: 0.25380706787109375
  batch 89 loss: 0.2534846735469411
  batch 90 loss: 0.2540506096349822
  batch 91 loss: 0.25366978920423067
  batch 92 loss: 0.25358748484564864
  batch 93 loss: 0.2534970493726833
  batch 94 loss: 0.2537589628011622
  batch 95 loss: 0.25405052774830866
  batch 96 loss: 0.25410820233325165
  batch 97 loss: 0.25448783709830847
  batch 98 loss: 0.2543606910170341
  batch 99 loss: 0.2543102945342208
  batch 100 loss: 0.25439558178186417
  batch 101 loss: 0.2544021202195989
  batch 102 loss: 0.2544960455567229
  batch 103 loss: 0.2543928491548427
  batch 104 loss: 0.2547269244320117
  batch 105 loss: 0.2546654373407364
  batch 106 loss: 0.2546960819723471
  batch 107 loss: 0.25453209445298275
  batch 108 loss: 0.2541619540640601
  batch 109 loss: 0.2543578089103786
  batch 110 loss: 0.2541308445009318
  batch 111 loss: 0.2540357444468919
  batch 112 loss: 0.2540504766095962
  batch 113 loss: 0.2535180769926679
  batch 114 loss: 0.2533415697122875
  batch 115 loss: 0.25273820122946866
  batch 116 loss: 0.2524750704138443
  batch 117 loss: 0.25215128942942006
  batch 118 loss: 0.2521452389783778
  batch 119 loss: 0.2522344525371279
  batch 120 loss: 0.25173227277894816
  batch 121 loss: 0.2516715202696067
  batch 122 loss: 0.25147565100036684
  batch 123 loss: 0.25133988140075186
  batch 124 loss: 0.2510005302967564
  batch 125 loss: 0.25066651606559753
  batch 126 loss: 0.25078446855620734
  batch 127 loss: 0.25054930385172836
  batch 128 loss: 0.250380028039217
  batch 129 loss: 0.250523143036421
  batch 130 loss: 0.25035731425652136
  batch 131 loss: 0.25048016409837565
  batch 132 loss: 0.2504166193080671
  batch 133 loss: 0.2501550787373593
  batch 134 loss: 0.2499295490891186
  batch 135 loss: 0.2497570655964039
  batch 136 loss: 0.24986571643282385
  batch 137 loss: 0.25004322990013733
  batch 138 loss: 0.2502052144727845
  batch 139 loss: 0.25024333617670075
  batch 140 loss: 0.25042351271424973
  batch 141 loss: 0.25011924131119506
  batch 142 loss: 0.2500033287305227
  batch 143 loss: 0.25021681741817847
  batch 144 loss: 0.2501360672629542
  batch 145 loss: 0.2501141369342804
  batch 146 loss: 0.2501549192079126
  batch 147 loss: 0.2501190454781461
  batch 148 loss: 0.25012985596785675
  batch 149 loss: 0.24968574741142707
  batch 150 loss: 0.2495004587372144
  batch 151 loss: 0.24907464233060547
  batch 152 loss: 0.24888849209405875
  batch 153 loss: 0.24908552401595646
  batch 154 loss: 0.24874560118882688
  batch 155 loss: 0.24875491655641987
  batch 156 loss: 0.24866244024955309
  batch 157 loss: 0.24859121612682464
  batch 158 loss: 0.2486777460273308
  batch 159 loss: 0.2488082874121156
  batch 160 loss: 0.2487024139612913
  batch 161 loss: 0.24843047457452147
  batch 162 loss: 0.24824806495949073
  batch 163 loss: 0.2480955792168167
  batch 164 loss: 0.24833056158045444
  batch 165 loss: 0.24850998692440265
  batch 166 loss: 0.24855675404689398
  batch 167 loss: 0.2484871692464737
  batch 168 loss: 0.2482564122016941
  batch 169 loss: 0.24835692804593307
  batch 170 loss: 0.24837734462583766
  batch 171 loss: 0.2484507230464478
  batch 172 loss: 0.24843546886776768
  batch 173 loss: 0.24854188623455908
  batch 174 loss: 0.24857997380453964
  batch 175 loss: 0.2484189805814198
  batch 176 loss: 0.2486166118390181
  batch 177 loss: 0.24882914327968986
  batch 178 loss: 0.24851393322931248
  batch 179 loss: 0.24880994140435864
  batch 180 loss: 0.24872500664658018
  batch 181 loss: 0.24856292649856587
  batch 182 loss: 0.24845250491257553
  batch 183 loss: 0.24815547922269895
  batch 184 loss: 0.24791648983955383
  batch 185 loss: 0.24769088239283174
  batch 186 loss: 0.24763458294253196
  batch 187 loss: 0.24785612786517425
  batch 188 loss: 0.24781287041433314
  batch 189 loss: 0.24785633795160464
  batch 190 loss: 0.247887611310733
  batch 191 loss: 0.2480893332452674
  batch 192 loss: 0.24788657125706473
  batch 193 loss: 0.24799984322928395
  batch 194 loss: 0.24800994746463814
  batch 195 loss: 0.24814626635649265
  batch 196 loss: 0.24803365295638843
  batch 197 loss: 0.24813122888506972
  batch 198 loss: 0.24816619612351812
  batch 199 loss: 0.2482244424784004
  batch 200 loss: 0.24817619524896145
  batch 201 loss: 0.24799700029453828
  batch 202 loss: 0.24787880258985084
  batch 203 loss: 0.24780927106664685
  batch 204 loss: 0.24766257558675372
  batch 205 loss: 0.2474539263946254
  batch 206 loss: 0.2474641104056997
  batch 207 loss: 0.24732298882687148
  batch 208 loss: 0.24717823673899358
  batch 209 loss: 0.24705164840346888
  batch 210 loss: 0.24709525207678476
  batch 211 loss: 0.24702337151172601
  batch 212 loss: 0.24704670336730075
  batch 213 loss: 0.24694807855456088
  batch 214 loss: 0.2468815559399462
  batch 215 loss: 0.24678970277309417
  batch 216 loss: 0.2468681481011488
  batch 217 loss: 0.246711865067482
  batch 218 loss: 0.24655799656559568
  batch 219 loss: 0.24631511423413613
  batch 220 loss: 0.24654861302538353
  batch 221 loss: 0.24639611789004295
  batch 222 loss: 0.2462809422800133
  batch 223 loss: 0.24629984936372046
  batch 224 loss: 0.2463965354753392
  batch 225 loss: 0.24645419677098593
  batch 226 loss: 0.246579324117804
  batch 227 loss: 0.24648715555667877
  batch 228 loss: 0.24665540293381924
  batch 229 loss: 0.24684062476501714
  batch 230 loss: 0.24696018106263617
  batch 231 loss: 0.24701987400457456
  batch 232 loss: 0.24694842549747434
  batch 233 loss: 0.24705220944380044
  batch 234 loss: 0.24705478421643248
  batch 235 loss: 0.24689693431904974
  batch 236 loss: 0.24689931365645537
  batch 237 loss: 0.24701079216938984
  batch 238 loss: 0.24698892692808344
  batch 239 loss: 0.24705804497876427
  batch 240 loss: 0.24706036976228157
  batch 241 loss: 0.24712580275486118
  batch 242 loss: 0.2471159156565824
  batch 243 loss: 0.24706726542716165
  batch 244 loss: 0.2471022967432366
  batch 245 loss: 0.24726079653720467
  batch 246 loss: 0.24729571764062092
  batch 247 loss: 0.2472746944137913
  batch 248 loss: 0.24753406631850428
  batch 249 loss: 0.24778101207262063
  batch 250 loss: 0.2477158306837082
  batch 251 loss: 0.2476462448735636
  batch 252 loss: 0.24772851074498797
  batch 253 loss: 0.24791562015360052
  batch 254 loss: 0.2480081096878202
  batch 255 loss: 0.24786719113003974
  batch 256 loss: 0.24801186117110774
  batch 257 loss: 0.24814468245330024
  batch 258 loss: 0.24808293113181756
  batch 259 loss: 0.247966076473932
  batch 260 loss: 0.24797969901790987
  batch 261 loss: 0.24836365935674573
  batch 262 loss: 0.24840759633834125
  batch 263 loss: 0.24838701757414713
  batch 264 loss: 0.24860923325247836
  batch 265 loss: 0.24873244273212722
  batch 266 loss: 0.24881903418248758
  batch 267 loss: 0.2488315665208445
  batch 268 loss: 0.24878349744561892
  batch 269 loss: 0.24882637834017163
  batch 270 loss: 0.2488308518021195
  batch 271 loss: 0.24898709131342897
  batch 272 loss: 0.24896927780526526
  batch 273 loss: 0.24888986365480736
  batch 274 loss: 0.24879967519184099
  batch 275 loss: 0.24881102762439034
  batch 276 loss: 0.2488115646916887
  batch 277 loss: 0.24882288400877253
  batch 278 loss: 0.24875600378719165
  batch 279 loss: 0.24877857328743064
  batch 280 loss: 0.2486064471836601
  batch 281 loss: 0.24859353677233767
  batch 282 loss: 0.24854567826639676
  batch 283 loss: 0.24849745315292276
  batch 284 loss: 0.24846214979467257
  batch 285 loss: 0.24838548401991525
  batch 286 loss: 0.24839183026468836
  batch 287 loss: 0.24831571261226507
  batch 288 loss: 0.2483410635549161
  batch 289 loss: 0.2482637516045653
  batch 290 loss: 0.24813527315855027
  batch 291 loss: 0.24808133595792697
  batch 292 loss: 0.24800728434977465
  batch 293 loss: 0.24795251037073623
  batch 294 loss: 0.24783228089412054
  batch 295 loss: 0.24774105553909884
  batch 296 loss: 0.24777729049123623
  batch 297 loss: 0.24775304512343424
  batch 298 loss: 0.24768980367471707
  batch 299 loss: 0.24772341315164215
  batch 300 loss: 0.24771440515915552
  batch 301 loss: 0.24773338437080383
  batch 302 loss: 0.24782856104784454
  batch 303 loss: 0.2477714864432615
  batch 304 loss: 0.247798657780023
  batch 305 loss: 0.24778037740558875
  batch 306 loss: 0.24791608239505805
  batch 307 loss: 0.2479699250638291
  batch 308 loss: 0.24804507800704473
  batch 309 loss: 0.24796932627082258
  batch 310 loss: 0.24810475841645271
  batch 311 loss: 0.24803260634758081
  batch 312 loss: 0.24805224022995204
  batch 313 loss: 0.24803079450473237
  batch 314 loss: 0.2480218167043036
  batch 315 loss: 0.24801011255809238
  batch 316 loss: 0.24796552838215344
  batch 317 loss: 0.24803430363770915
  batch 318 loss: 0.24794422024450963
  batch 319 loss: 0.247874945682418
  batch 320 loss: 0.24798342306166887
  batch 321 loss: 0.24794841304746373
  batch 322 loss: 0.24778782275522718
  batch 323 loss: 0.2477467502971921
  batch 324 loss: 0.2477516026960479
  batch 325 loss: 0.24771639911028054
  batch 326 loss: 0.2477961456391709
  batch 327 loss: 0.24775366116007533
  batch 328 loss: 0.24774348663120735
  batch 329 loss: 0.24782024989736842
  batch 330 loss: 0.2478242925170696
  batch 331 loss: 0.24781188427321502
  batch 332 loss: 0.2479484068252236
  batch 333 loss: 0.24792769824241376
  batch 334 loss: 0.2480729239697228
  batch 335 loss: 0.2480610026797252
  batch 336 loss: 0.24807854506763674
  batch 337 loss: 0.24804924861790517
  batch 338 loss: 0.2480024434405671
  batch 339 loss: 0.24794657976753945
  batch 340 loss: 0.24801196013303364
  batch 341 loss: 0.24802838047403744
  batch 342 loss: 0.24802820423715993
  batch 343 loss: 0.24810079343986233
  batch 344 loss: 0.24807018279856027
  batch 345 loss: 0.24806021348289822
  batch 346 loss: 0.24803462882951505
  batch 347 loss: 0.2478916117282697
  batch 348 loss: 0.24783997344730915
  batch 349 loss: 0.24788992397252332
  batch 350 loss: 0.24792131862470082
  batch 351 loss: 0.24789742335804507
  batch 352 loss: 0.24784766721793197
  batch 353 loss: 0.2478379669675746
  batch 354 loss: 0.24797009773510323
  batch 355 loss: 0.24789425869223097
  batch 356 loss: 0.24791914245553232
  batch 357 loss: 0.24779584377753636
  batch 358 loss: 0.2476767425597047
  batch 359 loss: 0.24766062909846187
  batch 360 loss: 0.24755494511789747
  batch 361 loss: 0.24750010570165523
  batch 362 loss: 0.2474100962281227
  batch 363 loss: 0.24744831797341013
  batch 364 loss: 0.24734648480847643
  batch 365 loss: 0.24736243976305608
  batch 366 loss: 0.24733264113221665
  batch 367 loss: 0.24736239405841204
  batch 368 loss: 0.24719824961832035
  batch 369 loss: 0.2471808303743197
  batch 370 loss: 0.24727696635433147
  batch 371 loss: 0.24718157055403667
  batch 372 loss: 0.24718047494208942
  batch 373 loss: 0.24724184715715875
  batch 374 loss: 0.247205020033102
  batch 375 loss: 0.2471797967751821
  batch 376 loss: 0.2472989337241396
  batch 377 loss: 0.24725643665942337
  batch 378 loss: 0.2471947716105552
  batch 379 loss: 0.2471508986874432
  batch 380 loss: 0.24711317949389155
  batch 381 loss: 0.24706532244920104
  batch 382 loss: 0.24702486271009394
  batch 383 loss: 0.24689394675253576
  batch 384 loss: 0.24688815981304893
  batch 385 loss: 0.2468432036700187
  batch 386 loss: 0.24677123750892946
  batch 387 loss: 0.24673844696785435
  batch 388 loss: 0.24672911826943614
  batch 389 loss: 0.24667929438515004
  batch 390 loss: 0.246626175290499
  batch 391 loss: 0.2464692628444613
  batch 392 loss: 0.24649726036860017
  batch 393 loss: 0.24645450533043034
  batch 394 loss: 0.24645425852028852
  batch 395 loss: 0.24642375907565975
  batch 396 loss: 0.2464530648983488
  batch 397 loss: 0.2465259101363814
  batch 398 loss: 0.24649429238920834
  batch 399 loss: 0.24656862625502107
  batch 400 loss: 0.246685177013278
  batch 401 loss: 0.2466494106220783
  batch 402 loss: 0.2466646894367773
  batch 403 loss: 0.2466133510504408
  batch 404 loss: 0.2466452019934607
  batch 405 loss: 0.24672009746233622
  batch 406 loss: 0.246712539143163
  batch 407 loss: 0.24678995297347592
  batch 408 loss: 0.24680773271065132
  batch 409 loss: 0.24689453352051435
  batch 410 loss: 0.24681421769101444
  batch 411 loss: 0.24676388019917944
  batch 412 loss: 0.24683685727345134
  batch 413 loss: 0.24682960762094355
  batch 414 loss: 0.24684813701444203
  batch 415 loss: 0.24674735618643015
  batch 416 loss: 0.24674268933729485
  batch 417 loss: 0.24667169321641075
  batch 418 loss: 0.2466183278406636
  batch 419 loss: 0.2465500882815632
  batch 420 loss: 0.24653466886707714
  batch 421 loss: 0.24658490788200405
  batch 422 loss: 0.24652403547159304
  batch 423 loss: 0.24651482659028778
  batch 424 loss: 0.24658958318660845
  batch 425 loss: 0.2464542420471416
  batch 426 loss: 0.24633590709155714
  batch 427 loss: 0.24629045593654803
  batch 428 loss: 0.2462027998658541
  batch 429 loss: 0.24611735798956907
  batch 430 loss: 0.24615198938652527
  batch 431 loss: 0.2461253376612807
  batch 432 loss: 0.24605713781245328
  batch 433 loss: 0.24599776742227084
  batch 434 loss: 0.245915012837555
  batch 435 loss: 0.24595238216992082
  batch 436 loss: 0.24599353312899214
  batch 437 loss: 0.2459388301287963
  batch 438 loss: 0.24590699167148164
  batch 439 loss: 0.24582874045149342
  batch 440 loss: 0.24590960155156524
  batch 441 loss: 0.24593858103227723
  batch 442 loss: 0.24591786633519566
  batch 443 loss: 0.24579964527696305
  batch 444 loss: 0.24589728147865417
  batch 445 loss: 0.24582303534732775
  batch 446 loss: 0.24584287993041923
  batch 447 loss: 0.24585452369128832
  batch 448 loss: 0.24588406558281609
  batch 449 loss: 0.24588162496785543
  batch 450 loss: 0.24580223245753183
  batch 451 loss: 0.24573341279627214
  batch 452 loss: 0.24565626845686836
  batch 453 loss: 0.24562654820235885
  batch 454 loss: 0.24554719605241052
  batch 455 loss: 0.24560224069998815
  batch 456 loss: 0.24556446317256542
  batch 457 loss: 0.2456200041317053
  batch 458 loss: 0.24557168859842043
  batch 459 loss: 0.2455394961475547
  batch 460 loss: 0.2455291593204374
  batch 461 loss: 0.2455684322209265
  batch 462 loss: 0.24556722833867714
  batch 463 loss: 0.2455679831249925
  batch 464 loss: 0.2455504422621994
  batch 465 loss: 0.24552824705518703
  batch 466 loss: 0.24549516053798373
  batch 467 loss: 0.24557065794631297
  batch 468 loss: 0.2456383825176292
  batch 469 loss: 0.24551100631766737
  batch 470 loss: 0.2456436571288616
  batch 471 loss: 0.24573195132480305
  batch 472 loss: 0.24555146113290624
LOSS train 0.24555146113290624 valid 0.2169743925333023
LOSS train 0.24555146113290624 valid 0.2234417125582695
LOSS train 0.24555146113290624 valid 0.2426473448673884
LOSS train 0.24555146113290624 valid 0.2330368533730507
LOSS train 0.24555146113290624 valid 0.24277120232582092
LOSS train 0.24555146113290624 valid 0.24525070687135062
LOSS train 0.24555146113290624 valid 0.23572593927383423
LOSS train 0.24555146113290624 valid 0.23619494959712029
LOSS train 0.24555146113290624 valid 0.23640058437983194
LOSS train 0.24555146113290624 valid 0.23288121223449706
LOSS train 0.24555146113290624 valid 0.23164139281619678
LOSS train 0.24555146113290624 valid 0.23248627285162607
LOSS train 0.24555146113290624 valid 0.23502301711302537
LOSS train 0.24555146113290624 valid 0.23330038892371313
LOSS train 0.24555146113290624 valid 0.23209730287392935
LOSS train 0.24555146113290624 valid 0.23529435973614454
LOSS train 0.24555146113290624 valid 0.23537773770444534
LOSS train 0.24555146113290624 valid 0.23525147140026093
LOSS train 0.24555146113290624 valid 0.23630281498557643
LOSS train 0.24555146113290624 valid 0.2357719600200653
LOSS train 0.24555146113290624 valid 0.23759605345271884
LOSS train 0.24555146113290624 valid 0.23613480952653018
LOSS train 0.24555146113290624 valid 0.23463113346825476
LOSS train 0.24555146113290624 valid 0.23451976664364338
LOSS train 0.24555146113290624 valid 0.23487335860729217
LOSS train 0.24555146113290624 valid 0.23390411986754492
LOSS train 0.24555146113290624 valid 0.234254476096895
LOSS train 0.24555146113290624 valid 0.23451990634202957
LOSS train 0.24555146113290624 valid 0.23317767422774743
LOSS train 0.24555146113290624 valid 0.2317216436068217
LOSS train 0.24555146113290624 valid 0.23153184306237004
LOSS train 0.24555146113290624 valid 0.23226644471287727
LOSS train 0.24555146113290624 valid 0.2319211381854433
LOSS train 0.24555146113290624 valid 0.23149490970022538
LOSS train 0.24555146113290624 valid 0.23207444974354335
LOSS train 0.24555146113290624 valid 0.23292915026346842
LOSS train 0.24555146113290624 valid 0.23213712307246956
LOSS train 0.24555146113290624 valid 0.231851306400801
LOSS train 0.24555146113290624 valid 0.2311970293521881
LOSS train 0.24555146113290624 valid 0.2317149043083191
LOSS train 0.24555146113290624 valid 0.23119652343959343
LOSS train 0.24555146113290624 valid 0.2333841146457763
LOSS train 0.24555146113290624 valid 0.23383768838505412
LOSS train 0.24555146113290624 valid 0.23316886072809045
LOSS train 0.24555146113290624 valid 0.2328910575972663
LOSS train 0.24555146113290624 valid 0.23249213559472043
LOSS train 0.24555146113290624 valid 0.232200951018232
LOSS train 0.24555146113290624 valid 0.23409005254507065
LOSS train 0.24555146113290624 valid 0.23362851477399163
LOSS train 0.24555146113290624 valid 0.23426988571882248
LOSS train 0.24555146113290624 valid 0.23371525517865724
LOSS train 0.24555146113290624 valid 0.23367049172520638
LOSS train 0.24555146113290624 valid 0.234758235092433
LOSS train 0.24555146113290624 valid 0.23484855503947646
LOSS train 0.24555146113290624 valid 0.23471490442752838
LOSS train 0.24555146113290624 valid 0.23482072991984232
LOSS train 0.24555146113290624 valid 0.23416169719737873
LOSS train 0.24555146113290624 valid 0.23507667487037592
LOSS train 0.24555146113290624 valid 0.23493674953105087
LOSS train 0.24555146113290624 valid 0.23482614904642105
LOSS train 0.24555146113290624 valid 0.2351839542388916
LOSS train 0.24555146113290624 valid 0.23482517657741422
LOSS train 0.24555146113290624 valid 0.2347565231815217
LOSS train 0.24555146113290624 valid 0.23493209294974804
LOSS train 0.24555146113290624 valid 0.23368016733573033
LOSS train 0.24555146113290624 valid 0.23349843309684234
LOSS train 0.24555146113290624 valid 0.23428851566207942
LOSS train 0.24555146113290624 valid 0.23364357431145275
LOSS train 0.24555146113290624 valid 0.23418546327646228
LOSS train 0.24555146113290624 valid 0.23480815163680485
LOSS train 0.24555146113290624 valid 0.23521154867091648
LOSS train 0.24555146113290624 valid 0.23533512589832148
LOSS train 0.24555146113290624 valid 0.23599804121337525
LOSS train 0.24555146113290624 valid 0.23593143595231547
LOSS train 0.24555146113290624 valid 0.23571419874827068
LOSS train 0.24555146113290624 valid 0.23593286856224663
LOSS train 0.24555146113290624 valid 0.236180268711858
LOSS train 0.24555146113290624 valid 0.23587776204714409
LOSS train 0.24555146113290624 valid 0.2357992959927909
LOSS train 0.24555146113290624 valid 0.2353897735476494
LOSS train 0.24555146113290624 valid 0.23539423905772927
LOSS train 0.24555146113290624 valid 0.23506120065363442
LOSS train 0.24555146113290624 valid 0.2347805291773325
LOSS train 0.24555146113290624 valid 0.23475438019349462
LOSS train 0.24555146113290624 valid 0.23496415492366343
LOSS train 0.24555146113290624 valid 0.2348005709260009
LOSS train 0.24555146113290624 valid 0.2343306553432311
LOSS train 0.24555146113290624 valid 0.23421700129454787
LOSS train 0.24555146113290624 valid 0.2343200101946177
LOSS train 0.24555146113290624 valid 0.23446691913737192
LOSS train 0.24555146113290624 valid 0.2345403131547865
LOSS train 0.24555146113290624 valid 0.23428773183537566
LOSS train 0.24555146113290624 valid 0.23427348704107345
LOSS train 0.24555146113290624 valid 0.234398586160325
LOSS train 0.24555146113290624 valid 0.23443021570381364
LOSS train 0.24555146113290624 valid 0.234961639624089
LOSS train 0.24555146113290624 valid 0.2350459355361683
LOSS train 0.24555146113290624 valid 0.23523845493185278
LOSS train 0.24555146113290624 valid 0.23536713752481672
LOSS train 0.24555146113290624 valid 0.2352905371785164
LOSS train 0.24555146113290624 valid 0.23545666732410392
LOSS train 0.24555146113290624 valid 0.23595689558515362
LOSS train 0.24555146113290624 valid 0.235717935758887
LOSS train 0.24555146113290624 valid 0.23556294507132128
LOSS train 0.24555146113290624 valid 0.2359617490144003
LOSS train 0.24555146113290624 valid 0.2361326317461032
LOSS train 0.24555146113290624 valid 0.23580775389047426
LOSS train 0.24555146113290624 valid 0.2357446268476822
LOSS train 0.24555146113290624 valid 0.23512931236433326
LOSS train 0.24555146113290624 valid 0.2349377382885326
LOSS train 0.24555146113290624 valid 0.23504102901295498
LOSS train 0.24555146113290624 valid 0.23506870905735663
LOSS train 0.24555146113290624 valid 0.2348162633391608
LOSS train 0.24555146113290624 valid 0.2346619862997741
LOSS train 0.24555146113290624 valid 0.23516833354597505
LOSS train 0.24555146113290624 valid 0.23488936020896353
LOSS train 0.24555146113290624 valid 0.23541472393732804
LOSS train 0.24555146113290624 valid 0.23575021591732057
LOSS train 0.24555146113290624 valid 0.235461555734402
LOSS train 0.24555146113290624 valid 0.2352215992907683
LOSS train 0.24555146113290624 valid 0.2352036615787459
LOSS train 0.24555146113290624 valid 0.23543842466639692
LOSS train 0.24555146113290624 valid 0.23548135609646154
LOSS train 0.24555146113290624 valid 0.2354360377836612
LOSS train 0.24555146113290624 valid 0.2355729945898056
LOSS train 0.24555146113290624 valid 0.23584202440485122
LOSS train 0.24555146113290624 valid 0.23581688023927644
LOSS train 0.24555146113290624 valid 0.23567628755699843
LOSS train 0.24555146113290624 valid 0.23527628252672594
LOSS train 0.24555146113290624 valid 0.23497570409224583
LOSS train 0.24555146113290624 valid 0.23504762897509654
LOSS train 0.24555146113290624 valid 0.2349911682081945
LOSS train 0.24555146113290624 valid 0.23487321797170138
LOSS train 0.24555146113290624 valid 0.23506118201497775
LOSS train 0.24555146113290624 valid 0.23526522782113818
LOSS train 0.24555146113290624 valid 0.23548561794792905
LOSS train 0.24555146113290624 valid 0.2355159688605009
LOSS train 0.24555146113290624 valid 0.23542811937522198
LOSS train 0.24555146113290624 valid 0.23529318918427117
LOSS train 0.24555146113290624 valid 0.23549025612218039
LOSS train 0.24555146113290624 valid 0.2356962303743295
LOSS train 0.24555146113290624 valid 0.235950929388194
LOSS train 0.24555146113290624 valid 0.23604151116801309
LOSS train 0.24555146113290624 valid 0.2361494419682357
LOSS train 0.24555146113290624 valid 0.2359690323985856
LOSS train 0.24555146113290624 valid 0.23583173619149483
LOSS train 0.24555146113290624 valid 0.23577154655845797
LOSS train 0.24555146113290624 valid 0.23544871525184527
LOSS train 0.24555146113290624 valid 0.23552471159288546
LOSS train 0.24555146113290624 valid 0.23542758186658225
LOSS train 0.24555146113290624 valid 0.23525064009309604
LOSS train 0.24555146113290624 valid 0.23526676999110924
LOSS train 0.24555146113290624 valid 0.23498513807658278
LOSS train 0.24555146113290624 valid 0.2349890052498161
LOSS train 0.24555146113290624 valid 0.23498565727664578
LOSS train 0.24555146113290624 valid 0.23530350434474456
LOSS train 0.24555146113290624 valid 0.2352108428622507
LOSS train 0.24555146113290624 valid 0.2351036499006839
LOSS train 0.24555146113290624 valid 0.2353418967071569
LOSS train 0.24555146113290624 valid 0.23541863588616252
LOSS train 0.24555146113290624 valid 0.2355660782282397
LOSS train 0.24555146113290624 valid 0.23561797033489487
LOSS train 0.24555146113290624 valid 0.23533670713930774
LOSS train 0.24555146113290624 valid 0.23540536514142665
LOSS train 0.24555146113290624 valid 0.23551987481839728
LOSS train 0.24555146113290624 valid 0.2354579873113747
LOSS train 0.24555146113290624 valid 0.23553225085763874
LOSS train 0.24555146113290624 valid 0.2352657579772529
LOSS train 0.24555146113290624 valid 0.2350544345802104
LOSS train 0.24555146113290624 valid 0.23507314008824964
LOSS train 0.24555146113290624 valid 0.23487680462020183
LOSS train 0.24555146113290624 valid 0.2347730276889579
LOSS train 0.24555146113290624 valid 0.23471612565090202
LOSS train 0.24555146113290624 valid 0.2347044229678724
LOSS train 0.24555146113290624 valid 0.23454649124826704
LOSS train 0.24555146113290624 valid 0.23449155586687
LOSS train 0.24555146113290624 valid 0.23455607133396603
LOSS train 0.24555146113290624 valid 0.23458881508768276
LOSS train 0.24555146113290624 valid 0.23474383936913987
LOSS train 0.24555146113290624 valid 0.23484648383326
LOSS train 0.24555146113290624 valid 0.2347053719159648
LOSS train 0.24555146113290624 valid 0.23469311924098613
LOSS train 0.24555146113290624 valid 0.23468821564984452
LOSS train 0.24555146113290624 valid 0.23461594618856907
LOSS train 0.24555146113290624 valid 0.23468242875627568
LOSS train 0.24555146113290624 valid 0.2347416330569534
LOSS train 0.24555146113290624 valid 0.23453082032700912
LOSS train 0.24555146113290624 valid 0.23453990013358442
LOSS train 0.24555146113290624 valid 0.23446396639738135
LOSS train 0.24555146113290624 valid 0.23443166942972887
LOSS train 0.24555146113290624 valid 0.23467188589860008
LOSS train 0.24555146113290624 valid 0.23475025679605702
LOSS train 0.24555146113290624 valid 0.23456762182897856
LOSS train 0.24555146113290624 valid 0.2346096866831337
LOSS train 0.24555146113290624 valid 0.23451809332920956
LOSS train 0.24555146113290624 valid 0.23479330250803304
LOSS train 0.24555146113290624 valid 0.23470269869729346
LOSS train 0.24555146113290624 valid 0.23506152170776118
LOSS train 0.24555146113290624 valid 0.23500083723858972
LOSS train 0.24555146113290624 valid 0.23497772440314293
LOSS train 0.24555146113290624 valid 0.23501494719614438
LOSS train 0.24555146113290624 valid 0.23515293636534473
LOSS train 0.24555146113290624 valid 0.23533567683450107
LOSS train 0.24555146113290624 valid 0.2352369938264875
LOSS train 0.24555146113290624 valid 0.2351806369496555
LOSS train 0.24555146113290624 valid 0.23516290916979893
LOSS train 0.24555146113290624 valid 0.23513841607432434
LOSS train 0.24555146113290624 valid 0.23494689503254798
LOSS train 0.24555146113290624 valid 0.23497975735287918
LOSS train 0.24555146113290624 valid 0.23506184695732027
LOSS train 0.24555146113290624 valid 0.23529754564942906
LOSS train 0.24555146113290624 valid 0.2351233849547944
LOSS train 0.24555146113290624 valid 0.23525063210809735
LOSS train 0.24555146113290624 valid 0.23517603970298143
LOSS train 0.24555146113290624 valid 0.2348124840231829
LOSS train 0.24555146113290624 valid 0.23459426886229603
LOSS train 0.24555146113290624 valid 0.2343498096900052
LOSS train 0.24555146113290624 valid 0.2343297011535102
LOSS train 0.24555146113290624 valid 0.2343244506343859
LOSS train 0.24555146113290624 valid 0.23427231074734167
LOSS train 0.24555146113290624 valid 0.23415833111532133
LOSS train 0.24555146113290624 valid 0.23434179840055672
LOSS train 0.24555146113290624 valid 0.2342854418161204
LOSS train 0.24555146113290624 valid 0.23419830462496197
LOSS train 0.24555146113290624 valid 0.23410833842224546
LOSS train 0.24555146113290624 valid 0.23395592020412462
LOSS train 0.24555146113290624 valid 0.2339102235922204
LOSS train 0.24555146113290624 valid 0.23377556412627823
LOSS train 0.24555146113290624 valid 0.2336125659500147
LOSS train 0.24555146113290624 valid 0.2336114470725474
LOSS train 0.24555146113290624 valid 0.23362242569138994
LOSS train 0.24555146113290624 valid 0.23367065499568807
LOSS train 0.24555146113290624 valid 0.2338489847889274
LOSS train 0.24555146113290624 valid 0.23368286048499948
LOSS train 0.24555146113290624 valid 0.2336900659064029
LOSS train 0.24555146113290624 valid 0.23351400307679582
LOSS train 0.24555146113290624 valid 0.23358168864803475
LOSS train 0.24555146113290624 valid 0.233709704913512
LOSS train 0.24555146113290624 valid 0.23358203570463668
LOSS train 0.24555146113290624 valid 0.23322655589630206
LOSS train 0.24555146113290624 valid 0.2331176668529194
LOSS train 0.24555146113290624 valid 0.2331526444470587
LOSS train 0.24555146113290624 valid 0.2330631322453542
LOSS train 0.24555146113290624 valid 0.23299319901671567
LOSS train 0.24555146113290624 valid 0.23305124780353234
LOSS train 0.24555146113290624 valid 0.23312718627171788
LOSS train 0.24555146113290624 valid 0.23315978345842014
LOSS train 0.24555146113290624 valid 0.23327539242323367
LOSS train 0.24555146113290624 valid 0.2330734219895788
LOSS train 0.24555146113290624 valid 0.23332100880146026
LOSS train 0.24555146113290624 valid 0.23355877708153897
LOSS train 0.24555146113290624 valid 0.23366646397681462
LOSS train 0.24555146113290624 valid 0.23362751228536072
LOSS train 0.24555146113290624 valid 0.233754739634634
LOSS train 0.24555146113290624 valid 0.23367998711034363
LOSS train 0.24555146113290624 valid 0.23360277217580006
LOSS train 0.24555146113290624 valid 0.2335785820442415
LOSS train 0.24555146113290624 valid 0.23360114931598192
LOSS train 0.24555146113290624 valid 0.2334683807315053
LOSS train 0.24555146113290624 valid 0.23332493196313198
LOSS train 0.24555146113290624 valid 0.2332720997461414
LOSS train 0.24555146113290624 valid 0.2334625725527756
LOSS train 0.24555146113290624 valid 0.2333276121108704
LOSS train 0.24555146113290624 valid 0.23334453401691985
LOSS train 0.24555146113290624 valid 0.2332484560192756
LOSS train 0.24555146113290624 valid 0.2332095167130456
LOSS train 0.24555146113290624 valid 0.2331242079145453
LOSS train 0.24555146113290624 valid 0.2332409085400069
LOSS train 0.24555146113290624 valid 0.2333809373325575
LOSS train 0.24555146113290624 valid 0.23328309721416898
LOSS train 0.24555146113290624 valid 0.2332245850475072
LOSS train 0.24555146113290624 valid 0.23308547460200155
LOSS train 0.24555146113290624 valid 0.23302809517462175
LOSS train 0.24555146113290624 valid 0.23298996164850946
LOSS train 0.24555146113290624 valid 0.23315225395289335
LOSS train 0.24555146113290624 valid 0.2330846602394097
LOSS train 0.24555146113290624 valid 0.23301737984164958
LOSS train 0.24555146113290624 valid 0.23312583415628338
LOSS train 0.24555146113290624 valid 0.23318333720861797
LOSS train 0.24555146113290624 valid 0.23299712714340007
LOSS train 0.24555146113290624 valid 0.23290165892911552
LOSS train 0.24555146113290624 valid 0.23280997215010596
LOSS train 0.24555146113290624 valid 0.23290296683463108
LOSS train 0.24555146113290624 valid 0.232727316221301
LOSS train 0.24555146113290624 valid 0.23281241472353015
LOSS train 0.24555146113290624 valid 0.23282073250600507
LOSS train 0.24555146113290624 valid 0.2328721268771002
LOSS train 0.24555146113290624 valid 0.23281388983337414
LOSS train 0.24555146113290624 valid 0.23281816849452813
LOSS train 0.24555146113290624 valid 0.23273761477963678
LOSS train 0.24555146113290624 valid 0.23271951074247918
LOSS train 0.24555146113290624 valid 0.2327041532589148
LOSS train 0.24555146113290624 valid 0.23267069530161574
LOSS train 0.24555146113290624 valid 0.23257132614550946
LOSS train 0.24555146113290624 valid 0.2325972753561149
LOSS train 0.24555146113290624 valid 0.23256729417354674
LOSS train 0.24555146113290624 valid 0.2326360065427292
LOSS train 0.24555146113290624 valid 0.2326968945812859
LOSS train 0.24555146113290624 valid 0.23262736086662
LOSS train 0.24555146113290624 valid 0.232597686201334
LOSS train 0.24555146113290624 valid 0.23252355265260932
LOSS train 0.24555146113290624 valid 0.2325613416780699
LOSS train 0.24555146113290624 valid 0.2324926480405008
LOSS train 0.24555146113290624 valid 0.23249561819983156
LOSS train 0.24555146113290624 valid 0.23240888758760983
LOSS train 0.24555146113290624 valid 0.23267828517295178
LOSS train 0.24555146113290624 valid 0.2326063279317334
LOSS train 0.24555146113290624 valid 0.23276044646060312
LOSS train 0.24555146113290624 valid 0.2327992209628176
LOSS train 0.24555146113290624 valid 0.23282407416451362
LOSS train 0.24555146113290624 valid 0.2328718574388234
LOSS train 0.24555146113290624 valid 0.23287285381975847
LOSS train 0.24555146113290624 valid 0.23300279429354986
LOSS train 0.24555146113290624 valid 0.232920126218325
LOSS train 0.24555146113290624 valid 0.23295105217941223
LOSS train 0.24555146113290624 valid 0.23298440595405012
LOSS train 0.24555146113290624 valid 0.23296923286719276
LOSS train 0.24555146113290624 valid 0.23300109311657133
LOSS train 0.24555146113290624 valid 0.23298626442127468
LOSS train 0.24555146113290624 valid 0.2328845215961337
LOSS train 0.24555146113290624 valid 0.23290497621643208
LOSS train 0.24555146113290624 valid 0.23288841411378813
LOSS train 0.24555146113290624 valid 0.23273628608730187
LOSS train 0.24555146113290624 valid 0.23270388688018293
LOSS train 0.24555146113290624 valid 0.23251310871197628
LOSS train 0.24555146113290624 valid 0.2326426375314502
LOSS train 0.24555146113290624 valid 0.23265107258561918
LOSS train 0.24555146113290624 valid 0.23270909783498542
LOSS train 0.24555146113290624 valid 0.2326668143091231
LOSS train 0.24555146113290624 valid 0.23269624082428036
LOSS train 0.24555146113290624 valid 0.23264798684062554
LOSS train 0.24555146113290624 valid 0.2325720366045653
LOSS train 0.24555146113290624 valid 0.23265455837722296
LOSS train 0.24555146113290624 valid 0.2326159414327787
LOSS train 0.24555146113290624 valid 0.2325987441771066
LOSS train 0.24555146113290624 valid 0.2325215172022581
LOSS train 0.24555146113290624 valid 0.23267629963707853
LOSS train 0.24555146113290624 valid 0.23258181224913288
LOSS train 0.24555146113290624 valid 0.23256469093768645
LOSS train 0.24555146113290624 valid 0.23268924824455206
LOSS train 0.24555146113290624 valid 0.23268181138024652
LOSS train 0.24555146113290624 valid 0.23264793106164153
LOSS train 0.24555146113290624 valid 0.23259477643160362
LOSS train 0.24555146113290624 valid 0.2326276207905869
LOSS train 0.24555146113290624 valid 0.2327081044514974
LOSS train 0.24555146113290624 valid 0.23262474095890287
LOSS train 0.24555146113290624 valid 0.23255888855766493
LOSS train 0.24555146113290624 valid 0.23261064736322426
LOSS train 0.24555146113290624 valid 0.23253712028179607
LOSS train 0.24555146113290624 valid 0.23258350759744645
LOSS train 0.24555146113290624 valid 0.23267504288430227
LOSS train 0.24555146113290624 valid 0.23278733999045056
LOSS train 0.24555146113290624 valid 0.23290171193502443
LOSS train 0.24555146113290624 valid 0.23286524065638667
LOSS train 0.24555146113290624 valid 0.23277492397268054
LOSS train 0.24555146113290624 valid 0.23274374296993353
LOSS train 0.24555146113290624 valid 0.2328188379689091
LOSS train 0.24555146113290624 valid 0.23282556169192883
LOSS train 0.24555146113290624 valid 0.23277329751043932
LOSS train 0.24555146113290624 valid 0.2329178526169724
LOSS train 0.24555146113290624 valid 0.23288069958501906
LOSS train 0.24555146113290624 valid 0.23300171804032932
LOSS train 0.24555146113290624 valid 0.23305046952460423
LOSS train 0.24555146113290624 valid 0.23290071873874454
LOSS train 0.24555146113290624 valid 0.23301880645425352
LOSS train 0.24555146113290624 valid 0.23304492471322336
LOSS train 0.24555146113290624 valid 0.23305733501911163
LOSS train 0.24555146113290624 valid 0.232961921831188
LOSS train 0.24555146113290624 valid 0.23302317828666874
EPOCH 14:
  batch 1 loss: 0.26056772470474243
  batch 2 loss: 0.26216503977775574
  batch 3 loss: 0.253190815448761
  batch 4 loss: 0.25632739812135696
  batch 5 loss: 0.2702970027923584
  batch 6 loss: 0.26256391406059265
  batch 7 loss: 0.26554528304508757
  batch 8 loss: 0.26331087946891785
  batch 9 loss: 0.26571177111731636
  batch 10 loss: 0.262705397605896
  batch 11 loss: 0.2633940333669836
  batch 12 loss: 0.2605757601559162
  batch 13 loss: 0.26063128274220687
  batch 14 loss: 0.26052772466625485
  batch 15 loss: 0.2597433219353358
  batch 16 loss: 0.25881995912641287
  batch 17 loss: 0.26020389357033896
  batch 18 loss: 0.2599766378601392
  batch 19 loss: 0.2594302194683175
  batch 20 loss: 0.2561471305787563
  batch 21 loss: 0.25769869841280435
  batch 22 loss: 0.2588536421006376
  batch 23 loss: 0.2572752172532289
  batch 24 loss: 0.2571265722314517
  batch 25 loss: 0.2566002851724625
  batch 26 loss: 0.2569661639057673
  batch 27 loss: 0.2559781626418785
  batch 28 loss: 0.2561686784029007
  batch 29 loss: 0.25543183910435646
  batch 30 loss: 0.255511486530304
  batch 31 loss: 0.2552109754854633
  batch 32 loss: 0.25568665470927954
  batch 33 loss: 0.25744905706607935
  batch 34 loss: 0.2583952817846747
  batch 35 loss: 0.2594254629952567
  batch 36 loss: 0.25887148247824776
  batch 37 loss: 0.2586241030209773
  batch 38 loss: 0.2600240374082013
  batch 39 loss: 0.2610963923044694
  batch 40 loss: 0.2608115017414093
  batch 41 loss: 0.2614763277332957
  batch 42 loss: 0.2615805403107688
  batch 43 loss: 0.2618695906428404
  batch 44 loss: 0.26203967563130637
  batch 45 loss: 0.26171439753638376
  batch 46 loss: 0.2601754046652628
  batch 47 loss: 0.2609541419338673
  batch 48 loss: 0.2602405333891511
  batch 49 loss: 0.2603437581232616
  batch 50 loss: 0.26006540536880496
  batch 51 loss: 0.26006927092870075
  batch 52 loss: 0.2591256550871409
  batch 53 loss: 0.25794215331662373
  batch 54 loss: 0.2573348507285118
  batch 55 loss: 0.2564880595965819
  batch 56 loss: 0.25618108654660837
  batch 57 loss: 0.2552511830601776
  batch 58 loss: 0.25419392333976154
  batch 59 loss: 0.254556697556528
  batch 60 loss: 0.2553433972100417
  batch 61 loss: 0.25589288038308505
  batch 62 loss: 0.2564364693337871
  batch 63 loss: 0.2560424390766356
  batch 64 loss: 0.25587015179917216
  batch 65 loss: 0.255458432665238
  batch 66 loss: 0.2551448024583585
  batch 67 loss: 0.2550464757342837
  batch 68 loss: 0.2549099729341619
  batch 69 loss: 0.25503577572712
  batch 70 loss: 0.25435773730278016
  batch 71 loss: 0.25417047424215666
  batch 72 loss: 0.2541250331948201
  batch 73 loss: 0.2537410261288081
  batch 74 loss: 0.2533403004746179
  batch 75 loss: 0.2532692631085714
  batch 76 loss: 0.253243051469326
  batch 77 loss: 0.25332423541452975
  batch 78 loss: 0.253016203832932
  batch 79 loss: 0.2526501956619794
  batch 80 loss: 0.2521196497604251
  batch 81 loss: 0.25166916755246527
  batch 82 loss: 0.25184262216818043
  batch 83 loss: 0.2515237406793847
  batch 84 loss: 0.2509285679885319
  batch 85 loss: 0.2510546203921823
  batch 86 loss: 0.25119975451813187
  batch 87 loss: 0.25093452529660587
  batch 88 loss: 0.2508222685957497
  batch 89 loss: 0.25043260013119556
  batch 90 loss: 0.2507965568039152
  batch 91 loss: 0.2504498197154684
  batch 92 loss: 0.250449286368878
  batch 93 loss: 0.25041138532982077
  batch 94 loss: 0.250613142359764
  batch 95 loss: 0.25083355950681785
  batch 96 loss: 0.25106583364928764
  batch 97 loss: 0.2514951221414448
  batch 98 loss: 0.25135886897237936
  batch 99 loss: 0.2513127852268893
  batch 100 loss: 0.25140287771821024
  batch 101 loss: 0.2514183626021489
  batch 102 loss: 0.2515942859006863
  batch 103 loss: 0.2515194362517699
  batch 104 loss: 0.25179833503296745
  batch 105 loss: 0.2517354670025054
  batch 106 loss: 0.25160781350338235
  batch 107 loss: 0.25143640467496675
  batch 108 loss: 0.25106876343488693
  batch 109 loss: 0.2511814058920659
  batch 110 loss: 0.2510345854542472
  batch 111 loss: 0.2509113029316739
  batch 112 loss: 0.2507937676938517
  batch 113 loss: 0.2503287329346733
  batch 114 loss: 0.2501185065821597
  batch 115 loss: 0.24938082008258156
  batch 116 loss: 0.24904799448518916
  batch 117 loss: 0.2487065957652198
  batch 118 loss: 0.2486820273985297
  batch 119 loss: 0.24880263034035177
  batch 120 loss: 0.2482876763989528
  batch 121 loss: 0.2483486091056146
  batch 122 loss: 0.24810940295946401
  batch 123 loss: 0.2480446244400691
  batch 124 loss: 0.24762379366063303
  batch 125 loss: 0.247448477268219
  batch 126 loss: 0.24769636326366
  batch 127 loss: 0.24745174468032957
  batch 128 loss: 0.24716801720205694
  batch 129 loss: 0.24721993251826413
  batch 130 loss: 0.24709437489509584
  batch 131 loss: 0.24714497300504729
  batch 132 loss: 0.24703756415031172
  batch 133 loss: 0.24682182779437617
  batch 134 loss: 0.2465612586976877
  batch 135 loss: 0.2464399817917082
  batch 136 loss: 0.24660402065252557
  batch 137 loss: 0.2467089065887632
  batch 138 loss: 0.24677963544061218
  batch 139 loss: 0.24684528985040652
  batch 140 loss: 0.24711165502667426
  batch 141 loss: 0.24686386980486255
  batch 142 loss: 0.24674372091679506
  batch 143 loss: 0.24696299548332507
  batch 144 loss: 0.2468779483396146
  batch 145 loss: 0.24690432661566242
  batch 146 loss: 0.24701559921241786
  batch 147 loss: 0.2469244102636973
  batch 148 loss: 0.246907138341182
  batch 149 loss: 0.24647230879972445
  batch 150 loss: 0.2462600099047025
  batch 151 loss: 0.24582319307011485
  batch 152 loss: 0.24566768364686714
  batch 153 loss: 0.245742450936947
  batch 154 loss: 0.2454867015604849
  batch 155 loss: 0.2455048435157345
  batch 156 loss: 0.24541030537623626
  batch 157 loss: 0.2452105140420282
  batch 158 loss: 0.2453423435740833
  batch 159 loss: 0.24541402228598325
  batch 160 loss: 0.24521978218108414
  batch 161 loss: 0.24494216151489234
  batch 162 loss: 0.24472356615243135
  batch 163 loss: 0.24452005646711478
  batch 164 loss: 0.2445981393500072
  batch 165 loss: 0.24468142570871296
  batch 166 loss: 0.2446859112705093
  batch 167 loss: 0.24456975390454253
  batch 168 loss: 0.2443155005042042
  batch 169 loss: 0.24438984938979855
  batch 170 loss: 0.2444310783463366
  batch 171 loss: 0.2445619961148814
  batch 172 loss: 0.24458350345145824
  batch 173 loss: 0.24463674202130709
  batch 174 loss: 0.24466561740157247
  batch 175 loss: 0.24457744990076338
  batch 176 loss: 0.24471442723138767
  batch 177 loss: 0.24498200062978065
  batch 178 loss: 0.2446903062335561
  batch 179 loss: 0.24490329023846036
  batch 180 loss: 0.24476182618074946
  batch 181 loss: 0.24461156039277493
  batch 182 loss: 0.24454726450718367
  batch 183 loss: 0.24430267317373244
  batch 184 loss: 0.24411824005453484
  batch 185 loss: 0.24386377608453905
  batch 186 loss: 0.24387748050753788
  batch 187 loss: 0.24424958747019743
  batch 188 loss: 0.24410406753737876
  batch 189 loss: 0.2440726310803146
  batch 190 loss: 0.24419981413765957
  batch 191 loss: 0.24452086799431846
  batch 192 loss: 0.24437899142503738
  batch 193 loss: 0.24443690270339888
  batch 194 loss: 0.244519267960922
  batch 195 loss: 0.24473591630275432
  batch 196 loss: 0.24468775678958213
  batch 197 loss: 0.2447890997084264
  batch 198 loss: 0.24485855414108795
  batch 199 loss: 0.24499606554532172
  batch 200 loss: 0.2450190143287182
  batch 201 loss: 0.24483754029914515
  batch 202 loss: 0.24469422261313636
  batch 203 loss: 0.24472529606278895
  batch 204 loss: 0.24461552150109234
  batch 205 loss: 0.24445820923258618
  batch 206 loss: 0.24438071742798517
  batch 207 loss: 0.24423731431580972
  batch 208 loss: 0.24415504122869328
  batch 209 loss: 0.24397265390631115
  batch 210 loss: 0.24399565926619937
  batch 211 loss: 0.2438223332307915
  batch 212 loss: 0.24377786578997127
  batch 213 loss: 0.24370550493679136
  batch 214 loss: 0.24365409289565043
  batch 215 loss: 0.24367100152858467
  batch 216 loss: 0.24387074951772336
  batch 217 loss: 0.24379135057124124
  batch 218 loss: 0.24362618400963074
  batch 219 loss: 0.24336188929538205
  batch 220 loss: 0.24358058470216665
  batch 221 loss: 0.24343873479517336
  batch 222 loss: 0.24333105907515362
  batch 223 loss: 0.24328545865189335
  batch 224 loss: 0.24334027930828078
  batch 225 loss: 0.24342969212267135
  batch 226 loss: 0.24355059921477748
  batch 227 loss: 0.24351767158455787
  batch 228 loss: 0.24361395097354002
  batch 229 loss: 0.2437443848390246
  batch 230 loss: 0.24385840108861093
  batch 231 loss: 0.24393371166862968
  batch 232 loss: 0.2437992199356186
  batch 233 loss: 0.2438397931310752
  batch 234 loss: 0.24387065384887222
  batch 235 loss: 0.24376214577796612
  batch 236 loss: 0.24377491312511898
  batch 237 loss: 0.24382208721547188
  batch 238 loss: 0.24389737944643036
  batch 239 loss: 0.24392787257996562
  batch 240 loss: 0.24394686333835125
  batch 241 loss: 0.2440890668586082
  batch 242 loss: 0.24422210701240982
  batch 243 loss: 0.2441786523954368
  batch 244 loss: 0.24429164081811905
  batch 245 loss: 0.24454557056329687
  batch 246 loss: 0.2446585587127422
  batch 247 loss: 0.2446566806872364
  batch 248 loss: 0.24487233390250512
  batch 249 loss: 0.2451522297648541
  batch 250 loss: 0.24525265681743622
  batch 251 loss: 0.24512503975891023
  batch 252 loss: 0.24509232408470577
  batch 253 loss: 0.24518746396769647
  batch 254 loss: 0.2453413508304461
  batch 255 loss: 0.24520456148128883
  batch 256 loss: 0.24517208233010024
  batch 257 loss: 0.24516090066516447
  batch 258 loss: 0.24512072837398957
  batch 259 loss: 0.24510859315459793
  batch 260 loss: 0.24502959446265146
  batch 261 loss: 0.24531596567895678
  batch 262 loss: 0.24529206354654473
  batch 263 loss: 0.24522053560829887
  batch 264 loss: 0.24550654806874014
  batch 265 loss: 0.24562111305740644
  batch 266 loss: 0.24561841914752372
  batch 267 loss: 0.24566603036185775
  batch 268 loss: 0.245634263464764
  batch 269 loss: 0.24566475842078822
  batch 270 loss: 0.2456268870168262
  batch 271 loss: 0.24576247618207192
  batch 272 loss: 0.24574909956358812
  batch 273 loss: 0.2456841589985313
  batch 274 loss: 0.24557705870727553
  batch 275 loss: 0.24554890692234038
  batch 276 loss: 0.24548721351269362
  batch 277 loss: 0.2455462743982081
  batch 278 loss: 0.245494565464181
  batch 279 loss: 0.24548787754496365
  batch 280 loss: 0.24530457449810847
  batch 281 loss: 0.24532079113335797
  batch 282 loss: 0.2453480672540394
  batch 283 loss: 0.2452330757788129
  batch 284 loss: 0.24516879247737602
  batch 285 loss: 0.24508602577343322
  batch 286 loss: 0.24519476242415555
  batch 287 loss: 0.24511502262607268
  batch 288 loss: 0.24514847228096592
  batch 289 loss: 0.24509839172181785
  batch 290 loss: 0.24505862809460738
  batch 291 loss: 0.24504474097305967
  batch 292 loss: 0.2449679104738856
  batch 293 loss: 0.24496651702773448
  batch 294 loss: 0.24490284580154484
  batch 295 loss: 0.24486161283517288
  batch 296 loss: 0.24483618064708002
  batch 297 loss: 0.24480480908946156
  batch 298 loss: 0.24479916931798795
  batch 299 loss: 0.24485773436201855
  batch 300 loss: 0.24481329783797265
  batch 301 loss: 0.24483440628281464
  batch 302 loss: 0.24501165851259862
  batch 303 loss: 0.2450108865503431
  batch 304 loss: 0.24501742466696
  batch 305 loss: 0.2449798426178635
  batch 306 loss: 0.2452087486588877
  batch 307 loss: 0.24529375310635335
  batch 308 loss: 0.24541036170217898
  batch 309 loss: 0.24532548822824238
  batch 310 loss: 0.24552877752050276
  batch 311 loss: 0.245487897390338
  batch 312 loss: 0.24552655883897573
  batch 313 loss: 0.24554889142132416
  batch 314 loss: 0.2455336247469969
  batch 315 loss: 0.24550814183931502
  batch 316 loss: 0.24549577870889555
  batch 317 loss: 0.24560585932964782
  batch 318 loss: 0.24554180827155803
  batch 319 loss: 0.24545595769224496
  batch 320 loss: 0.24560314677655698
  batch 321 loss: 0.2455267155096167
  batch 322 loss: 0.24535378674912897
  batch 323 loss: 0.24530998605877252
  batch 324 loss: 0.24528854639257913
  batch 325 loss: 0.24525656603849852
  batch 326 loss: 0.24530164454063755
  batch 327 loss: 0.24524640093702788
  batch 328 loss: 0.24525803441136348
  batch 329 loss: 0.24528150450676045
  batch 330 loss: 0.24527872529896824
  batch 331 loss: 0.24527020014124695
  batch 332 loss: 0.24541385054408785
  batch 333 loss: 0.24539032887231121
  batch 334 loss: 0.24551447311382807
  batch 335 loss: 0.2455182043918923
  batch 336 loss: 0.2455461196867483
  batch 337 loss: 0.2455187608827823
  batch 338 loss: 0.24549986109049363
  batch 339 loss: 0.2454684770564414
  batch 340 loss: 0.24553490035674153
  batch 341 loss: 0.24555385357473603
  batch 342 loss: 0.2455451719419301
  batch 343 loss: 0.24560532582049466
  batch 344 loss: 0.24560271680008533
  batch 345 loss: 0.2456177233785823
  batch 346 loss: 0.2456300166042554
  batch 347 loss: 0.24550274967803734
  batch 348 loss: 0.245463402492219
  batch 349 loss: 0.24553736339838253
  batch 350 loss: 0.2455682869042669
  batch 351 loss: 0.2455234087345607
  batch 352 loss: 0.24546073482964526
  batch 353 loss: 0.2455051411734762
  batch 354 loss: 0.2456228144172221
  batch 355 loss: 0.24558083914535148
  batch 356 loss: 0.2456209407177534
  batch 357 loss: 0.24555354429727175
  batch 358 loss: 0.24542591762442828
  batch 359 loss: 0.2453710365843308
  batch 360 loss: 0.24529594774875377
  batch 361 loss: 0.24525707935362312
  batch 362 loss: 0.24519247709881534
  batch 363 loss: 0.24521405510501756
  batch 364 loss: 0.24510708930236952
  batch 365 loss: 0.24516873037161893
  batch 366 loss: 0.2450947384120988
  batch 367 loss: 0.24512569403291073
  batch 368 loss: 0.24495433485540358
  batch 369 loss: 0.24495468969106027
  batch 370 loss: 0.24510570059756975
  batch 371 loss: 0.24506126874380035
  batch 372 loss: 0.24509589978924362
  batch 373 loss: 0.24518344801009181
  batch 374 loss: 0.2452036653530789
  batch 375 loss: 0.24519672242800394
  batch 376 loss: 0.24531045524363823
  batch 377 loss: 0.2452938215960242
  batch 378 loss: 0.24521511734005005
  batch 379 loss: 0.245211344713586
  batch 380 loss: 0.2451715238392353
  batch 381 loss: 0.24510079821733038
  batch 382 loss: 0.24507417865292563
  batch 383 loss: 0.2449245744289991
  batch 384 loss: 0.24493229377549142
  batch 385 loss: 0.24483998379149993
  batch 386 loss: 0.24475913436919297
  batch 387 loss: 0.24472553300303082
  batch 388 loss: 0.24466771229179865
  batch 389 loss: 0.24461616721281967
  batch 390 loss: 0.2446350512978358
  batch 391 loss: 0.24446837943228308
  batch 392 loss: 0.24450569014464105
  batch 393 loss: 0.244471077473109
  batch 394 loss: 0.24449941205796857
  batch 395 loss: 0.2444361576928368
  batch 396 loss: 0.24449477769961261
  batch 397 loss: 0.24450094600018085
  batch 398 loss: 0.24446056861823529
  batch 399 loss: 0.24443947189583218
  batch 400 loss: 0.24452304285019635
  batch 401 loss: 0.24449919944540818
  batch 402 loss: 0.24446803097849462
  batch 403 loss: 0.24439267455496208
  batch 404 loss: 0.24447707846613215
  batch 405 loss: 0.24454322788450453
  batch 406 loss: 0.24453161627494643
  batch 407 loss: 0.24458548983133396
  batch 408 loss: 0.24459367190652034
  batch 409 loss: 0.24473282975351898
  batch 410 loss: 0.24467594020250366
  batch 411 loss: 0.24463959128897267
  batch 412 loss: 0.24469580436215818
  batch 413 loss: 0.24475789899976144
  batch 414 loss: 0.24483866089784004
  batch 415 loss: 0.24474625515650553
  batch 416 loss: 0.24473289722720018
  batch 417 loss: 0.24469912495258614
  batch 418 loss: 0.24466352899108776
  batch 419 loss: 0.2446232505302725
  batch 420 loss: 0.24458178615286236
  batch 421 loss: 0.24466110273947908
  batch 422 loss: 0.2446496576353272
  batch 423 loss: 0.2445966638360463
  batch 424 loss: 0.2446596186326922
  batch 425 loss: 0.24453513113891376
  batch 426 loss: 0.24444039634695636
  batch 427 loss: 0.2444188890747499
  batch 428 loss: 0.24434102075957806
  batch 429 loss: 0.24427371112616747
  batch 430 loss: 0.24428947422393532
  batch 431 loss: 0.2442600615063289
  batch 432 loss: 0.24416943046229858
  batch 433 loss: 0.2440856995263221
  batch 434 loss: 0.24404925770611258
  batch 435 loss: 0.24409450138437336
  batch 436 loss: 0.24411586251690848
  batch 437 loss: 0.24406433681737913
  batch 438 loss: 0.24403014125889294
  batch 439 loss: 0.24393620839287317
  batch 440 loss: 0.2439582397653298
  batch 441 loss: 0.2440019480698233
  batch 442 loss: 0.24402559346338204
  batch 443 loss: 0.243932355270967
  batch 444 loss: 0.24398287996515497
  batch 445 loss: 0.2439405979065413
  batch 446 loss: 0.2439573286226512
  batch 447 loss: 0.243937420531674
  batch 448 loss: 0.24393233746689344
  batch 449 loss: 0.24396440250703647
  batch 450 loss: 0.24393861565324995
  batch 451 loss: 0.24386834229307533
  batch 452 loss: 0.24373861870407004
  batch 453 loss: 0.24373907321204677
  batch 454 loss: 0.2436942578281075
  batch 455 loss: 0.24374538398051
  batch 456 loss: 0.24368952632996074
  batch 457 loss: 0.24380188939086198
  batch 458 loss: 0.24382751470830244
  batch 459 loss: 0.2438027112702139
  batch 460 loss: 0.24380428376405136
  batch 461 loss: 0.24389207557070022
  batch 462 loss: 0.2439392288396885
  batch 463 loss: 0.24398048334708738
  batch 464 loss: 0.2438939304636984
  batch 465 loss: 0.24390560590451763
  batch 466 loss: 0.24389265106727126
  batch 467 loss: 0.243975928368354
  batch 468 loss: 0.24396983288928994
  batch 469 loss: 0.24385651081864004
  batch 470 loss: 0.243892887615143
  batch 471 loss: 0.24392200269263767
  batch 472 loss: 0.24377121031284332
LOSS train 0.24377121031284332 valid 0.24744874238967896
LOSS train 0.24377121031284332 valid 0.2507918179035187
LOSS train 0.24377121031284332 valid 0.27318910757700604
LOSS train 0.24377121031284332 valid 0.26362087577581406
LOSS train 0.24377121031284332 valid 0.2746167242527008
LOSS train 0.24377121031284332 valid 0.2793545722961426
LOSS train 0.24377121031284332 valid 0.26905285673482077
LOSS train 0.24377121031284332 valid 0.2699850741773844
LOSS train 0.24377121031284332 valid 0.2702701538801193
LOSS train 0.24377121031284332 valid 0.2672732248902321
LOSS train 0.24377121031284332 valid 0.26670134744860907
LOSS train 0.24377121031284332 valid 0.2678760029375553
LOSS train 0.24377121031284332 valid 0.2703084819591962
LOSS train 0.24377121031284332 valid 0.2676002244864191
LOSS train 0.24377121031284332 valid 0.2656837433576584
LOSS train 0.24377121031284332 valid 0.2698555560782552
LOSS train 0.24377121031284332 valid 0.27041550857179303
LOSS train 0.24377121031284332 valid 0.2705404915743404
LOSS train 0.24377121031284332 valid 0.2714788498062837
LOSS train 0.24377121031284332 valid 0.270733592659235
LOSS train 0.24377121031284332 valid 0.2727784493139812
LOSS train 0.24377121031284332 valid 0.270791963420131
LOSS train 0.24377121031284332 valid 0.26876030473605445
LOSS train 0.24377121031284332 valid 0.2683269412567218
LOSS train 0.24377121031284332 valid 0.2691065555810928
LOSS train 0.24377121031284332 valid 0.2679705000840701
LOSS train 0.24377121031284332 valid 0.26809590061505634
LOSS train 0.24377121031284332 valid 0.268607639840671
LOSS train 0.24377121031284332 valid 0.266756275090678
LOSS train 0.24377121031284332 valid 0.264934508005778
LOSS train 0.24377121031284332 valid 0.2649273285942693
LOSS train 0.24377121031284332 valid 0.26546164136379957
LOSS train 0.24377121031284332 valid 0.2649843164465644
LOSS train 0.24377121031284332 valid 0.26449647063718124
LOSS train 0.24377121031284332 valid 0.2652283698320389
LOSS train 0.24377121031284332 valid 0.26620664323369664
LOSS train 0.24377121031284332 valid 0.26519328597429637
LOSS train 0.24377121031284332 valid 0.2648988107317372
LOSS train 0.24377121031284332 valid 0.264225436708866
LOSS train 0.24377121031284332 valid 0.26494771651923654
LOSS train 0.24377121031284332 valid 0.26432475447654724
LOSS train 0.24377121031284332 valid 0.26653547514052617
LOSS train 0.24377121031284332 valid 0.26699757991835127
LOSS train 0.24377121031284332 valid 0.2665371779691089
LOSS train 0.24377121031284332 valid 0.26604246894518535
LOSS train 0.24377121031284332 valid 0.2655771267802819
LOSS train 0.24377121031284332 valid 0.2650179498373194
LOSS train 0.24377121031284332 valid 0.26707309515525895
LOSS train 0.24377121031284332 valid 0.26658089580584543
LOSS train 0.24377121031284332 valid 0.26698619455099104
LOSS train 0.24377121031284332 valid 0.2665030161539714
LOSS train 0.24377121031284332 valid 0.26655535170665157
LOSS train 0.24377121031284332 valid 0.26758825554037996
LOSS train 0.24377121031284332 valid 0.26765297574025615
LOSS train 0.24377121031284332 valid 0.2674850642681122
LOSS train 0.24377121031284332 valid 0.26761613094380926
LOSS train 0.24377121031284332 valid 0.26697439974860143
LOSS train 0.24377121031284332 valid 0.2679417141038796
LOSS train 0.24377121031284332 valid 0.26782862791570566
LOSS train 0.24377121031284332 valid 0.2677212340136369
LOSS train 0.24377121031284332 valid 0.2681414629103708
LOSS train 0.24377121031284332 valid 0.2678367389786628
LOSS train 0.24377121031284332 valid 0.2676594238432627
LOSS train 0.24377121031284332 valid 0.26789072528481483
LOSS train 0.24377121031284332 valid 0.2664333547537143
LOSS train 0.24377121031284332 valid 0.26614860422683484
LOSS train 0.24377121031284332 valid 0.26716474944086216
LOSS train 0.24377121031284332 valid 0.2664683892884675
LOSS train 0.24377121031284332 valid 0.2671662255905677
LOSS train 0.24377121031284332 valid 0.26784254376377375
LOSS train 0.24377121031284332 valid 0.26830913000543355
LOSS train 0.24377121031284332 valid 0.2685494830624925
LOSS train 0.24377121031284332 valid 0.26938705636213905
LOSS train 0.24377121031284332 valid 0.26934825387355443
LOSS train 0.24377121031284332 valid 0.26901430904865264
LOSS train 0.24377121031284332 valid 0.26934405867206423
LOSS train 0.24377121031284332 valid 0.2696713454924621
LOSS train 0.24377121031284332 valid 0.269321681979375
LOSS train 0.24377121031284332 valid 0.26909480291076854
LOSS train 0.24377121031284332 valid 0.26860957592725754
LOSS train 0.24377121031284332 valid 0.2687206209441762
LOSS train 0.24377121031284332 valid 0.26842832474446876
LOSS train 0.24377121031284332 valid 0.2683092770447214
LOSS train 0.24377121031284332 valid 0.26830769348002614
LOSS train 0.24377121031284332 valid 0.2685754406101563
LOSS train 0.24377121031284332 valid 0.2682434287528659
LOSS train 0.24377121031284332 valid 0.2677877625857277
LOSS train 0.24377121031284332 valid 0.2675674982707609
LOSS train 0.24377121031284332 valid 0.2676822990179062
LOSS train 0.24377121031284332 valid 0.2677352363864581
LOSS train 0.24377121031284332 valid 0.2677105584969887
LOSS train 0.24377121031284332 valid 0.267355237480091
LOSS train 0.24377121031284332 valid 0.2672467643535265
LOSS train 0.24377121031284332 valid 0.2673784689383304
LOSS train 0.24377121031284332 valid 0.2673700058146527
LOSS train 0.24377121031284332 valid 0.2678651041351259
LOSS train 0.24377121031284332 valid 0.2680010247169082
LOSS train 0.24377121031284332 valid 0.2682791183493575
LOSS train 0.24377121031284332 valid 0.2684129920571741
LOSS train 0.24377121031284332 valid 0.26845899775624277
LOSS train 0.24377121031284332 valid 0.26854418808280833
LOSS train 0.24377121031284332 valid 0.26907339034711614
LOSS train 0.24377121031284332 valid 0.26884624260721857
LOSS train 0.24377121031284332 valid 0.268566649263868
LOSS train 0.24377121031284332 valid 0.2689707751785006
LOSS train 0.24377121031284332 valid 0.2690976207267563
LOSS train 0.24377121031284332 valid 0.26868374938162687
LOSS train 0.24377121031284332 valid 0.268676207849273
LOSS train 0.24377121031284332 valid 0.2679030117365198
LOSS train 0.24377121031284332 valid 0.26769359179518437
LOSS train 0.24377121031284332 valid 0.2677630528404906
LOSS train 0.24377121031284332 valid 0.2677736595006926
LOSS train 0.24377121031284332 valid 0.26748761402822174
LOSS train 0.24377121031284332 valid 0.267398853051035
LOSS train 0.24377121031284332 valid 0.2679070094357366
LOSS train 0.24377121031284332 valid 0.26761437849751835
LOSS train 0.24377121031284332 valid 0.2680961647604266
LOSS train 0.24377121031284332 valid 0.2684509640527984
LOSS train 0.24377121031284332 valid 0.2680561488666454
LOSS train 0.24377121031284332 valid 0.2678893523911635
LOSS train 0.24377121031284332 valid 0.2678269928644511
LOSS train 0.24377121031284332 valid 0.2680483243015946
LOSS train 0.24377121031284332 valid 0.268049322492708
LOSS train 0.24377121031284332 valid 0.26800355988164104
LOSS train 0.24377121031284332 valid 0.2680572135448456
LOSS train 0.24377121031284332 valid 0.2683156232039134
LOSS train 0.24377121031284332 valid 0.2682862490650237
LOSS train 0.24377121031284332 valid 0.2681754359509796
LOSS train 0.24377121031284332 valid 0.26768831481305205
LOSS train 0.24377121031284332 valid 0.2673685061243864
LOSS train 0.24377121031284332 valid 0.26736564479256403
LOSS train 0.24377121031284332 valid 0.26733285696669057
LOSS train 0.24377121031284332 valid 0.26726019729797107
LOSS train 0.24377121031284332 valid 0.2674206554222463
LOSS train 0.24377121031284332 valid 0.2675734335625613
LOSS train 0.24377121031284332 valid 0.2678804742720197
LOSS train 0.24377121031284332 valid 0.26795699855271915
LOSS train 0.24377121031284332 valid 0.26791737271823746
LOSS train 0.24377121031284332 valid 0.2678376255061129
LOSS train 0.24377121031284332 valid 0.2681171577955995
LOSS train 0.24377121031284332 valid 0.26831266231147954
LOSS train 0.24377121031284332 valid 0.26856949117401957
LOSS train 0.24377121031284332 valid 0.26863789464627114
LOSS train 0.24377121031284332 valid 0.26872243142376345
LOSS train 0.24377121031284332 valid 0.268519160048715
LOSS train 0.24377121031284332 valid 0.26838311022275113
LOSS train 0.24377121031284332 valid 0.2683260655727516
LOSS train 0.24377121031284332 valid 0.26792428632443016
LOSS train 0.24377121031284332 valid 0.2680948899696337
LOSS train 0.24377121031284332 valid 0.2679163709282875
LOSS train 0.24377121031284332 valid 0.2676725985593354
LOSS train 0.24377121031284332 valid 0.26764205607928726
LOSS train 0.24377121031284332 valid 0.2673163817209356
LOSS train 0.24377121031284332 valid 0.26728274063630536
LOSS train 0.24377121031284332 valid 0.2673383264772354
LOSS train 0.24377121031284332 valid 0.2676475132123018
LOSS train 0.24377121031284332 valid 0.2675340695745626
LOSS train 0.24377121031284332 valid 0.2674397628141355
LOSS train 0.24377121031284332 valid 0.26773119928701866
LOSS train 0.24377121031284332 valid 0.2678382685407996
LOSS train 0.24377121031284332 valid 0.2679669220255028
LOSS train 0.24377121031284332 valid 0.26803727687141043
LOSS train 0.24377121031284332 valid 0.26771836860413933
LOSS train 0.24377121031284332 valid 0.26778241873877806
LOSS train 0.24377121031284332 valid 0.267998053539883
LOSS train 0.24377121031284332 valid 0.267973981259099
LOSS train 0.24377121031284332 valid 0.2681094512432635
LOSS train 0.24377121031284332 valid 0.26783554167264984
LOSS train 0.24377121031284332 valid 0.267589043054355
LOSS train 0.24377121031284332 valid 0.26767357622875887
LOSS train 0.24377121031284332 valid 0.26753707874936666
LOSS train 0.24377121031284332 valid 0.26749286687997886
LOSS train 0.24377121031284332 valid 0.2674786285685666
LOSS train 0.24377121031284332 valid 0.26755024038854686
LOSS train 0.24377121031284332 valid 0.26739561532224926
LOSS train 0.24377121031284332 valid 0.267382059161636
LOSS train 0.24377121031284332 valid 0.2674860384336299
LOSS train 0.24377121031284332 valid 0.2675427750087856
LOSS train 0.24377121031284332 valid 0.26768345781211744
LOSS train 0.24377121031284332 valid 0.2678073393801848
LOSS train 0.24377121031284332 valid 0.2676520658790736
LOSS train 0.24377121031284332 valid 0.2676538191326372
LOSS train 0.24377121031284332 valid 0.26767862641094814
LOSS train 0.24377121031284332 valid 0.26754234439652896
LOSS train 0.24377121031284332 valid 0.2676490442172901
LOSS train 0.24377121031284332 valid 0.26777418454488117
LOSS train 0.24377121031284332 valid 0.26759285340334643
LOSS train 0.24377121031284332 valid 0.2675942686644006
LOSS train 0.24377121031284332 valid 0.26751251999663295
LOSS train 0.24377121031284332 valid 0.2675262113935069
LOSS train 0.24377121031284332 valid 0.2677991463563829
LOSS train 0.24377121031284332 valid 0.2679045742067198
LOSS train 0.24377121031284332 valid 0.2677065811935484
LOSS train 0.24377121031284332 valid 0.26776965054654583
LOSS train 0.24377121031284332 valid 0.26764120383140366
LOSS train 0.24377121031284332 valid 0.26787944563797544
LOSS train 0.24377121031284332 valid 0.2677611736628005
LOSS train 0.24377121031284332 valid 0.2681604702815865
LOSS train 0.24377121031284332 valid 0.2681053113847522
LOSS train 0.24377121031284332 valid 0.26810401417315005
LOSS train 0.24377121031284332 valid 0.2681524773587042
LOSS train 0.24377121031284332 valid 0.26830728698780043
LOSS train 0.24377121031284332 valid 0.26848652878124724
LOSS train 0.24377121031284332 valid 0.2683849721124359
LOSS train 0.24377121031284332 valid 0.2682813723639744
LOSS train 0.24377121031284332 valid 0.26825243719283814
LOSS train 0.24377121031284332 valid 0.26815705778806104
LOSS train 0.24377121031284332 valid 0.2679431385432298
LOSS train 0.24377121031284332 valid 0.26796394857493316
LOSS train 0.24377121031284332 valid 0.2680683005423773
LOSS train 0.24377121031284332 valid 0.26836684882923323
LOSS train 0.24377121031284332 valid 0.2681993027481268
LOSS train 0.24377121031284332 valid 0.268328232454582
LOSS train 0.24377121031284332 valid 0.2682090529075293
LOSS train 0.24377121031284332 valid 0.26779494930145353
LOSS train 0.24377121031284332 valid 0.26753267073244963
LOSS train 0.24377121031284332 valid 0.26725800596349253
LOSS train 0.24377121031284332 valid 0.2672178631118678
LOSS train 0.24377121031284332 valid 0.26721173354751987
LOSS train 0.24377121031284332 valid 0.2671280375258489
LOSS train 0.24377121031284332 valid 0.26697497750838955
LOSS train 0.24377121031284332 valid 0.26714173321788376
LOSS train 0.24377121031284332 valid 0.26709797096359356
LOSS train 0.24377121031284332 valid 0.2670511725757803
LOSS train 0.24377121031284332 valid 0.2670193933116065
LOSS train 0.24377121031284332 valid 0.2668459569062807
LOSS train 0.24377121031284332 valid 0.2667874492343827
LOSS train 0.24377121031284332 valid 0.2666561701580098
LOSS train 0.24377121031284332 valid 0.2664834739859968
LOSS train 0.24377121031284332 valid 0.2664391060238299
LOSS train 0.24377121031284332 valid 0.26644411812096963
LOSS train 0.24377121031284332 valid 0.2664607284140998
LOSS train 0.24377121031284332 valid 0.2666340952523277
LOSS train 0.24377121031284332 valid 0.2664226429202618
LOSS train 0.24377121031284332 valid 0.26646203075317626
LOSS train 0.24377121031284332 valid 0.2662068840305684
LOSS train 0.24377121031284332 valid 0.26625543782479655
LOSS train 0.24377121031284332 valid 0.26640149099486216
LOSS train 0.24377121031284332 valid 0.26628538741726254
LOSS train 0.24377121031284332 valid 0.26590002917995054
LOSS train 0.24377121031284332 valid 0.26578863818368476
LOSS train 0.24377121031284332 valid 0.2658455035041186
LOSS train 0.24377121031284332 valid 0.26578094371797617
LOSS train 0.24377121031284332 valid 0.26569764603112567
LOSS train 0.24377121031284332 valid 0.2657915680384149
LOSS train 0.24377121031284332 valid 0.26589510226395074
LOSS train 0.24377121031284332 valid 0.26592515119415544
LOSS train 0.24377121031284332 valid 0.26605157199646196
LOSS train 0.24377121031284332 valid 0.26584506603369273
LOSS train 0.24377121031284332 valid 0.26611438435316087
LOSS train 0.24377121031284332 valid 0.2664102063592212
LOSS train 0.24377121031284332 valid 0.2665250420215584
LOSS train 0.24377121031284332 valid 0.266453578125818
LOSS train 0.24377121031284332 valid 0.2665759387800074
LOSS train 0.24377121031284332 valid 0.26648654885151807
LOSS train 0.24377121031284332 valid 0.2663641188410111
LOSS train 0.24377121031284332 valid 0.2663673603697046
LOSS train 0.24377121031284332 valid 0.26639198996992997
LOSS train 0.24377121031284332 valid 0.26625316207473343
LOSS train 0.24377121031284332 valid 0.26607704007854827
LOSS train 0.24377121031284332 valid 0.26600533963619977
LOSS train 0.24377121031284332 valid 0.26624917802009873
LOSS train 0.24377121031284332 valid 0.26610217047961493
LOSS train 0.24377121031284332 valid 0.2661505391764821
LOSS train 0.24377121031284332 valid 0.2660353940050557
LOSS train 0.24377121031284332 valid 0.26601524981565045
LOSS train 0.24377121031284332 valid 0.2659000886067023
LOSS train 0.24377121031284332 valid 0.2660155110617182
LOSS train 0.24377121031284332 valid 0.266117464100118
LOSS train 0.24377121031284332 valid 0.2660368694199456
LOSS train 0.24377121031284332 valid 0.26596290499961683
LOSS train 0.24377121031284332 valid 0.26580322133925033
LOSS train 0.24377121031284332 valid 0.26574673511824765
LOSS train 0.24377121031284332 valid 0.26572873497748895
LOSS train 0.24377121031284332 valid 0.26591794593767687
LOSS train 0.24377121031284332 valid 0.2658632375515889
LOSS train 0.24377121031284332 valid 0.26580118589668067
LOSS train 0.24377121031284332 valid 0.26593018762713716
LOSS train 0.24377121031284332 valid 0.2659417607130543
LOSS train 0.24377121031284332 valid 0.26577729907419
LOSS train 0.24377121031284332 valid 0.26566334286194254
LOSS train 0.24377121031284332 valid 0.26555972382531945
LOSS train 0.24377121031284332 valid 0.26565744514600126
LOSS train 0.24377121031284332 valid 0.26546683388066966
LOSS train 0.24377121031284332 valid 0.26552155525015114
LOSS train 0.24377121031284332 valid 0.2655653191300539
LOSS train 0.24377121031284332 valid 0.2656531562892402
LOSS train 0.24377121031284332 valid 0.26560735314463574
LOSS train 0.24377121031284332 valid 0.26559543583838585
LOSS train 0.24377121031284332 valid 0.2654992387212556
LOSS train 0.24377121031284332 valid 0.26551961745183494
LOSS train 0.24377121031284332 valid 0.2655000118156002
LOSS train 0.24377121031284332 valid 0.26544975583463803
LOSS train 0.24377121031284332 valid 0.26532064535382655
LOSS train 0.24377121031284332 valid 0.2653163076962455
LOSS train 0.24377121031284332 valid 0.2652749351250964
LOSS train 0.24377121031284332 valid 0.265323226851245
LOSS train 0.24377121031284332 valid 0.2654150765914245
LOSS train 0.24377121031284332 valid 0.26532225031717166
LOSS train 0.24377121031284332 valid 0.26527957047025363
LOSS train 0.24377121031284332 valid 0.26521644075447537
LOSS train 0.24377121031284332 valid 0.26530498996475677
LOSS train 0.24377121031284332 valid 0.265216926487759
LOSS train 0.24377121031284332 valid 0.26523573642694637
LOSS train 0.24377121031284332 valid 0.26510947292945425
LOSS train 0.24377121031284332 valid 0.26536080314248217
LOSS train 0.24377121031284332 valid 0.26527965971623646
LOSS train 0.24377121031284332 valid 0.26545905370216866
LOSS train 0.24377121031284332 valid 0.26548131181584206
LOSS train 0.24377121031284332 valid 0.265501007629979
LOSS train 0.24377121031284332 valid 0.2655767195669401
LOSS train 0.24377121031284332 valid 0.2655756975977849
LOSS train 0.24377121031284332 valid 0.2656823208156866
LOSS train 0.24377121031284332 valid 0.26559974253177643
LOSS train 0.24377121031284332 valid 0.2656460822574676
LOSS train 0.24377121031284332 valid 0.2656457758025278
LOSS train 0.24377121031284332 valid 0.265628390037675
LOSS train 0.24377121031284332 valid 0.2656886303012476
LOSS train 0.24377121031284332 valid 0.2656568854953802
LOSS train 0.24377121031284332 valid 0.26554041570052506
LOSS train 0.24377121031284332 valid 0.2655300623345598
LOSS train 0.24377121031284332 valid 0.26549353075693854
LOSS train 0.24377121031284332 valid 0.2653542631550839
LOSS train 0.24377121031284332 valid 0.2653213183820983
LOSS train 0.24377121031284332 valid 0.2651232334283682
LOSS train 0.24377121031284332 valid 0.26527310030226325
LOSS train 0.24377121031284332 valid 0.2652597850251271
LOSS train 0.24377121031284332 valid 0.2653449731992512
LOSS train 0.24377121031284332 valid 0.2653052809571785
LOSS train 0.24377121031284332 valid 0.2653393628019275
LOSS train 0.24377121031284332 valid 0.26531204140078263
LOSS train 0.24377121031284332 valid 0.2652147919927017
LOSS train 0.24377121031284332 valid 0.265296349162096
LOSS train 0.24377121031284332 valid 0.2652607492790251
LOSS train 0.24377121031284332 valid 0.26527139006265954
LOSS train 0.24377121031284332 valid 0.265206630873893
LOSS train 0.24377121031284332 valid 0.2654274882920064
LOSS train 0.24377121031284332 valid 0.2653281048557462
LOSS train 0.24377121031284332 valid 0.265283218878912
LOSS train 0.24377121031284332 valid 0.26542522898491694
LOSS train 0.24377121031284332 valid 0.2653948506476942
LOSS train 0.24377121031284332 valid 0.2653223554990445
LOSS train 0.24377121031284332 valid 0.26528663155636356
LOSS train 0.24377121031284332 valid 0.2653325548352197
LOSS train 0.24377121031284332 valid 0.2654340792393339
LOSS train 0.24377121031284332 valid 0.2653574770413382
LOSS train 0.24377121031284332 valid 0.26527723189046126
LOSS train 0.24377121031284332 valid 0.26529806158665953
LOSS train 0.24377121031284332 valid 0.26520750409040206
LOSS train 0.24377121031284332 valid 0.2652391306417329
LOSS train 0.24377121031284332 valid 0.2653561519336836
LOSS train 0.24377121031284332 valid 0.26547268308191135
LOSS train 0.24377121031284332 valid 0.26563655295892075
LOSS train 0.24377121031284332 valid 0.26558766059451183
LOSS train 0.24377121031284332 valid 0.26548107094328166
LOSS train 0.24377121031284332 valid 0.2654527989582399
LOSS train 0.24377121031284332 valid 0.2655037066766194
LOSS train 0.24377121031284332 valid 0.2655151581714273
LOSS train 0.24377121031284332 valid 0.26547728080437377
LOSS train 0.24377121031284332 valid 0.26563332556850383
LOSS train 0.24377121031284332 valid 0.2656091997745625
LOSS train 0.24377121031284332 valid 0.2657361666553587
LOSS train 0.24377121031284332 valid 0.2657917547899173
LOSS train 0.24377121031284332 valid 0.26562682846254043
LOSS train 0.24377121031284332 valid 0.26574115316345265
LOSS train 0.24377121031284332 valid 0.2657713000484503
LOSS train 0.24377121031284332 valid 0.26574516357290645
LOSS train 0.24377121031284332 valid 0.2656265189871192
LOSS train 0.24377121031284332 valid 0.2656784620146118
EPOCH 15:
  batch 1 loss: 0.2685302495956421
  batch 2 loss: 0.2647291123867035
  batch 3 loss: 0.2537878801425298
  batch 4 loss: 0.25729598477482796
  batch 5 loss: 0.2716327875852585
  batch 6 loss: 0.26563744246959686
  batch 7 loss: 0.2670384390013559
  batch 8 loss: 0.2635212354362011
  batch 9 loss: 0.26591675480206806
  batch 10 loss: 0.2625395506620407
  batch 11 loss: 0.2638471939347007
  batch 12 loss: 0.2615058322747548
  batch 13 loss: 0.25983980068793666
  batch 14 loss: 0.25954880884715487
  batch 15 loss: 0.26077314019203185
  batch 16 loss: 0.25994104985147715
  batch 17 loss: 0.26103323084466595
  batch 18 loss: 0.26082868460151887
  batch 19 loss: 0.2604202212471711
  batch 20 loss: 0.25705229938030244
  batch 21 loss: 0.25840450326601666
  batch 22 loss: 0.2584296478466554
  batch 23 loss: 0.25654403411823773
  batch 24 loss: 0.2562221897145112
  batch 25 loss: 0.25553187608718875
  batch 26 loss: 0.255745788033192
  batch 27 loss: 0.2542796664767795
  batch 28 loss: 0.254611716738769
  batch 29 loss: 0.2533553905528167
  batch 30 loss: 0.25310379366079966
  batch 31 loss: 0.25320168272141486
  batch 32 loss: 0.2535144882276654
  batch 33 loss: 0.2546312718680411
  batch 34 loss: 0.25589860449819
  batch 35 loss: 0.25675501227378844
  batch 36 loss: 0.256471178183953
  batch 37 loss: 0.2565265712705818
  batch 38 loss: 0.25832137739972066
  batch 39 loss: 0.2591260064106721
  batch 40 loss: 0.25862530060112476
  batch 41 loss: 0.2592499048971548
  batch 42 loss: 0.25932120106049944
  batch 43 loss: 0.25963978677294974
  batch 44 loss: 0.25962796637957747
  batch 45 loss: 0.2590210838450326
  batch 46 loss: 0.25780113138582395
  batch 47 loss: 0.2580864426303417
  batch 48 loss: 0.25749371728549403
  batch 49 loss: 0.2579394709699008
  batch 50 loss: 0.2574529930949211
  batch 51 loss: 0.25737852939203676
  batch 52 loss: 0.2568744641657059
  batch 53 loss: 0.2557684050978355
  batch 54 loss: 0.25534290654791725
  batch 55 loss: 0.2546347699382088
  batch 56 loss: 0.25440855058176176
  batch 57 loss: 0.2536197994884692
  batch 58 loss: 0.252679250877479
  batch 59 loss: 0.2527474806470386
  batch 60 loss: 0.2535654271642367
  batch 61 loss: 0.2536297797179613
  batch 62 loss: 0.2538172842994813
  batch 63 loss: 0.2535947866383053
  batch 64 loss: 0.2533871359191835
  batch 65 loss: 0.2532127857208252
  batch 66 loss: 0.2529773260607864
  batch 67 loss: 0.2527240661542807
  batch 68 loss: 0.25273255872375827
  batch 69 loss: 0.2527495341888372
  batch 70 loss: 0.2520195978028434
  batch 71 loss: 0.2518445595469273
  batch 72 loss: 0.25183389853272176
  batch 73 loss: 0.25136556119134984
  batch 74 loss: 0.25096353405230754
  batch 75 loss: 0.25093328456083935
  batch 76 loss: 0.25089030948124436
  batch 77 loss: 0.25077677314931696
  batch 78 loss: 0.2507257643036353
  batch 79 loss: 0.2503570310677154
  batch 80 loss: 0.2498184410855174
  batch 81 loss: 0.24949563359036858
  batch 82 loss: 0.24971977421423283
  batch 83 loss: 0.24930601001503955
  batch 84 loss: 0.248671493714764
  batch 85 loss: 0.24866914433591505
  batch 86 loss: 0.2486855065406755
  batch 87 loss: 0.2480550726939892
  batch 88 loss: 0.24826267395507207
  batch 89 loss: 0.2479928805586997
  batch 90 loss: 0.24826034042570327
  batch 91 loss: 0.24792076991154596
  batch 92 loss: 0.24788353592157364
  batch 93 loss: 0.24782594974322986
  batch 94 loss: 0.24786319979961882
  batch 95 loss: 0.24819523660760176
  batch 96 loss: 0.24851739003012577
  batch 97 loss: 0.24885335779681647
  batch 98 loss: 0.24882216417059608
  batch 99 loss: 0.24873342583275804
  batch 100 loss: 0.248759136646986
  batch 101 loss: 0.24866388826677116
  batch 102 loss: 0.2488497941225183
  batch 103 loss: 0.24870243711957654
  batch 104 loss: 0.24887564678031665
  batch 105 loss: 0.24882547500587646
  batch 106 loss: 0.2487858994951788
  batch 107 loss: 0.2484937633309409
  batch 108 loss: 0.24818720916906992
  batch 109 loss: 0.2482400470917378
  batch 110 loss: 0.24802193384278903
  batch 111 loss: 0.24791703191963402
  batch 112 loss: 0.24783919658511877
  batch 113 loss: 0.2472811471835702
  batch 114 loss: 0.2471149714108099
  batch 115 loss: 0.24635226894979892
  batch 116 loss: 0.2460736038869825
  batch 117 loss: 0.24576348015385815
  batch 118 loss: 0.2458441959599317
  batch 119 loss: 0.24589524424376608
  batch 120 loss: 0.24540551304817199
  batch 121 loss: 0.24536824066284274
  batch 122 loss: 0.24506528504559252
  batch 123 loss: 0.2449345893976165
  batch 124 loss: 0.24471584983891057
  batch 125 loss: 0.2444184901714325
  batch 126 loss: 0.24470704959498513
  batch 127 loss: 0.2443372677630327
  batch 128 loss: 0.2440958444494754
  batch 129 loss: 0.24407141753869463
  batch 130 loss: 0.24393247916148258
  batch 131 loss: 0.24399745919322238
  batch 132 loss: 0.24391913481734015
  batch 133 loss: 0.24366582213039684
  batch 134 loss: 0.24334776579444087
  batch 135 loss: 0.2432452580443135
  batch 136 loss: 0.24345994926989079
  batch 137 loss: 0.24354034738384023
  batch 138 loss: 0.2435928409298261
  batch 139 loss: 0.24364598481346378
  batch 140 loss: 0.24378360243780273
  batch 141 loss: 0.24349968071947706
  batch 142 loss: 0.24342951692745718
  batch 143 loss: 0.24351960493551267
  batch 144 loss: 0.2434013325514065
  batch 145 loss: 0.24335912907945698
  batch 146 loss: 0.2433603700913795
  batch 147 loss: 0.2433168303196122
  batch 148 loss: 0.2433461452255378
  batch 149 loss: 0.2428917917829232
  batch 150 loss: 0.24268423209587733
  batch 151 loss: 0.24227630115107984
  batch 152 loss: 0.24206361831410936
  batch 153 loss: 0.24212217632851568
  batch 154 loss: 0.2417835013626458
  batch 155 loss: 0.24179197588274556
  batch 156 loss: 0.2417207020215499
  batch 157 loss: 0.24154079453960345
  batch 158 loss: 0.24164051609703258
  batch 159 loss: 0.2417059025299624
  batch 160 loss: 0.24146224753931164
  batch 161 loss: 0.2412033412397278
  batch 162 loss: 0.24097783210468882
  batch 163 loss: 0.24082147426034775
  batch 164 loss: 0.24098509413803496
  batch 165 loss: 0.24108572828047203
  batch 166 loss: 0.24110610450965814
  batch 167 loss: 0.2410585458228688
  batch 168 loss: 0.24079263458649316
  batch 169 loss: 0.2408288017179839
  batch 170 loss: 0.2408722128938226
  batch 171 loss: 0.24100763633934377
  batch 172 loss: 0.24102043862952743
  batch 173 loss: 0.24108663511414058
  batch 174 loss: 0.24110788792029195
  batch 175 loss: 0.24105760540281024
  batch 176 loss: 0.24122158353301612
  batch 177 loss: 0.24148217685478554
  batch 178 loss: 0.24118302662051125
  batch 179 loss: 0.2414266651902119
  batch 180 loss: 0.24133767179316945
  batch 181 loss: 0.2411730866570499
  batch 182 loss: 0.24117128870316915
  batch 183 loss: 0.24095938692653113
  batch 184 loss: 0.24077417887747288
  batch 185 loss: 0.24050577274850896
  batch 186 loss: 0.2404138493922449
  batch 187 loss: 0.2408615742775208
  batch 188 loss: 0.24064474996734173
  batch 189 loss: 0.2407263837478779
  batch 190 loss: 0.24080926477909087
  batch 191 loss: 0.24109261135780374
  batch 192 loss: 0.24104078680587313
  batch 193 loss: 0.24106461550905298
  batch 194 loss: 0.24108901704402314
  batch 195 loss: 0.2413519167747253
  batch 196 loss: 0.24129355691221296
  batch 197 loss: 0.24142401858332194
  batch 198 loss: 0.24149200267562962
  batch 199 loss: 0.24163329818440443
  batch 200 loss: 0.24169688381254673
  batch 201 loss: 0.24167051570332465
  batch 202 loss: 0.24159232445872655
  batch 203 loss: 0.24160162253039225
  batch 204 loss: 0.241580784539966
  batch 205 loss: 0.2414102373326697
  batch 206 loss: 0.24139443365404906
  batch 207 loss: 0.24142712995337978
  batch 208 loss: 0.24131843684097895
  batch 209 loss: 0.24112442299794923
  batch 210 loss: 0.2412427701410793
  batch 211 loss: 0.24122979121185592
  batch 212 loss: 0.24122733770395224
  batch 213 loss: 0.24112485202265457
  batch 214 loss: 0.24111875494785398
  batch 215 loss: 0.24110698464304903
  batch 216 loss: 0.24117665420527812
  batch 217 loss: 0.24100098824171426
  batch 218 loss: 0.24081806045606596
  batch 219 loss: 0.2405234420816648
  batch 220 loss: 0.24070701307871126
  batch 221 loss: 0.24058281235835133
  batch 222 loss: 0.24055694882665668
  batch 223 loss: 0.2405403562458107
  batch 224 loss: 0.24053772778383323
  batch 225 loss: 0.24061994989713034
  batch 226 loss: 0.24072877377007915
  batch 227 loss: 0.24063545924976534
  batch 228 loss: 0.24065753327388512
  batch 229 loss: 0.24072244366443835
  batch 230 loss: 0.24074900908314664
  batch 231 loss: 0.24080638187530237
  batch 232 loss: 0.2406375277787447
  batch 233 loss: 0.2405825827126851
  batch 234 loss: 0.2406800886632031
  batch 235 loss: 0.24063589940679833
  batch 236 loss: 0.24065643915180432
  batch 237 loss: 0.24071521170531648
  batch 238 loss: 0.24081776422612808
  batch 239 loss: 0.24093887920658957
  batch 240 loss: 0.24081166386604308
  batch 241 loss: 0.24085499439002073
  batch 242 loss: 0.241028930283775
  batch 243 loss: 0.2409478069700822
  batch 244 loss: 0.240864097522419
  batch 245 loss: 0.24107069865781433
  batch 246 loss: 0.24120983778218913
  batch 247 loss: 0.24134883483653127
  batch 248 loss: 0.24147337572949548
  batch 249 loss: 0.24160659223435874
  batch 250 loss: 0.24182603031396865
  batch 251 loss: 0.24177475160811526
  batch 252 loss: 0.2416702744861444
  batch 253 loss: 0.24176666876779715
  batch 254 loss: 0.24197209990165364
  batch 255 loss: 0.24193914830684662
  batch 256 loss: 0.2419338314794004
  batch 257 loss: 0.24190138628278726
  batch 258 loss: 0.24183472927457603
  batch 259 loss: 0.2418269367751928
  batch 260 loss: 0.24183803756649677
  batch 261 loss: 0.24201277326578383
  batch 262 loss: 0.2419833460154424
  batch 263 loss: 0.24197809256528266
  batch 264 loss: 0.2423105217290647
  batch 265 loss: 0.24241126112218173
  batch 266 loss: 0.24238242836374985
  batch 267 loss: 0.24228071535794476
  batch 268 loss: 0.24226772434898278
  batch 269 loss: 0.24236935271871135
  batch 270 loss: 0.24237695573656648
  batch 271 loss: 0.24246611899775333
  batch 272 loss: 0.24243269126643152
  batch 273 loss: 0.24240532850389515
  batch 274 loss: 0.24235546675911784
  batch 275 loss: 0.2423829614574259
  batch 276 loss: 0.2423054281676161
  batch 277 loss: 0.2423772201318603
  batch 278 loss: 0.24233031144245065
  batch 279 loss: 0.24235806809104044
  batch 280 loss: 0.2421505056321621
  batch 281 loss: 0.24214563367629816
  batch 282 loss: 0.24216078121400048
  batch 283 loss: 0.24211811024182256
  batch 284 loss: 0.24203948697573702
  batch 285 loss: 0.2419120496825168
  batch 286 loss: 0.2420164482160048
  batch 287 loss: 0.24204594063011195
  batch 288 loss: 0.24202324946721396
  batch 289 loss: 0.24190330855986653
  batch 290 loss: 0.2418473588495419
  batch 291 loss: 0.2418984102526891
  batch 292 loss: 0.24179562079171613
  batch 293 loss: 0.24173142600791853
  batch 294 loss: 0.24160931735825378
  batch 295 loss: 0.24156663392559957
  batch 296 loss: 0.24159887266924251
  batch 297 loss: 0.24155607950005065
  batch 298 loss: 0.241565946264555
  batch 299 loss: 0.24159173223884609
  batch 300 loss: 0.24157494808236757
  batch 301 loss: 0.24153784556048258
  batch 302 loss: 0.24163900478588823
  batch 303 loss: 0.24172212793291992
  batch 304 loss: 0.24184178124721112
  batch 305 loss: 0.24179894938820698
  batch 306 loss: 0.24198138582355835
  batch 307 loss: 0.24216896455722833
  batch 308 loss: 0.24240857493374254
  batch 309 loss: 0.24237446785937622
  batch 310 loss: 0.24252908186566446
  batch 311 loss: 0.2424941204272666
  batch 312 loss: 0.24267753342596385
  batch 313 loss: 0.2427954858960435
  batch 314 loss: 0.2428522915312439
  batch 315 loss: 0.24283298779101598
  batch 316 loss: 0.24281596283935294
  batch 317 loss: 0.24301047049674326
  batch 318 loss: 0.24304841960188728
  batch 319 loss: 0.24294296752509653
  batch 320 loss: 0.24308298924006522
  batch 321 loss: 0.24308169018071016
  batch 322 loss: 0.24299197236758582
  batch 323 loss: 0.24295436118040292
  batch 324 loss: 0.24295752780672944
  batch 325 loss: 0.24300142544966477
  batch 326 loss: 0.243078542267618
  batch 327 loss: 0.24301707102071254
  batch 328 loss: 0.2430380127142842
  batch 329 loss: 0.24308291652825828
  batch 330 loss: 0.24315367742921368
  batch 331 loss: 0.24313045452908807
  batch 332 loss: 0.24325370371162172
  batch 333 loss: 0.24323585061160652
  batch 334 loss: 0.2434439009981241
  batch 335 loss: 0.2434574526192537
  batch 336 loss: 0.24345138402921812
  batch 337 loss: 0.24343450358607294
  batch 338 loss: 0.24343972970571745
  batch 339 loss: 0.2433688030921604
  batch 340 loss: 0.2433787849457825
  batch 341 loss: 0.24337688704564775
  batch 342 loss: 0.2433714284534343
  batch 343 loss: 0.2434209074292864
  batch 344 loss: 0.24340619519352913
  batch 345 loss: 0.24337285882320958
  batch 346 loss: 0.24335261326649285
  batch 347 loss: 0.2432110450882733
  batch 348 loss: 0.24312418369554925
  batch 349 loss: 0.2432301459049427
  batch 350 loss: 0.2432458313873836
  batch 351 loss: 0.2432026835927936
  batch 352 loss: 0.24311702872033825
  batch 353 loss: 0.24308599582306048
  batch 354 loss: 0.24318794127406373
  batch 355 loss: 0.24313915392882388
  batch 356 loss: 0.2431149519561382
  batch 357 loss: 0.2430517481488674
  batch 358 loss: 0.2428995271385049
  batch 359 loss: 0.2428846444343126
  batch 360 loss: 0.2427708123707109
  batch 361 loss: 0.24269032920001285
  batch 362 loss: 0.24261028846489133
  batch 363 loss: 0.24262107325815302
  batch 364 loss: 0.24251989639558635
  batch 365 loss: 0.24256097188551132
  batch 366 loss: 0.24248779825662656
  batch 367 loss: 0.24257402454962199
  batch 368 loss: 0.24239770306841188
  batch 369 loss: 0.24237614058382143
  batch 370 loss: 0.24252686995912243
  batch 371 loss: 0.2425234326615809
  batch 372 loss: 0.2425344960503681
  batch 373 loss: 0.2425647769472874
  batch 374 loss: 0.2425703718302084
  batch 375 loss: 0.24259571850299835
  batch 376 loss: 0.24276151329754514
  batch 377 loss: 0.24272336915570167
  batch 378 loss: 0.2426356622781703
  batch 379 loss: 0.2426050387932317
  batch 380 loss: 0.24265296074904893
  batch 381 loss: 0.24260222970500706
  batch 382 loss: 0.24254282049015555
  batch 383 loss: 0.24239350828115996
  batch 384 loss: 0.24241080701661608
  batch 385 loss: 0.24238113977692344
  batch 386 loss: 0.24233991648866723
  batch 387 loss: 0.2423358253884377
  batch 388 loss: 0.24237412774992972
  batch 389 loss: 0.24232581522133786
  batch 390 loss: 0.24227999120186536
  batch 391 loss: 0.24211884032734823
  batch 392 loss: 0.2422012187996689
  batch 393 loss: 0.24216458709488692
  batch 394 loss: 0.24216411516146005
  batch 395 loss: 0.24212451841257795
  batch 396 loss: 0.24221672712251394
  batch 397 loss: 0.24222998144644634
  batch 398 loss: 0.24216995257229063
  batch 399 loss: 0.24223827150531282
  batch 400 loss: 0.2423604741692543
  batch 401 loss: 0.24239546610530177
  batch 402 loss: 0.24236495655715762
  batch 403 loss: 0.24228391935008928
  batch 404 loss: 0.24239209467674247
  batch 405 loss: 0.24244099137959657
  batch 406 loss: 0.24241932866902188
  batch 407 loss: 0.24246472361925486
  batch 408 loss: 0.24246733179133312
  batch 409 loss: 0.24266441975216993
  batch 410 loss: 0.24265099313200975
  batch 411 loss: 0.2426590366955221
  batch 412 loss: 0.24267466127583123
  batch 413 loss: 0.2427115674168954
  batch 414 loss: 0.24291072677874911
  batch 415 loss: 0.24286210673401154
  batch 416 loss: 0.24285075441002846
  batch 417 loss: 0.24274493103547634
  batch 418 loss: 0.24274913029094633
  batch 419 loss: 0.24279091761476385
  batch 420 loss: 0.24281463803989548
  batch 421 loss: 0.24277590831073617
  batch 422 loss: 0.24268023055311627
  batch 423 loss: 0.24269187101658354
  batch 424 loss: 0.24285018630325794
  batch 425 loss: 0.24271532581133
  batch 426 loss: 0.24260182517794937
  batch 427 loss: 0.24266288235818476
  batch 428 loss: 0.24266225956032209
  batch 429 loss: 0.24258227820024067
  batch 430 loss: 0.24258684061987457
  batch 431 loss: 0.2425388926488894
  batch 432 loss: 0.24256145660937936
  batch 433 loss: 0.24248939666544317
  batch 434 loss: 0.24245881035168598
  batch 435 loss: 0.24248539856795606
  batch 436 loss: 0.24252308368546152
  batch 437 loss: 0.242453581476648
  batch 438 loss: 0.2424357874344473
  batch 439 loss: 0.2423382961872227
  batch 440 loss: 0.24234392063861543
  batch 441 loss: 0.24239476504915156
  batch 442 loss: 0.24239277566585066
  batch 443 loss: 0.24228660748855255
  batch 444 loss: 0.24232184380158647
  batch 445 loss: 0.24220125979921792
  batch 446 loss: 0.2422102364657171
  batch 447 loss: 0.24218206194289846
  batch 448 loss: 0.2421883633692882
  batch 449 loss: 0.2421859641183989
  batch 450 loss: 0.24211263878477945
  batch 451 loss: 0.24206408088460993
  batch 452 loss: 0.24193704863433288
  batch 453 loss: 0.24187286586293058
  batch 454 loss: 0.24177955222287367
  batch 455 loss: 0.24185095599719456
  batch 456 loss: 0.2417702059212484
  batch 457 loss: 0.24181977644790967
  batch 458 loss: 0.24175913918226566
  batch 459 loss: 0.24171154886029644
  batch 460 loss: 0.24169782085911087
  batch 461 loss: 0.2417322557880149
  batch 462 loss: 0.24169379098590835
  batch 463 loss: 0.24171524055575708
  batch 464 loss: 0.2416062462304173
  batch 465 loss: 0.24159916732259976
  batch 466 loss: 0.24155723271579702
  batch 467 loss: 0.24161103881528598
  batch 468 loss: 0.2415612634175863
  batch 469 loss: 0.24145773974563012
  batch 470 loss: 0.24151830470308344
  batch 471 loss: 0.24152785979385336
  batch 472 loss: 0.24138444696821398
LOSS train 0.24138444696821398 valid 0.20875434577465057
LOSS train 0.24138444696821398 valid 0.20883974432945251
LOSS train 0.24138444696821398 valid 0.22655969858169556
LOSS train 0.24138444696821398 valid 0.2152622751891613
LOSS train 0.24138444696821398 valid 0.22410910427570344
LOSS train 0.24138444696821398 valid 0.22621610015630722
LOSS train 0.24138444696821398 valid 0.21769333950110845
LOSS train 0.24138444696821398 valid 0.22000818140804768
LOSS train 0.24138444696821398 valid 0.22006410857041678
LOSS train 0.24138444696821398 valid 0.2175174593925476
LOSS train 0.24138444696821398 valid 0.21710916676304556
LOSS train 0.24138444696821398 valid 0.2179342545568943
LOSS train 0.24138444696821398 valid 0.2202608848993595
LOSS train 0.24138444696821398 valid 0.21781562162297113
LOSS train 0.24138444696821398 valid 0.21590115229288737
LOSS train 0.24138444696821398 valid 0.21893174573779106
LOSS train 0.24138444696821398 valid 0.21949589515433593
LOSS train 0.24138444696821398 valid 0.21981085340181986
LOSS train 0.24138444696821398 valid 0.220767385081241
LOSS train 0.24138444696821398 valid 0.22012488916516304
LOSS train 0.24138444696821398 valid 0.22153455018997192
LOSS train 0.24138444696821398 valid 0.2193582742051645
LOSS train 0.24138444696821398 valid 0.21780726572741632
LOSS train 0.24138444696821398 valid 0.21751609817147255
LOSS train 0.24138444696821398 valid 0.21802511274814607
LOSS train 0.24138444696821398 valid 0.21671403142122123
LOSS train 0.24138444696821398 valid 0.21714346055631284
LOSS train 0.24138444696821398 valid 0.21762573985116823
LOSS train 0.24138444696821398 valid 0.21588061236102005
LOSS train 0.24138444696821398 valid 0.2139804442723592
LOSS train 0.24138444696821398 valid 0.21399749671259233
LOSS train 0.24138444696821398 valid 0.21475143544375896
LOSS train 0.24138444696821398 valid 0.21478068738272696
LOSS train 0.24138444696821398 valid 0.21442197570029428
LOSS train 0.24138444696821398 valid 0.21539571072374072
LOSS train 0.24138444696821398 valid 0.21632890527447066
LOSS train 0.24138444696821398 valid 0.21531718808251457
LOSS train 0.24138444696821398 valid 0.21521996706724167
LOSS train 0.24138444696821398 valid 0.21436521372733972
LOSS train 0.24138444696821398 valid 0.21493544541299342
LOSS train 0.24138444696821398 valid 0.214196297090228
LOSS train 0.24138444696821398 valid 0.21611884910435902
LOSS train 0.24138444696821398 valid 0.21657882838748221
LOSS train 0.24138444696821398 valid 0.21613159945065324
LOSS train 0.24138444696821398 valid 0.21608435544702742
LOSS train 0.24138444696821398 valid 0.21569880195285962
LOSS train 0.24138444696821398 valid 0.21544779868836098
LOSS train 0.24138444696821398 valid 0.21710826518634954
LOSS train 0.24138444696821398 valid 0.2167345340154609
LOSS train 0.24138444696821398 valid 0.21741884648799897
LOSS train 0.24138444696821398 valid 0.21698460099743863
LOSS train 0.24138444696821398 valid 0.21716382067937118
LOSS train 0.24138444696821398 valid 0.21811832909314138
LOSS train 0.24138444696821398 valid 0.2183445327811771
LOSS train 0.24138444696821398 valid 0.21823196627876976
LOSS train 0.24138444696821398 valid 0.21827072144619056
LOSS train 0.24138444696821398 valid 0.21752466731949857
LOSS train 0.24138444696821398 valid 0.21835266130751577
LOSS train 0.24138444696821398 valid 0.21831560362193544
LOSS train 0.24138444696821398 valid 0.21814800103505452
LOSS train 0.24138444696821398 valid 0.21858204583652685
LOSS train 0.24138444696821398 valid 0.21824028102620954
LOSS train 0.24138444696821398 valid 0.21804197274503254
LOSS train 0.24138444696821398 valid 0.2183340226765722
LOSS train 0.24138444696821398 valid 0.2170957326889038
LOSS train 0.24138444696821398 valid 0.2168611947334174
LOSS train 0.24138444696821398 valid 0.21778150992607004
LOSS train 0.24138444696821398 valid 0.21719811374650283
LOSS train 0.24138444696821398 valid 0.2178581922814466
LOSS train 0.24138444696821398 valid 0.2184731308903013
LOSS train 0.24138444696821398 valid 0.21888823450451167
LOSS train 0.24138444696821398 valid 0.219104229989979
LOSS train 0.24138444696821398 valid 0.21977340807653453
LOSS train 0.24138444696821398 valid 0.2199160593586999
LOSS train 0.24138444696821398 valid 0.21952655911445618
LOSS train 0.24138444696821398 valid 0.21976211137677493
LOSS train 0.24138444696821398 valid 0.21997409671932072
LOSS train 0.24138444696821398 valid 0.21972930030180857
LOSS train 0.24138444696821398 valid 0.21960585385183745
LOSS train 0.24138444696821398 valid 0.21903773173689842
LOSS train 0.24138444696821398 valid 0.21912222787921812
LOSS train 0.24138444696821398 valid 0.2187829811762019
LOSS train 0.24138444696821398 valid 0.21866943480738674
LOSS train 0.24138444696821398 valid 0.21864683908366023
LOSS train 0.24138444696821398 valid 0.21895468182423536
LOSS train 0.24138444696821398 valid 0.2186417271231496
LOSS train 0.24138444696821398 valid 0.2180582488747849
LOSS train 0.24138444696821398 valid 0.2177769739858129
LOSS train 0.24138444696821398 valid 0.21782718548613988
LOSS train 0.24138444696821398 valid 0.21790572471088834
LOSS train 0.24138444696821398 valid 0.21785704316673699
LOSS train 0.24138444696821398 valid 0.21770440965243007
LOSS train 0.24138444696821398 valid 0.21767020594048245
LOSS train 0.24138444696821398 valid 0.21776385098061662
LOSS train 0.24138444696821398 valid 0.21773645815096404
LOSS train 0.24138444696821398 valid 0.21826652996242046
LOSS train 0.24138444696821398 valid 0.21846463508212688
LOSS train 0.24138444696821398 valid 0.21851370407610524
LOSS train 0.24138444696821398 valid 0.2186106982255223
LOSS train 0.24138444696821398 valid 0.21864361226558685
LOSS train 0.24138444696821398 valid 0.21884176208831296
LOSS train 0.24138444696821398 valid 0.2193074753763629
LOSS train 0.24138444696821398 valid 0.2191249095988505
LOSS train 0.24138444696821398 valid 0.21888343946864972
LOSS train 0.24138444696821398 valid 0.21931296288967134
LOSS train 0.24138444696821398 valid 0.21952657837350414
LOSS train 0.24138444696821398 valid 0.21919255710650828
LOSS train 0.24138444696821398 valid 0.21914782637247332
LOSS train 0.24138444696821398 valid 0.21852507708816354
LOSS train 0.24138444696821398 valid 0.21831500476056878
LOSS train 0.24138444696821398 valid 0.21844026258399896
LOSS train 0.24138444696821398 valid 0.21848377465669597
LOSS train 0.24138444696821398 valid 0.21816954385917797
LOSS train 0.24138444696821398 valid 0.21803597321635798
LOSS train 0.24138444696821398 valid 0.21847503081611966
LOSS train 0.24138444696821398 valid 0.2181451955232127
LOSS train 0.24138444696821398 valid 0.21862195521338373
LOSS train 0.24138444696821398 valid 0.2190202062917968
LOSS train 0.24138444696821398 valid 0.21868961160423375
LOSS train 0.24138444696821398 valid 0.21849246906737488
LOSS train 0.24138444696821398 valid 0.2184201312705505
LOSS train 0.24138444696821398 valid 0.21869616247102863
LOSS train 0.24138444696821398 valid 0.21868747036631514
LOSS train 0.24138444696821398 valid 0.21856939335984568
LOSS train 0.24138444696821398 valid 0.2186739959716797
LOSS train 0.24138444696821398 valid 0.21891063546377515
LOSS train 0.24138444696821398 valid 0.2189616467070392
LOSS train 0.24138444696821398 valid 0.21890529664233327
LOSS train 0.24138444696821398 valid 0.21850844755653262
LOSS train 0.24138444696821398 valid 0.21823088641350086
LOSS train 0.24138444696821398 valid 0.2183028684772608
LOSS train 0.24138444696821398 valid 0.21823682139317194
LOSS train 0.24138444696821398 valid 0.21818655157895914
LOSS train 0.24138444696821398 valid 0.21829935185500046
LOSS train 0.24138444696821398 valid 0.2184332843180056
LOSS train 0.24138444696821398 valid 0.21868469960549297
LOSS train 0.24138444696821398 valid 0.21885334379481572
LOSS train 0.24138444696821398 valid 0.21879173426524454
LOSS train 0.24138444696821398 valid 0.2187476069163933
LOSS train 0.24138444696821398 valid 0.2189819767006806
LOSS train 0.24138444696821398 valid 0.21917708746507658
LOSS train 0.24138444696821398 valid 0.21938200882622894
LOSS train 0.24138444696821398 valid 0.21947915445674548
LOSS train 0.24138444696821398 valid 0.21956275703592432
LOSS train 0.24138444696821398 valid 0.21945451508308278
LOSS train 0.24138444696821398 valid 0.21937449847998686
LOSS train 0.24138444696821398 valid 0.21929066825886162
LOSS train 0.24138444696821398 valid 0.2190556185873779
LOSS train 0.24138444696821398 valid 0.21915558840604438
LOSS train 0.24138444696821398 valid 0.21901278992493947
LOSS train 0.24138444696821398 valid 0.218836561338791
LOSS train 0.24138444696821398 valid 0.21881404499474325
LOSS train 0.24138444696821398 valid 0.21850908503812902
LOSS train 0.24138444696821398 valid 0.2184790564434869
LOSS train 0.24138444696821398 valid 0.21851856208616688
LOSS train 0.24138444696821398 valid 0.21882530588370103
LOSS train 0.24138444696821398 valid 0.21874267906899664
LOSS train 0.24138444696821398 valid 0.21865148512245733
LOSS train 0.24138444696821398 valid 0.21892127647714796
LOSS train 0.24138444696821398 valid 0.21904654130339624
LOSS train 0.24138444696821398 valid 0.2192168618951525
LOSS train 0.24138444696821398 valid 0.21930312273311026
LOSS train 0.24138444696821398 valid 0.21899798082793417
LOSS train 0.24138444696821398 valid 0.21916638114830342
LOSS train 0.24138444696821398 valid 0.21928621277664648
LOSS train 0.24138444696821398 valid 0.21926211422107306
LOSS train 0.24138444696821398 valid 0.21938710244829784
LOSS train 0.24138444696821398 valid 0.21913622594660237
LOSS train 0.24138444696821398 valid 0.2188993784097525
LOSS train 0.24138444696821398 valid 0.21895595999325024
LOSS train 0.24138444696821398 valid 0.21880964682116147
LOSS train 0.24138444696821398 valid 0.2187998462901559
LOSS train 0.24138444696821398 valid 0.21879027068959495
LOSS train 0.24138444696821398 valid 0.21885718079819078
LOSS train 0.24138444696821398 valid 0.2187001313482012
LOSS train 0.24138444696821398 valid 0.21867290520193902
LOSS train 0.24138444696821398 valid 0.2187977141916415
LOSS train 0.24138444696821398 valid 0.2187911504774951
LOSS train 0.24138444696821398 valid 0.21892625759433768
LOSS train 0.24138444696821398 valid 0.21900031343102455
LOSS train 0.24138444696821398 valid 0.21886637329396622
LOSS train 0.24138444696821398 valid 0.21885483318960275
LOSS train 0.24138444696821398 valid 0.21884250502443053
LOSS train 0.24138444696821398 valid 0.21875807019355503
LOSS train 0.24138444696821398 valid 0.21882260320959865
LOSS train 0.24138444696821398 valid 0.21888498465220133
LOSS train 0.24138444696821398 valid 0.21872664366495162
LOSS train 0.24138444696821398 valid 0.2187528075214396
LOSS train 0.24138444696821398 valid 0.21871578243989792
LOSS train 0.24138444696821398 valid 0.21868135662455307
LOSS train 0.24138444696821398 valid 0.21890056382923226
LOSS train 0.24138444696821398 valid 0.21898333976666132
LOSS train 0.24138444696821398 valid 0.2188263712306097
LOSS train 0.24138444696821398 valid 0.218939169626875
LOSS train 0.24138444696821398 valid 0.21884160584364182
LOSS train 0.24138444696821398 valid 0.21909463702111828
LOSS train 0.24138444696821398 valid 0.21900286249400394
LOSS train 0.24138444696821398 valid 0.21935171649010496
LOSS train 0.24138444696821398 valid 0.2192691601251238
LOSS train 0.24138444696821398 valid 0.2192215471714735
LOSS train 0.24138444696821398 valid 0.21928162301950788
LOSS train 0.24138444696821398 valid 0.2194230523292381
LOSS train 0.24138444696821398 valid 0.21956523144480042
LOSS train 0.24138444696821398 valid 0.2194734586074072
LOSS train 0.24138444696821398 valid 0.21938223061038226
LOSS train 0.24138444696821398 valid 0.21935397769930293
LOSS train 0.24138444696821398 valid 0.2192699915688971
LOSS train 0.24138444696821398 valid 0.2190684755690969
LOSS train 0.24138444696821398 valid 0.21908536296712156
LOSS train 0.24138444696821398 valid 0.21913340886433919
LOSS train 0.24138444696821398 valid 0.2193835517523978
LOSS train 0.24138444696821398 valid 0.21922944654833595
LOSS train 0.24138444696821398 valid 0.2193599903667477
LOSS train 0.24138444696821398 valid 0.21927420934227027
LOSS train 0.24138444696821398 valid 0.21892455726168877
LOSS train 0.24138444696821398 valid 0.2186666322113187
LOSS train 0.24138444696821398 valid 0.21841844277722494
LOSS train 0.24138444696821398 valid 0.21839339067356303
LOSS train 0.24138444696821398 valid 0.2183373190088359
LOSS train 0.24138444696821398 valid 0.21820181892676788
LOSS train 0.24138444696821398 valid 0.2180853517077088
LOSS train 0.24138444696821398 valid 0.21826109639159194
LOSS train 0.24138444696821398 valid 0.21818643661357898
LOSS train 0.24138444696821398 valid 0.21810604818165302
LOSS train 0.24138444696821398 valid 0.21807812737094032
LOSS train 0.24138444696821398 valid 0.21790806289795225
LOSS train 0.24138444696821398 valid 0.21782890776180486
LOSS train 0.24138444696821398 valid 0.21771338314079403
LOSS train 0.24138444696821398 valid 0.2175732530230518
LOSS train 0.24138444696821398 valid 0.21752733296674232
LOSS train 0.24138444696821398 valid 0.21756966457222449
LOSS train 0.24138444696821398 valid 0.21761160350308337
LOSS train 0.24138444696821398 valid 0.2177383203235307
LOSS train 0.24138444696821398 valid 0.2175430041602534
LOSS train 0.24138444696821398 valid 0.21759017160598268
LOSS train 0.24138444696821398 valid 0.2173595871713202
LOSS train 0.24138444696821398 valid 0.21740054543511275
LOSS train 0.24138444696821398 valid 0.21755993040669866
LOSS train 0.24138444696821398 valid 0.21744974599722539
LOSS train 0.24138444696821398 valid 0.21711401734501123
LOSS train 0.24138444696821398 valid 0.216979829973205
LOSS train 0.24138444696821398 valid 0.21703003987300495
LOSS train 0.24138444696821398 valid 0.21700522009237314
LOSS train 0.24138444696821398 valid 0.21692518425769494
LOSS train 0.24138444696821398 valid 0.2169959073164025
LOSS train 0.24138444696821398 valid 0.21709734879858125
LOSS train 0.24138444696821398 valid 0.21714015968656733
LOSS train 0.24138444696821398 valid 0.21723619572097255
LOSS train 0.24138444696821398 valid 0.21703652606671114
LOSS train 0.24138444696821398 valid 0.21732368844747543
LOSS train 0.24138444696821398 valid 0.21762133742946077
LOSS train 0.24138444696821398 valid 0.21773366813385298
LOSS train 0.24138444696821398 valid 0.21765676323368616
LOSS train 0.24138444696821398 valid 0.21774374347502792
LOSS train 0.24138444696821398 valid 0.21764117064429264
LOSS train 0.24138444696821398 valid 0.21755350130843
LOSS train 0.24138444696821398 valid 0.2174936835056149
LOSS train 0.24138444696821398 valid 0.21753717779882195
LOSS train 0.24138444696821398 valid 0.217413796996518
LOSS train 0.24138444696821398 valid 0.21729453956851594
LOSS train 0.24138444696821398 valid 0.21721394114567402
LOSS train 0.24138444696821398 valid 0.21741238395676357
LOSS train 0.24138444696821398 valid 0.21730389392194638
LOSS train 0.24138444696821398 valid 0.21734719977460124
LOSS train 0.24138444696821398 valid 0.21725593681605357
LOSS train 0.24138444696821398 valid 0.21720444831184874
LOSS train 0.24138444696821398 valid 0.21710339830386058
LOSS train 0.24138444696821398 valid 0.2172322566273497
LOSS train 0.24138444696821398 valid 0.21735163193653065
LOSS train 0.24138444696821398 valid 0.21727570869304516
LOSS train 0.24138444696821398 valid 0.21722891250439674
LOSS train 0.24138444696821398 valid 0.21710018024725072
LOSS train 0.24138444696821398 valid 0.21705918243298164
LOSS train 0.24138444696821398 valid 0.21704326640732968
LOSS train 0.24138444696821398 valid 0.21723008302125063
LOSS train 0.24138444696821398 valid 0.21717631633298984
LOSS train 0.24138444696821398 valid 0.21711064880505365
LOSS train 0.24138444696821398 valid 0.2172336705940233
LOSS train 0.24138444696821398 valid 0.21726309777801608
LOSS train 0.24138444696821398 valid 0.2171164667499917
LOSS train 0.24138444696821398 valid 0.2169932603199711
LOSS train 0.24138444696821398 valid 0.21690227418926591
LOSS train 0.24138444696821398 valid 0.21698864420395436
LOSS train 0.24138444696821398 valid 0.21680459542803363
LOSS train 0.24138444696821398 valid 0.21682656449184082
LOSS train 0.24138444696821398 valid 0.21686272760788045
LOSS train 0.24138444696821398 valid 0.2169515060630825
LOSS train 0.24138444696821398 valid 0.21685929068674645
LOSS train 0.24138444696821398 valid 0.21684259150473717
LOSS train 0.24138444696821398 valid 0.21675786679161005
LOSS train 0.24138444696821398 valid 0.21675231453684188
LOSS train 0.24138444696821398 valid 0.21672236516255222
LOSS train 0.24138444696821398 valid 0.2167147687372494
LOSS train 0.24138444696821398 valid 0.21659060679122705
LOSS train 0.24138444696821398 valid 0.2165617002268969
LOSS train 0.24138444696821398 valid 0.21652229925667918
LOSS train 0.24138444696821398 valid 0.2165701694861807
LOSS train 0.24138444696821398 valid 0.216608986158499
LOSS train 0.24138444696821398 valid 0.21651781392735384
LOSS train 0.24138444696821398 valid 0.21646354258060455
LOSS train 0.24138444696821398 valid 0.2163956035609261
LOSS train 0.24138444696821398 valid 0.21644768750430732
LOSS train 0.24138444696821398 valid 0.21636433284668247
LOSS train 0.24138444696821398 valid 0.21635955591735087
LOSS train 0.24138444696821398 valid 0.2162262812989657
LOSS train 0.24138444696821398 valid 0.2164761626642514
LOSS train 0.24138444696821398 valid 0.21642084740078021
LOSS train 0.24138444696821398 valid 0.2166114126803813
LOSS train 0.24138444696821398 valid 0.21660733483370068
LOSS train 0.24138444696821398 valid 0.21661173850297927
LOSS train 0.24138444696821398 valid 0.21666975424795673
LOSS train 0.24138444696821398 valid 0.21666071057701722
LOSS train 0.24138444696821398 valid 0.21673530473495825
LOSS train 0.24138444696821398 valid 0.21665051683878442
LOSS train 0.24138444696821398 valid 0.21668210185709455
LOSS train 0.24138444696821398 valid 0.21669212417512002
LOSS train 0.24138444696821398 valid 0.2166514738877113
LOSS train 0.24138444696821398 valid 0.21664909549854086
LOSS train 0.24138444696821398 valid 0.21659334863428037
LOSS train 0.24138444696821398 valid 0.21650273338891565
LOSS train 0.24138444696821398 valid 0.21650397754346842
LOSS train 0.24138444696821398 valid 0.2164510609755605
LOSS train 0.24138444696821398 valid 0.216288071898484
LOSS train 0.24138444696821398 valid 0.21623062926494044
LOSS train 0.24138444696821398 valid 0.21606178315786215
LOSS train 0.24138444696821398 valid 0.2162119550116223
LOSS train 0.24138444696821398 valid 0.21619568327698138
LOSS train 0.24138444696821398 valid 0.21627674011013856
LOSS train 0.24138444696821398 valid 0.21622838818193568
LOSS train 0.24138444696821398 valid 0.21626924574375153
LOSS train 0.24138444696821398 valid 0.21623156825221196
LOSS train 0.24138444696821398 valid 0.2161440501460828
LOSS train 0.24138444696821398 valid 0.21623270902726743
LOSS train 0.24138444696821398 valid 0.2161967704396048
LOSS train 0.24138444696821398 valid 0.21616154410945834
LOSS train 0.24138444696821398 valid 0.21611251260730482
LOSS train 0.24138444696821398 valid 0.21630423045123012
LOSS train 0.24138444696821398 valid 0.21623454481010607
LOSS train 0.24138444696821398 valid 0.21621354249550528
LOSS train 0.24138444696821398 valid 0.21635349809246904
LOSS train 0.24138444696821398 valid 0.21633704155246533
LOSS train 0.24138444696821398 valid 0.21627497446467306
LOSS train 0.24138444696821398 valid 0.2162418629615717
LOSS train 0.24138444696821398 valid 0.21626478580887928
LOSS train 0.24138444696821398 valid 0.21637006877125173
LOSS train 0.24138444696821398 valid 0.21630246347257856
LOSS train 0.24138444696821398 valid 0.21623243683525054
LOSS train 0.24138444696821398 valid 0.21629240702109775
LOSS train 0.24138444696821398 valid 0.21623308743139394
LOSS train 0.24138444696821398 valid 0.2162840430225645
LOSS train 0.24138444696821398 valid 0.21636863086807762
LOSS train 0.24138444696821398 valid 0.21649482554163446
LOSS train 0.24138444696821398 valid 0.2166378767817959
LOSS train 0.24138444696821398 valid 0.21660421599270935
LOSS train 0.24138444696821398 valid 0.21651954348658173
LOSS train 0.24138444696821398 valid 0.21648819213000575
LOSS train 0.24138444696821398 valid 0.2165448269089397
LOSS train 0.24138444696821398 valid 0.21653483783066604
LOSS train 0.24138444696821398 valid 0.21651386450924248
LOSS train 0.24138444696821398 valid 0.21666080546047953
LOSS train 0.24138444696821398 valid 0.21663752247752244
LOSS train 0.24138444696821398 valid 0.21672193603127043
LOSS train 0.24138444696821398 valid 0.21674909633545836
LOSS train 0.24138444696821398 valid 0.21660079442701496
LOSS train 0.24138444696821398 valid 0.21671145113363657
LOSS train 0.24138444696821398 valid 0.2167436899057503
LOSS train 0.24138444696821398 valid 0.21672515161030947
LOSS train 0.24138444696821398 valid 0.21663048365355833
LOSS train 0.24138444696821398 valid 0.21667116965220226
Training bichrom
DEVICE = cpu
####################
Total Parameters = 606342
Total Trainable Parameters = 1157
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
base_model.conv1d.weight False
base_model.conv1d.bias False
base_model.batchNorm1d.weight False
base_model.batchNorm1d.bias False
base_model.lstm.weight_ih_l0 False
base_model.lstm.weight_hh_l0 False
base_model.lstm.bias_ih_l0 False
base_model.lstm.bias_hh_l0 False
base_model.model_dense_repeat.0.weight False
base_model.model_dense_repeat.0.bias False
base_model.model_dense_repeat.3.weight False
base_model.model_dense_repeat.3.bias False
base_model.model_dense_repeat.6.weight False
base_model.model_dense_repeat.6.bias False
base_model.linear.weight False
base_model.linear.bias False
linear.weight True
linear.bias True
model.conv1d.weight True
model.conv1d.bias True
model.lstm.weight_ih_l0 True
model.lstm.weight_hh_l0 True
model.lstm.bias_ih_l0 True
model.lstm.bias_hh_l0 True
model.linear.weight True
model.linear.bias True
linear2.weight True
linear2.bias True
####################
Epochs = 15
EPOCH 1:
  batch 1 loss: 0.7742646336555481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.7616885900497437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.7554046312967936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.7479287087917328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.7372676491737366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.7286918660004934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.7257930636405945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.721386045217514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.7167720595995585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.713185441493988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.709778904914856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.7061700572570165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.7031697722581717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.6995348121438708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.6951642553011577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.6922126896679401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.6876671279177946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.6839450365967221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.6799910978267067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.6756792038679122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.6715644030343919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.6681311862035231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.6640280433323072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.6601605266332626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.6564672470092774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.652992342527096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.6495603168452228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.6458925072635923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.6422991917051118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.6386450270811717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.6347935411237902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.6307923309504986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.626811657891129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.6233726722352645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.6199205466679164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.6165335385335816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.6129262165443318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.6089147352858594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.6057281455932519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.6023007445037365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.598896770215616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.5955218537932351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.5918716469476389
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.5885592224923047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.5852452543046739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.5817373777213304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.5788911530312072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.575676112746199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.57259287578719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.5697320657968521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.5668533759958604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.5639292271091387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.5609793601171026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.5579943690035079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.5549660503864289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.5525312530142921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.54981505191117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.547182207991337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.5448992575629282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.5428664276997248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.5405730316873456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.5378423972475913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.5352907649108342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.5328027135692537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.5305537297175481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.5283735715078585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.5262883438992856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.5241215776871232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.5218292137850886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.5193904323237283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.5171533138819144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.5150636095139716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.5132819495788993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.5113209792085596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.5094003959496816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.5076706366319406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.5056452352505225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.503840046051221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.502014300868481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.5004222139716148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.4987937936812271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.49705491487572834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.4952864560736231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.4936829067411877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.4919384521596572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.4901689297931139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.48858984928021487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.48684812066229904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.4853180968359615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.4837125341097514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.4820142535717933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.480760149333788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.4790311231408068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.47759863607426906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.47604255362560877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.4744738057876627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.47316383178701105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.4717775355188214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.4706378160703062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.4692109456658363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.4680030555418222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.46660386171995427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.46539959571893935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.46424860048752564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.46282085662796385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.46168349964438743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.4603794116840184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.4593009868705714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.4581610251457319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.4574222851883281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.45624573220003833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.4549970238336495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.45369167243484904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.45286788046360016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.4519229020761407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.45079196141711597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.4496946380688594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.44846243120856205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.44751320140702383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.4463708447913329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.4455614013612763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.44461148268863804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.4436704271692571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.4424541534916047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.44164753103256227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.44069746420496986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.4396992074692343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.4385585868731141
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.4374650395655817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.43622843348062956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.43524977314563196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.43417912089463434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.4332217533785598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.4322987583590977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.43130750876885876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.43069930330795403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.4296602232612833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.4286772932695306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.42776226268397816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.42675140031746456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.425859383868833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.42502970615742913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.42429555379427397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.42329864348802304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.42253946600289183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.4217103799320247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.42064697928980094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.4198760120449839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.4192712913023545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.41834950824578604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.4177198370560905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.416967883313957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.41644227699516645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.41583983871069824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.4150463915640308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.41425362401283705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.4133277822072339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.4125524302826652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.4118223156569139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.4112554114311934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.41078502754246965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.41013188494576347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.4095221620761544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.40886737569803144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.40796525875727335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.40723083769700613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.40666667322912614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.4058317004569939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.40513582589358266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.4045600934940226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.4038958019680447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.40314502823491427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.4025350693677891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.40202023842553986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.40148245556013923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.40092625756832684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.40065533714105855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.39996060410912115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.3994603233630431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.39882411261399586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.3982574738850251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.3977703267073893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.3972077285005747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.3966078418104545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.3959883175991677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.39550147758376214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.39506573138389994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.3946128625501978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.39404384689356287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.393443479506593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.39278306449271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.3923902994332214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.3919016070316493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.39146757801783455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.39085708764883187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.39034118396895273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.38963093098045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.38908034532961217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.38849411762539465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.3880565182864666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.38763752223840403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.38703879625490395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.386487942170627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.38604966361148685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.38545515697176863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.3850936464314322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.3846457460940172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.3840492588396256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.38357518691765635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.3832420898335321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.3827770322702507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.3824742387207049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.38208583892790926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.3814823293240271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.3809086109316626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.3803774802773087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.37984064303784876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.37944837898836226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.37907901773714037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.37858450710773467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.3781143283951876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.37765718070236415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.3772543537777101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.3767212586743491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.37633778757519193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.3757871719060746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.37538992465855264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.3747345248335286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.3742156938434168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.37391214863113736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.3735662402012648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.37310264734872456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.3725749741552214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.3721253929229883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.371590260241894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.37123068510475804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.37069529439578075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.37026543896488784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.3697410568158497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.36928752195090053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.3688921722510049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.3684608821164478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.36802981891995107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.3676115702532354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.3671248691422599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.36655076308463647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.3660974757150117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.36568611896326464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.365232116965405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.3648923432826996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.3645128183868302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.3641887708079247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.3637689231883867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.3633952515097115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.36306011396295884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.36273739230819046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.3623009920120239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.36188988248969234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.3615261172465836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.3611432959253971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.36083000430202117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.3604166002673957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.3599952414575185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.3596196139061993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.35934337664325283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.35887798807002547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.3585865907789616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.3583526477328877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.3579580849763629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.35763404573555346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.357159018681498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.3568005920661723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.3563834073451849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.3560744359219161
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.35559339447454974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.3552903151814488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.35505922821884983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.35465487671627416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.3543188091781404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.3539849348898445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.35355822858649216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.3532205744520992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.3528947607472592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.35261850767362285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.3523235187718743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.35194722286887936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.3517014172434391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.3512495462782681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.35086206308079426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.3505324760901517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.35030154976033673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.34992763787916265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.3497539967196793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.34941535853609745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.3491191700353461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.3487401992283963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.3483079825888579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.34798011848790533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.3477385915741075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.3474110004802545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.34704849703169344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.34666367151484584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.346319653954443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.3460013506149775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.3457570451693457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.3453463430887733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.34493711047141484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.34460335244219026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.3442951838657694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.3438366546746223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.3434188480450026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.343024765021908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.34268223158658123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.3423560061936925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.34211022754510245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.3418231241970877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.34154405592931936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.341194508257527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.34084362487621067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.3404877757187933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.3401249863926867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.3398509842360982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.33942057164645417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.3390382041717753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.33880622377762426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.338452107244474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.3381024888896067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.3378824515313637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.3376208908833269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.3372631133054242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.33699958044416595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.3367692514446126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.336456966919226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.3362056591553602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.33592542200835784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.3356967709869856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.3353567608474624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.3349797021178804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.3345815686613409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.334343684552347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.33403066477992316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.33377541775940456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.3334441410035503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.3331296850688929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.33287159718465115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.332508133799699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.3322994444916503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.3320630230009556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.33170915153784875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.3314839210680553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.33124067594013323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.33089851499111816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.33062290448811504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.3303758243551362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.3300766918860691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.32974215860614614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.3293968892398001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.3290991560457139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.3287294407823956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.3284544631424877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.3281078851289036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.3278767912756672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.3275775143430253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.32725981712996305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.3270255706081652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.32678975348101286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.3264635791164653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.3262488681256123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.32601218729161313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.3257356957406611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.3254619497815875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.325159355036674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.32484747049955515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.3245988643982194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.32430829139550527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.3240123091939282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.32376985892218685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.3234743669077202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.3232388002969973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.32299964659308134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.3226724507927582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.3224092756266369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.3220883533164042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.3218234603603681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.321493505618789
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.3212576847079504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.3208713539580042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.320648159601332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.3203670158407682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.3200870338540811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.31989562675318756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.3196762717728104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.3194220264373541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.3191471096827899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.3189169414435761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.31860123292514775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.31832704255803107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.31802692994400483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.3177678276572311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.31748504530638455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.31729357858697077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.31705845518046943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.31677400799602196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.3164908271098491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.3161840854971497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.3158930519898537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.3156400655718928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.3153306774283741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.3151065083471079
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.31484792083501817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.3146353916167633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.3143631333430994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.3140650574144959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.3137179869744513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.31348760497857286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.3132613916308261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.31292831429617585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.31264664239860607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.31240272785143525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.31207758294684546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.3118373839419132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.3116537651038283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.3113601444460822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.3111671613142738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.310920043798054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.3106369793485028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.3103789659587784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.3101439887535906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.30987303716180487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.3096214975035468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.3093848139255063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.3092070753444676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.3089643079813303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.30879477394341326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.30853440915716107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.30833803209553073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.3081231238640146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.30790745422720367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.30764502086644835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.3074081828309731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.30716706278507944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.3068951531165865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.3066811020029856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.306417645903321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.3061503734481469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.3058504809379043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.3056046461698993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.30538112580377075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.3051353107250082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.304857361416022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.3046176703171825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.3043410306615112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.3040838534666213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.3037765473455585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.30351811722739713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.3032734514982031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.30302072503895416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.3027139909923337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.30239622677982764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.30220768746474513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.30198387497805723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.3017129873558556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.3014855109654029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.3013179850090167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.3010120354352459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.3008060939769888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.300588565062915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.30035549676061696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.30012871992232193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.2998277797026837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.2996427622760177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.29940002465273363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.29940002465273363 valid 0.2137509286403656
LOSS train 0.29940002465273363 valid 0.22813718020915985
LOSS train 0.29940002465273363 valid 0.2410511076450348
LOSS train 0.29940002465273363 valid 0.23872707039117813
LOSS train 0.29940002465273363 valid 0.24104607701301575
LOSS train 0.29940002465273363 valid 0.24510642886161804
LOSS train 0.29940002465273363 valid 0.23967266934258596
LOSS train 0.29940002465273363 valid 0.2382703609764576
LOSS train 0.29940002465273363 valid 0.23725136121114096
LOSS train 0.29940002465273363 valid 0.23771773725748063
LOSS train 0.29940002465273363 valid 0.23476709019054065
LOSS train 0.29940002465273363 valid 0.23651004334290823
LOSS train 0.29940002465273363 valid 0.23542276827188638
LOSS train 0.29940002465273363 valid 0.23534167557954788
LOSS train 0.29940002465273363 valid 0.23385827541351317
LOSS train 0.29940002465273363 valid 0.23508568294346333
LOSS train 0.29940002465273363 valid 0.23467632602242863
LOSS train 0.29940002465273363 valid 0.23434735586245856
LOSS train 0.29940002465273363 valid 0.23597672503245504
LOSS train 0.29940002465273363 valid 0.2353609435260296
LOSS train 0.29940002465273363 valid 0.23562960894334883
LOSS train 0.29940002465273363 valid 0.2348841212012551
LOSS train 0.29940002465273363 valid 0.23361202815304632
LOSS train 0.29940002465273363 valid 0.2343359279135863
LOSS train 0.29940002465273363 valid 0.23324579536914825
LOSS train 0.29940002465273363 valid 0.23266641107889321
LOSS train 0.29940002465273363 valid 0.23234122936372403
LOSS train 0.29940002465273363 valid 0.2322640578661646
LOSS train 0.29940002465273363 valid 0.2313873639394497
LOSS train 0.29940002465273363 valid 0.23076994121074676
LOSS train 0.29940002465273363 valid 0.23024658714571306
LOSS train 0.29940002465273363 valid 0.23086334532126784
LOSS train 0.29940002465273363 valid 0.2297356589273973
LOSS train 0.29940002465273363 valid 0.22963057633708506
LOSS train 0.29940002465273363 valid 0.23041558265686035
LOSS train 0.29940002465273363 valid 0.2305210762553745
LOSS train 0.29940002465273363 valid 0.22979241810940407
LOSS train 0.29940002465273363 valid 0.22944762518531397
LOSS train 0.29940002465273363 valid 0.22906674177218705
LOSS train 0.29940002465273363 valid 0.22910670675337313
LOSS train 0.29940002465273363 valid 0.22901443991719223
LOSS train 0.29940002465273363 valid 0.2303044373790423
LOSS train 0.29940002465273363 valid 0.2311866529459177
LOSS train 0.29940002465273363 valid 0.23057485608892006
LOSS train 0.29940002465273363 valid 0.2301331424050861
LOSS train 0.29940002465273363 valid 0.22897006150173105
LOSS train 0.29940002465273363 valid 0.22864793811706788
LOSS train 0.29940002465273363 valid 0.22994605358690023
LOSS train 0.29940002465273363 valid 0.22975517505285692
LOSS train 0.29940002465273363 valid 0.23027722984552385
LOSS train 0.29940002465273363 valid 0.22992417303954854
LOSS train 0.29940002465273363 valid 0.22987876574580485
LOSS train 0.29940002465273363 valid 0.23080437841280452
LOSS train 0.29940002465273363 valid 0.23074552885912084
LOSS train 0.29940002465273363 valid 0.23098227381706238
LOSS train 0.29940002465273363 valid 0.2309453213321311
LOSS train 0.29940002465273363 valid 0.230491924442743
LOSS train 0.29940002465273363 valid 0.23103799362634792
LOSS train 0.29940002465273363 valid 0.2311477663658433
LOSS train 0.29940002465273363 valid 0.23090163245797157
LOSS train 0.29940002465273363 valid 0.23087524903602288
LOSS train 0.29940002465273363 valid 0.23046062838646672
LOSS train 0.29940002465273363 valid 0.2305408919614459
LOSS train 0.29940002465273363 valid 0.23039850033819675
LOSS train 0.29940002465273363 valid 0.22945464092951554
LOSS train 0.29940002465273363 valid 0.2293202585794709
LOSS train 0.29940002465273363 valid 0.22981323479716456
LOSS train 0.29940002465273363 valid 0.2292919822913759
LOSS train 0.29940002465273363 valid 0.2296368913806003
LOSS train 0.29940002465273363 valid 0.22986855187586375
LOSS train 0.29940002465273363 valid 0.23004658352321303
LOSS train 0.29940002465273363 valid 0.23057389983700383
LOSS train 0.29940002465273363 valid 0.2308447783532208
LOSS train 0.29940002465273363 valid 0.23081477046818347
LOSS train 0.29940002465273363 valid 0.2305765958627065
LOSS train 0.29940002465273363 valid 0.23066039834367602
LOSS train 0.29940002465273363 valid 0.23087388122236574
LOSS train 0.29940002465273363 valid 0.23074061977557647
LOSS train 0.29940002465273363 valid 0.23105285627932487
LOSS train 0.29940002465273363 valid 0.2307621708139777
LOSS train 0.29940002465273363 valid 0.2308436569608288
LOSS train 0.29940002465273363 valid 0.2311238533839947
LOSS train 0.29940002465273363 valid 0.2310657213969403
LOSS train 0.29940002465273363 valid 0.23092007725721314
LOSS train 0.29940002465273363 valid 0.23126339158591103
LOSS train 0.29940002465273363 valid 0.23141623357700747
LOSS train 0.29940002465273363 valid 0.23101329991872283
LOSS train 0.29940002465273363 valid 0.23104458623989063
LOSS train 0.29940002465273363 valid 0.2311583040805345
LOSS train 0.29940002465273363 valid 0.23107520523998473
LOSS train 0.29940002465273363 valid 0.23119636944362096
LOSS train 0.29940002465273363 valid 0.23117865913588068
LOSS train 0.29940002465273363 valid 0.23140182225934922
LOSS train 0.29940002465273363 valid 0.23159552825258134
LOSS train 0.29940002465273363 valid 0.2318021667631049
LOSS train 0.29940002465273363 valid 0.23245138054092726
LOSS train 0.29940002465273363 valid 0.23262908056224743
LOSS train 0.29940002465273363 valid 0.2329895151208858
LOSS train 0.29940002465273363 valid 0.23304335532164333
LOSS train 0.29940002465273363 valid 0.23319981426000594
LOSS train 0.29940002465273363 valid 0.2331134868730413
LOSS train 0.29940002465273363 valid 0.23340140838249057
LOSS train 0.29940002465273363 valid 0.23305415036608873
LOSS train 0.29940002465273363 valid 0.2329217172586001
LOSS train 0.29940002465273363 valid 0.23308729245549156
LOSS train 0.29940002465273363 valid 0.23315235564731202
LOSS train 0.29940002465273363 valid 0.23297085926354488
LOSS train 0.29940002465273363 valid 0.23294073688211264
LOSS train 0.29940002465273363 valid 0.23246021363713326
LOSS train 0.29940002465273363 valid 0.23230979293584825
LOSS train 0.29940002465273363 valid 0.23240195724878226
LOSS train 0.29940002465273363 valid 0.23244416381099395
LOSS train 0.29940002465273363 valid 0.2323383906510024
LOSS train 0.29940002465273363 valid 0.23240379204875544
LOSS train 0.29940002465273363 valid 0.23282245786293693
LOSS train 0.29940002465273363 valid 0.232746339692124
LOSS train 0.29940002465273363 valid 0.23278782586766106
LOSS train 0.29940002465273363 valid 0.23278186735460313
LOSS train 0.29940002465273363 valid 0.2324408786136563
LOSS train 0.29940002465273363 valid 0.23224068668981393
LOSS train 0.29940002465273363 valid 0.23229218157362347
LOSS train 0.29940002465273363 valid 0.23242254213231509
LOSS train 0.29940002465273363 valid 0.23232718660094873
LOSS train 0.29940002465273363 valid 0.23234101180588046
LOSS train 0.29940002465273363 valid 0.2324162415266037
LOSS train 0.29940002465273363 valid 0.23244386225465744
LOSS train 0.29940002465273363 valid 0.23244401671755033
LOSS train 0.29940002465273363 valid 0.2322735101915896
LOSS train 0.29940002465273363 valid 0.23210036015325738
LOSS train 0.29940002465273363 valid 0.2318399299795811
LOSS train 0.29940002465273363 valid 0.23188005501532372
LOSS train 0.29940002465273363 valid 0.2319940740637707
LOSS train 0.29940002465273363 valid 0.23197527700348905
LOSS train 0.29940002465273363 valid 0.232119700913109
LOSS train 0.29940002465273363 valid 0.2320874073991069
LOSS train 0.29940002465273363 valid 0.23212107490090764
LOSS train 0.29940002465273363 valid 0.23198821618609186
LOSS train 0.29940002465273363 valid 0.23172612401886264
LOSS train 0.29940002465273363 valid 0.23155875536177656
LOSS train 0.29940002465273363 valid 0.23154435956052372
LOSS train 0.29940002465273363 valid 0.2316909761082196
LOSS train 0.29940002465273363 valid 0.23194447799887455
LOSS train 0.29940002465273363 valid 0.231978208347634
LOSS train 0.29940002465273363 valid 0.23202563573916754
LOSS train 0.29940002465273363 valid 0.2318300923396801
LOSS train 0.29940002465273363 valid 0.23191473541194446
LOSS train 0.29940002465273363 valid 0.2318342206429462
LOSS train 0.29940002465273363 valid 0.23162315544244405
LOSS train 0.29940002465273363 valid 0.23176358410176015
LOSS train 0.29940002465273363 valid 0.23170621891816456
LOSS train 0.29940002465273363 valid 0.23160775341340248
LOSS train 0.29940002465273363 valid 0.23155882229146205
LOSS train 0.29940002465273363 valid 0.23128974203969918
LOSS train 0.29940002465273363 valid 0.23132630515021163
LOSS train 0.29940002465273363 valid 0.23117218171396564
LOSS train 0.29940002465273363 valid 0.23138204025916564
LOSS train 0.29940002465273363 valid 0.23125760522997305
LOSS train 0.29940002465273363 valid 0.23122253268957138
LOSS train 0.29940002465273363 valid 0.2315335548336401
LOSS train 0.29940002465273363 valid 0.23152509648352862
LOSS train 0.29940002465273363 valid 0.23157357447636054
LOSS train 0.29940002465273363 valid 0.23151805058673577
LOSS train 0.29940002465273363 valid 0.23128325240743672
LOSS train 0.29940002465273363 valid 0.2313195436102588
LOSS train 0.29940002465273363 valid 0.2314128973267295
LOSS train 0.29940002465273363 valid 0.23132818928325033
LOSS train 0.29940002465273363 valid 0.2313483689359562
LOSS train 0.29940002465273363 valid 0.23115534690164385
LOSS train 0.29940002465273363 valid 0.23099410525087774
LOSS train 0.29940002465273363 valid 0.23090653226656072
LOSS train 0.29940002465273363 valid 0.23075451566810495
LOSS train 0.29940002465273363 valid 0.23077252143344215
LOSS train 0.29940002465273363 valid 0.23077871844258613
LOSS train 0.29940002465273363 valid 0.23081469296038837
LOSS train 0.29940002465273363 valid 0.23060209001813617
LOSS train 0.29940002465273363 valid 0.23052677131173285
LOSS train 0.29940002465273363 valid 0.23050506463495352
LOSS train 0.29940002465273363 valid 0.2305630770292175
LOSS train 0.29940002465273363 valid 0.23053645762984312
LOSS train 0.29940002465273363 valid 0.23058994321359527
LOSS train 0.29940002465273363 valid 0.23050623679358656
LOSS train 0.29940002465273363 valid 0.23042849467678383
LOSS train 0.29940002465273363 valid 0.23050948847187022
LOSS train 0.29940002465273363 valid 0.23055363786609276
LOSS train 0.29940002465273363 valid 0.2305473427514772
LOSS train 0.29940002465273363 valid 0.23064264591022204
LOSS train 0.29940002465273363 valid 0.2305698281622188
LOSS train 0.29940002465273363 valid 0.2306403380600696
LOSS train 0.29940002465273363 valid 0.2304739236200928
LOSS train 0.29940002465273363 valid 0.23046922338636297
LOSS train 0.29940002465273363 valid 0.23060157926294816
LOSS train 0.29940002465273363 valid 0.23071218266462287
LOSS train 0.29940002465273363 valid 0.23055558748195826
LOSS train 0.29940002465273363 valid 0.23057927029956246
LOSS train 0.29940002465273363 valid 0.23050879270602495
LOSS train 0.29940002465273363 valid 0.23071314500910894
LOSS train 0.29940002465273363 valid 0.23072213970767666
LOSS train 0.29940002465273363 valid 0.23083857808149222
LOSS train 0.29940002465273363 valid 0.23092330303323927
LOSS train 0.29940002465273363 valid 0.23098341055214405
LOSS train 0.29940002465273363 valid 0.2309458409227542
LOSS train 0.29940002465273363 valid 0.23109388211280993
LOSS train 0.29940002465273363 valid 0.2312322084980058
LOSS train 0.29940002465273363 valid 0.2311457761508577
LOSS train 0.29940002465273363 valid 0.23103515840158229
LOSS train 0.29940002465273363 valid 0.23099232016547214
LOSS train 0.29940002465273363 valid 0.23089563724211448
LOSS train 0.29940002465273363 valid 0.2306709183523288
LOSS train 0.29940002465273363 valid 0.2307255211058986
LOSS train 0.29940002465273363 valid 0.23088134257566362
LOSS train 0.29940002465273363 valid 0.2310457752214224
LOSS train 0.29940002465273363 valid 0.23092102906051673
LOSS train 0.29940002465273363 valid 0.23085687005184066
LOSS train 0.29940002465273363 valid 0.23078484122998247
LOSS train 0.29940002465273363 valid 0.23050481032493503
LOSS train 0.29940002465273363 valid 0.23029699086866998
LOSS train 0.29940002465273363 valid 0.23022603061616695
LOSS train 0.29940002465273363 valid 0.23019781052519422
LOSS train 0.29940002465273363 valid 0.23022323499803674
LOSS train 0.29940002465273363 valid 0.23019634559750557
LOSS train 0.29940002465273363 valid 0.23012694345610174
LOSS train 0.29940002465273363 valid 0.23019733192684413
LOSS train 0.29940002465273363 valid 0.23013011670166067
LOSS train 0.29940002465273363 valid 0.23000322608277202
LOSS train 0.29940002465273363 valid 0.22981066445509593
LOSS train 0.29940002465273363 valid 0.22967808667273648
LOSS train 0.29940002465273363 valid 0.2297023230843607
LOSS train 0.29940002465273363 valid 0.22966852130597099
LOSS train 0.29940002465273363 valid 0.22956983159462957
LOSS train 0.29940002465273363 valid 0.2296438139417897
LOSS train 0.29940002465273363 valid 0.22961867481341094
LOSS train 0.29940002465273363 valid 0.2296321614292161
LOSS train 0.29940002465273363 valid 0.22981231944243796
LOSS train 0.29940002465273363 valid 0.22976855545217156
LOSS train 0.29940002465273363 valid 0.22978587410551438
LOSS train 0.29940002465273363 valid 0.2296830853920872
LOSS train 0.29940002465273363 valid 0.22968384296582217
LOSS train 0.29940002465273363 valid 0.22970379356827056
LOSS train 0.29940002465273363 valid 0.22966113848666267
LOSS train 0.29940002465273363 valid 0.22932937716444332
LOSS train 0.29940002465273363 valid 0.22916127815533477
LOSS train 0.29940002465273363 valid 0.22919807102808282
LOSS train 0.29940002465273363 valid 0.22921162169166062
LOSS train 0.29940002465273363 valid 0.22923163269631197
LOSS train 0.29940002465273363 valid 0.22940744812391242
LOSS train 0.29940002465273363 valid 0.22947818083249458
LOSS train 0.29940002465273363 valid 0.2295420023231854
LOSS train 0.29940002465273363 valid 0.22948771510874072
LOSS train 0.29940002465273363 valid 0.22940788738220091
LOSS train 0.29940002465273363 valid 0.2295328402519226
LOSS train 0.29940002465273363 valid 0.2295783559877084
LOSS train 0.29940002465273363 valid 0.2296403988249718
LOSS train 0.29940002465273363 valid 0.2295442894984611
LOSS train 0.29940002465273363 valid 0.22959970676992822
LOSS train 0.29940002465273363 valid 0.22960813904509825
LOSS train 0.29940002465273363 valid 0.22958815738093108
LOSS train 0.29940002465273363 valid 0.22957673089049668
LOSS train 0.29940002465273363 valid 0.22965696604214897
LOSS train 0.29940002465273363 valid 0.22963126423736335
LOSS train 0.29940002465273363 valid 0.22950808382951296
LOSS train 0.29940002465273363 valid 0.22950869969937993
LOSS train 0.29940002465273363 valid 0.22972041823481784
LOSS train 0.29940002465273363 valid 0.22964352103013955
LOSS train 0.29940002465273363 valid 0.22971549878517786
LOSS train 0.29940002465273363 valid 0.22971105524953805
LOSS train 0.29940002465273363 valid 0.22971495117684057
LOSS train 0.29940002465273363 valid 0.22972969387131237
LOSS train 0.29940002465273363 valid 0.2297896604373384
LOSS train 0.29940002465273363 valid 0.22983412943143383
LOSS train 0.29940002465273363 valid 0.22971435051273417
LOSS train 0.29940002465273363 valid 0.2296696176058252
LOSS train 0.29940002465273363 valid 0.22950334781233003
LOSS train 0.29940002465273363 valid 0.22940702905585042
LOSS train 0.29940002465273363 valid 0.22937167316240115
LOSS train 0.29940002465273363 valid 0.2294508605111729
LOSS train 0.29940002465273363 valid 0.22954184686144194
LOSS train 0.29940002465273363 valid 0.22953134511567194
LOSS train 0.29940002465273363 valid 0.22954045370113935
LOSS train 0.29940002465273363 valid 0.2295942680916906
LOSS train 0.29940002465273363 valid 0.229453255502241
LOSS train 0.29940002465273363 valid 0.2293674465918456
LOSS train 0.29940002465273363 valid 0.22934647538560501
LOSS train 0.29940002465273363 valid 0.22937892493215972
LOSS train 0.29940002465273363 valid 0.22924392339839061
LOSS train 0.29940002465273363 valid 0.2293107462033891
LOSS train 0.29940002465273363 valid 0.22935276092974455
LOSS train 0.29940002465273363 valid 0.2294079945893238
LOSS train 0.29940002465273363 valid 0.22936406141767898
LOSS train 0.29940002465273363 valid 0.22932236551413487
LOSS train 0.29940002465273363 valid 0.22926492166930232
LOSS train 0.29940002465273363 valid 0.2292217542625375
LOSS train 0.29940002465273363 valid 0.22917457925130244
LOSS train 0.29940002465273363 valid 0.22916169912131573
LOSS train 0.29940002465273363 valid 0.22914475440776266
LOSS train 0.29940002465273363 valid 0.22914781858355313
LOSS train 0.29940002465273363 valid 0.22919699730905327
LOSS train 0.29940002465273363 valid 0.2292142611260366
LOSS train 0.29940002465273363 valid 0.22928585757145145
LOSS train 0.29940002465273363 valid 0.22928859702121454
LOSS train 0.29940002465273363 valid 0.22929697493712106
LOSS train 0.29940002465273363 valid 0.22926277734116463
LOSS train 0.29940002465273363 valid 0.2292900958795421
LOSS train 0.29940002465273363 valid 0.22927902186467702
LOSS train 0.29940002465273363 valid 0.22924676924747855
LOSS train 0.29940002465273363 valid 0.22918883518117372
LOSS train 0.29940002465273363 valid 0.22930514904993032
LOSS train 0.29940002465273363 valid 0.22921922113685733
LOSS train 0.29940002465273363 valid 0.22930397615804302
LOSS train 0.29940002465273363 valid 0.22934738968568327
LOSS train 0.29940002465273363 valid 0.2293682728082903
LOSS train 0.29940002465273363 valid 0.229415353663098
LOSS train 0.29940002465273363 valid 0.22937974104514489
LOSS train 0.29940002465273363 valid 0.22947647891486414
LOSS train 0.29940002465273363 valid 0.2295016384903033
LOSS train 0.29940002465273363 valid 0.22951756181224944
LOSS train 0.29940002465273363 valid 0.22954957201322423
LOSS train 0.29940002465273363 valid 0.2295480883835994
LOSS train 0.29940002465273363 valid 0.22959787937455206
LOSS train 0.29940002465273363 valid 0.22961000953348454
LOSS train 0.29940002465273363 valid 0.2295221275649965
LOSS train 0.29940002465273363 valid 0.22950754108087296
LOSS train 0.29940002465273363 valid 0.22949976413886738
LOSS train 0.29940002465273363 valid 0.2294230416370988
LOSS train 0.29940002465273363 valid 0.22940203312922408
LOSS train 0.29940002465273363 valid 0.22924790020172411
LOSS train 0.29940002465273363 valid 0.2293116074123997
LOSS train 0.29940002465273363 valid 0.22920885832484708
LOSS train 0.29940002465273363 valid 0.22922227054652644
LOSS train 0.29940002465273363 valid 0.22924234770170462
LOSS train 0.29940002465273363 valid 0.2292714428269502
LOSS train 0.29940002465273363 valid 0.22921946851868644
LOSS train 0.29940002465273363 valid 0.22922224358442317
LOSS train 0.29940002465273363 valid 0.2292815967723056
LOSS train 0.29940002465273363 valid 0.22925775698915926
LOSS train 0.29940002465273363 valid 0.22919191606898806
LOSS train 0.29940002465273363 valid 0.22912228834770976
LOSS train 0.29940002465273363 valid 0.2292761949474097
LOSS train 0.29940002465273363 valid 0.2291872479771016
LOSS train 0.29940002465273363 valid 0.22911614729064053
LOSS train 0.29940002465273363 valid 0.2291344569009893
LOSS train 0.29940002465273363 valid 0.22914781808153975
LOSS train 0.29940002465273363 valid 0.22914429213742762
LOSS train 0.29940002465273363 valid 0.22904015829889837
LOSS train 0.29940002465273363 valid 0.22908312164593575
LOSS train 0.29940002465273363 valid 0.22913389236166856
LOSS train 0.29940002465273363 valid 0.22905676191769583
LOSS train 0.29940002465273363 valid 0.22901819478881463
LOSS train 0.29940002465273363 valid 0.22898128808572374
LOSS train 0.29940002465273363 valid 0.22892623217536248
LOSS train 0.29940002465273363 valid 0.22895026011126382
LOSS train 0.29940002465273363 valid 0.2289796668155241
LOSS train 0.29940002465273363 valid 0.22900483185764064
LOSS train 0.29940002465273363 valid 0.22904346516044552
LOSS train 0.29940002465273363 valid 0.22899348314030696
LOSS train 0.29940002465273363 valid 0.22889936507587702
LOSS train 0.29940002465273363 valid 0.22888559906670217
LOSS train 0.29940002465273363 valid 0.22892080447753937
LOSS train 0.29940002465273363 valid 0.22900357896555734
LOSS train 0.29940002465273363 valid 0.228998258958952
LOSS train 0.29940002465273363 valid 0.22916221887701088
LOSS train 0.29940002465273363 valid 0.2291071426505197
LOSS train 0.29940002465273363 valid 0.22920373263280036
LOSS train 0.29940002465273363 valid 0.22924984137709803
LOSS train 0.29940002465273363 valid 0.22918805901657094
LOSS train 0.29940002465273363 valid 0.2292258476149546
LOSS train 0.29940002465273363 valid 0.22920956254982558
LOSS train 0.29940002465273363 valid 0.22923435721475358
LOSS train 0.29940002465273363 valid 0.2291524749573158
LOSS train 0.29940002465273363 valid 0.22922839664508335
EPOCH 2:
  batch 1 loss: 0.21452027559280396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.18996185064315796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1847654084364573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.18358943611383438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1844661980867386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1842633311947187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.18334350628512247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1808548215776682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.18233645459016165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.18177312165498732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1825466508215124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1829213798046112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.18239849576583275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1811704124723162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1807810992002487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.18170527555048466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.182603722986053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.18334848268164528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.18349933232131757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.18375175446271896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.18428705845560348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.18434650721875104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.18280502700287363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.18264279576639333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.18271773874759675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.18400397724830186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.18457149300310347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1852377631834575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.18598875763087436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1859184960524241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.18545944411908427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1846088981255889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.18431506915525955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.18504491799017964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1858046020780291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1864677464796437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.18666761026189133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1858459636569023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.18525897730619478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.18554721474647523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.18541867703926274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1853858700820378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1849733452464259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.18511116098273883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.18497628536489275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1846076126979745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.18479648169050825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.18495753376434246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.18516512123905882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1851448267698288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.18487237423074013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1845252367739494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.18447997013352951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.18436227876830985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.18433310145681556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.18450141910995757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.18457879988770737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.18422694915327534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.18467321032184666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.18530387729406356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.18567817279549895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1852344168770698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.18488005111141811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.18484953488223255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.18474662831196417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.18470323379292633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1848287482315035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1848751039189451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.18461657045544058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.18427848198584149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.18430880488644183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1839619210610787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.18415381259297672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.18435638176428304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1841454537709554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.18410283954519974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.18411930485979303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.18422076793817374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.18420955425576319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1842887930572033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.18429999384615156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.18433011359557872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.18413821473179093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.18419803288720904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.18420036803273593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.18386635548153588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1838718114913195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1836043084886941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.18363456679194162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.18369396213028166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.18360378896142102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1835641984058463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.18337171132205635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1834048684290115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.18316957040836937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.183051031238089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.18296894722992613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.18279830411988862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.18301727585118227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.18301886156201363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.18311143172259378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.18308037139621436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.18305670853378703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.18321811121243697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1830537627140681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1830444797030035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.18305687634187323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.18299197460766192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.18301133332996194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1832711867310784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.18314864224678762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.18285283246742828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.18268668704328284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.18276067261110274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1828862427369408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.18286490299064537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1829266068008211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1826813771563061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.18276847498256618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.18260586000978946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1827049139610007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1827527438519431
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.18275026177487722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1825910112069499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.18274991834163665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1826750249380157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.18273289290469463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1825572414090857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.18231723440247913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1820766168145033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.18198000122117633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.18170912981484877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.18164254825814327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1814044123487686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.18124977195704425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.18142357326167471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1813562829999158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.18123322520135104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.18121887732752792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.18094037153891154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.18088848582396272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.18086416421222015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.18082214464674462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.18066791362232631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.18079936247447442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.18081095365628805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.18053954738337977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.18056713014438347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.18066473255221477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.18060405711332958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.18064978896387365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.18065625242888927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.18073473208480412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.18066395490200488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.18051866571749411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.18040901326980346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.180282325216919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1801702914924561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.18011055849258256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1800564207136631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1800769499925353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.18018456116134737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1802438599931682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.180163224295872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.17996966287945257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.17981578275022736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1797785742911036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.17960643475609167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.17946217636737596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.17952704780242024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.17950960784627681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.17936770427365636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1792668962065195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.17925675033495345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.17929690744195664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.17928786897523838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1795620014101772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.17949960857964634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.17949044371450412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1794278358419736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.17937767719695583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.17938742958582365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.17937894035232524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.17929870675763357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.17922356201184764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.17914953146890927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1792219562485894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.17914505366315234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.17913697991106245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.17911887851200606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.17900228235109938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.17914071857618788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.17913567981262898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.17910823433362333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.17901086203562908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.17895892923887896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.17871285490880762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1786368369604602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.17849797579511326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.17859060414135455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.17857181855398624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.17849774311969777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1784854399159624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.17844226445053138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.17831691736128272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.17836988059062403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.17824573210184125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1780513141017694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.17789975421850762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.17793580357517516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.17788694530584237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.17805574588337034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.17813167423709458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.17801599552698225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.17791424989700316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.17784739878994446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.17781615106191503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1777990391358323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1778729916571482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1776485451920466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1776036597889473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.17754495204300494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.17758203748897586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.17751756263896823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.17753217306401994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.17739339753062325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1774734615754451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.17729333049634047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.17717685846522385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.17718967801850774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.17720585964716873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.17716885011257796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.17709797013215242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.17702564132264537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1769158955584181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1768980351418762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.17683575431254342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.17688610805433339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1767482941494826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1766621827458342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.17661038319352257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.17661738641991104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1765569038229224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.17650105553816575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.17644768217388465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.17628132854777623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.17624295222373143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.17627444953447388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.17622147596265417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1762480982542038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1762151532914059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1762016106337782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.17612503745810318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.17602473069129027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.17606471467251872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.17607068974757567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1759606527911086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.17590219573688137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.17593118088125723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.17593206258920524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1760430215533209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1760478333093738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1759131345685444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.17593303367947088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.17594266854367166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.17579997286088483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1758057504111015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.17585530387821482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.17575510372459666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1758083701133728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.17569491679598045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.17560956008074916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.17553208932116793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1755209313909503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1753817383809523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1753520976914012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.17542183501410571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1753304759911496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.17526373512855994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1752820340118238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.17519714818518356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1752006969024949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.17512482820677672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1751211296084901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.17521476039760991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1751889638029612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.17525248733131726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.17512933345925477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.17499655737802644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.17495952870311407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.17494651987585416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.17491722862197928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1750139249266211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.17498112799359017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.17501059063410354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.17501348926610238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.17492454496497659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.17499937337116908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.17500238260098525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.17492590536673863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.17483116547728694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1746973686956412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.17470184539017505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.17470228569091936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.17472704868824757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.17467655460624135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.17460652598534812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.17457958291490355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.17463040728013493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.17453893519216968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.17446870895827316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.17440436107034868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.17441593429531915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1744143043164235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.17449540500603025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.17448472212764282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.17448722864927177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.17445332411700074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1744199308762356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.17435532272793353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.17432399771852286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1744019214225852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.17427393305043318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.17420712071988317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.17427518963813782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.17422819137573242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.17419653948658467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.17419536579854605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.17423230242040744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.17414986897598614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.17411722526449452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.17415084999548383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.17416577797394256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1741479951553716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1740983830903893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.17414312867359036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1741246299414677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.17404700652558422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.17392774717470186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.17392980210921344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.17390201955247134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.17384996809806044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.17380713200082584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1737673704534076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.17378787450168443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1737489442346413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.17379322587928442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1737983481979918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.17369154112728413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.17370488371167864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.17373376303588564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.17368743145330387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1736608285235278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.17374277013843342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.17370549327051135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.17364875989013842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1735540426829282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.17350553267494925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.17345423303274723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.17344859374894037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.17338846656919515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.17345103897277822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1734088016443016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.17335541755124761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.17336151938732355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.17334200384838333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.17324253340183227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.17329399769558854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1733036485467823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1733247524177706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.17333941776154818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.17329350858926773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.17320713680487215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1732520404505857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.17321259005864462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.17319972523824967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.17319565398781622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.17316920341795713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1732045690821469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.17323873674398974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.17321126851353433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.17318316439839557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1731407226065743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1731195169656227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.173028116411977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.173035616084084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.17290782406733635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.17293399785520489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1729011017551459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.17288126355180375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.17293280002939732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.17294089235745522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1729168980062463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.17287337513366327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1729063438652437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.17284635080005786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.17282456420455833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.17279039051889175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1727793621352143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.17275905473157763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.17280179884591304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.17282367910985924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.17278191526755507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1727526634747144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.17269664842773366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.17266031573044843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.17263164711745424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1725424753648101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.17255378776847005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.172528778725281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.17257703287830609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1725488017376476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.17245947291790428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.17235803947860492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.17235033991466087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.17237553933563715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.17226634074529584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.17219307257584407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.17216102201002595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1721050811488004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.17208644310609744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.17211729494669425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.17204833293144303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.17208298852772647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.17206389176494935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.17199180084564877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.17196662484971365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.17198533893815268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1719572260836899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.17196514895835588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.17192842448324724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.17198122266886962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1719614509794233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.17200371734626282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.17198910050351043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1720279576000544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.17203714408713566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.17205497773883005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.17203758899674057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.17202941431579263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1720138885368025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.17196291889068227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1719635337692082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.171913601038558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.17185927326424738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.17179630928977724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.17175835372684253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.17175485002475657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.17172335282399553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.17165440160367224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.17163801617804758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.17157654548311127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.17153777739662207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1714303221569975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.17135306657343122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.17132134305868754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.17128256346451637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.17120047217828738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.17110233964938223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.17112317517723727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.17112244964615642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.17106599997712937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1710475013947384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.17110957778540664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.17103845542797477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.17104311765455382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.17105676257699665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1710445038241963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1710351151920585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1709280125796795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.17094119169972757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.17088600180222321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.17088600180222321 valid 0.188135027885437
LOSS train 0.17088600180222321 valid 0.2109958976507187
LOSS train 0.17088600180222321 valid 0.2249139646689097
LOSS train 0.17088600180222321 valid 0.2213563397526741
LOSS train 0.17088600180222321 valid 0.22331576347351073
LOSS train 0.17088600180222321 valid 0.2282509058713913
LOSS train 0.17088600180222321 valid 0.22236346985612596
LOSS train 0.17088600180222321 valid 0.22096018865704536
LOSS train 0.17088600180222321 valid 0.2196463876300388
LOSS train 0.17088600180222321 valid 0.22057106643915175
LOSS train 0.17088600180222321 valid 0.21716916696591812
LOSS train 0.17088600180222321 valid 0.2191017804046472
LOSS train 0.17088600180222321 valid 0.21780213025900033
LOSS train 0.17088600180222321 valid 0.218742912369115
LOSS train 0.17088600180222321 valid 0.2173445443312327
LOSS train 0.17088600180222321 valid 0.21780149266123772
LOSS train 0.17088600180222321 valid 0.21747006563579335
LOSS train 0.17088600180222321 valid 0.21742576360702515
LOSS train 0.17088600180222321 valid 0.21933386514061376
LOSS train 0.17088600180222321 valid 0.21872982680797576
LOSS train 0.17088600180222321 valid 0.21896853191511972
LOSS train 0.17088600180222321 valid 0.21778420900756662
LOSS train 0.17088600180222321 valid 0.21621116477510202
LOSS train 0.17088600180222321 valid 0.21689165756106377
LOSS train 0.17088600180222321 valid 0.21542792677879333
LOSS train 0.17088600180222321 valid 0.21478806894559127
LOSS train 0.17088600180222321 valid 0.2142103910446167
LOSS train 0.17088600180222321 valid 0.2141531136419092
LOSS train 0.17088600180222321 valid 0.21317526441195916
LOSS train 0.17088600180222321 valid 0.2125886728366216
LOSS train 0.17088600180222321 valid 0.21183610443145998
LOSS train 0.17088600180222321 valid 0.21248921379446983
LOSS train 0.17088600180222321 valid 0.21125434745441785
LOSS train 0.17088600180222321 valid 0.2110192631097401
LOSS train 0.17088600180222321 valid 0.21163567858082907
LOSS train 0.17088600180222321 valid 0.21168160314361253
LOSS train 0.17088600180222321 valid 0.21090284755101074
LOSS train 0.17088600180222321 valid 0.21071683537018926
LOSS train 0.17088600180222321 valid 0.2102672102359625
LOSS train 0.17088600180222321 valid 0.21008325479924678
LOSS train 0.17088600180222321 valid 0.20994468433101002
LOSS train 0.17088600180222321 valid 0.21127325225444066
LOSS train 0.17088600180222321 valid 0.21245149886885353
LOSS train 0.17088600180222321 valid 0.2116971110755747
LOSS train 0.17088600180222321 valid 0.21130508846706814
LOSS train 0.17088600180222321 valid 0.20988590432249982
LOSS train 0.17088600180222321 valid 0.2094389612370349
LOSS train 0.17088600180222321 valid 0.21073269844055176
LOSS train 0.17088600180222321 valid 0.21056146798085193
LOSS train 0.17088600180222321 valid 0.21091895759105683
LOSS train 0.17088600180222321 valid 0.21059572346070232
LOSS train 0.17088600180222321 valid 0.2105492943754563
LOSS train 0.17088600180222321 valid 0.21147156548949908
LOSS train 0.17088600180222321 valid 0.21147029157038089
LOSS train 0.17088600180222321 valid 0.21171128397638148
LOSS train 0.17088600180222321 valid 0.21147524352584565
LOSS train 0.17088600180222321 valid 0.21096289785284744
LOSS train 0.17088600180222321 valid 0.2115093459343088
LOSS train 0.17088600180222321 valid 0.21165282645467984
LOSS train 0.17088600180222321 valid 0.21133633106946945
LOSS train 0.17088600180222321 valid 0.21128084327353805
LOSS train 0.17088600180222321 valid 0.21088914957738691
LOSS train 0.17088600180222321 valid 0.21101870163092537
LOSS train 0.17088600180222321 valid 0.21087360894307494
LOSS train 0.17088600180222321 valid 0.209970718392959
LOSS train 0.17088600180222321 valid 0.20979742818709576
LOSS train 0.17088600180222321 valid 0.21041268443883354
LOSS train 0.17088600180222321 valid 0.20988268799641552
LOSS train 0.17088600180222321 valid 0.21021180502746417
LOSS train 0.17088600180222321 valid 0.21050061881542206
LOSS train 0.17088600180222321 valid 0.21068469907196474
LOSS train 0.17088600180222321 valid 0.2112658019695017
LOSS train 0.17088600180222321 valid 0.21154429977887299
LOSS train 0.17088600180222321 valid 0.2114685264390868
LOSS train 0.17088600180222321 valid 0.21130008816719056
LOSS train 0.17088600180222321 valid 0.2114279846611776
LOSS train 0.17088600180222321 valid 0.2116710605559411
LOSS train 0.17088600180222321 valid 0.2115783353264515
LOSS train 0.17088600180222321 valid 0.21182871543908421
LOSS train 0.17088600180222321 valid 0.2115110967308283
LOSS train 0.17088600180222321 valid 0.21158302363790113
LOSS train 0.17088600180222321 valid 0.21188943033538213
LOSS train 0.17088600180222321 valid 0.2117208346544978
LOSS train 0.17088600180222321 valid 0.21155592976581483
LOSS train 0.17088600180222321 valid 0.21188292924095603
LOSS train 0.17088600180222321 valid 0.21200945782800054
LOSS train 0.17088600180222321 valid 0.21157057600459833
LOSS train 0.17088600180222321 valid 0.21148157441480595
LOSS train 0.17088600180222321 valid 0.2115297881739863
LOSS train 0.17088600180222321 valid 0.2114163597424825
LOSS train 0.17088600180222321 valid 0.21151977761106178
LOSS train 0.17088600180222321 valid 0.2115060439252335
LOSS train 0.17088600180222321 valid 0.21185102017335994
LOSS train 0.17088600180222321 valid 0.21210256393285506
LOSS train 0.17088600180222321 valid 0.21242673695087433
LOSS train 0.17088600180222321 valid 0.21325810362274447
LOSS train 0.17088600180222321 valid 0.21341096218099298
LOSS train 0.17088600180222321 valid 0.21386361973626272
LOSS train 0.17088600180222321 valid 0.2137536093442127
LOSS train 0.17088600180222321 valid 0.21389927610754966
LOSS train 0.17088600180222321 valid 0.21379971991081048
LOSS train 0.17088600180222321 valid 0.2141471922981973
LOSS train 0.17088600180222321 valid 0.21369256238335543
LOSS train 0.17088600180222321 valid 0.21353648989819562
LOSS train 0.17088600180222321 valid 0.21376009228683654
LOSS train 0.17088600180222321 valid 0.21382984308139333
LOSS train 0.17088600180222321 valid 0.21362935563671254
LOSS train 0.17088600180222321 valid 0.21364146995323677
LOSS train 0.17088600180222321 valid 0.21316583413596546
LOSS train 0.17088600180222321 valid 0.21298497305674988
LOSS train 0.17088600180222321 valid 0.2130375309838905
LOSS train 0.17088600180222321 valid 0.21310102048196963
LOSS train 0.17088600180222321 valid 0.21296975333078771
LOSS train 0.17088600180222321 valid 0.21313658304381788
LOSS train 0.17088600180222321 valid 0.2135179991307466
LOSS train 0.17088600180222321 valid 0.21347073863806396
LOSS train 0.17088600180222321 valid 0.21342459957823795
LOSS train 0.17088600180222321 valid 0.21338781019893743
LOSS train 0.17088600180222321 valid 0.2130460521253217
LOSS train 0.17088600180222321 valid 0.21293261361618837
LOSS train 0.17088600180222321 valid 0.21296093569806784
LOSS train 0.17088600180222321 valid 0.21309626432227308
LOSS train 0.17088600180222321 valid 0.21298710759577713
LOSS train 0.17088600180222321 valid 0.21306600842264392
LOSS train 0.17088600180222321 valid 0.21317952620983124
LOSS train 0.17088600180222321 valid 0.21311546046109425
LOSS train 0.17088600180222321 valid 0.21306065757443585
LOSS train 0.17088600180222321 valid 0.21290477423463017
LOSS train 0.17088600180222321 valid 0.21275872396406278
LOSS train 0.17088600180222321 valid 0.21247788392580472
LOSS train 0.17088600180222321 valid 0.21259974637104354
LOSS train 0.17088600180222321 valid 0.2127815806730227
LOSS train 0.17088600180222321 valid 0.21277541695232677
LOSS train 0.17088600180222321 valid 0.21284955995741175
LOSS train 0.17088600180222321 valid 0.2128328460234183
LOSS train 0.17088600180222321 valid 0.21292007199543364
LOSS train 0.17088600180222321 valid 0.21275244457443265
LOSS train 0.17088600180222321 valid 0.21241621878268063
LOSS train 0.17088600180222321 valid 0.21230238622469869
LOSS train 0.17088600180222321 valid 0.21219956119145666
LOSS train 0.17088600180222321 valid 0.21240674646188182
LOSS train 0.17088600180222321 valid 0.212703879660284
LOSS train 0.17088600180222321 valid 0.21275393789881594
LOSS train 0.17088600180222321 valid 0.21276131613800922
LOSS train 0.17088600180222321 valid 0.21254117838267622
LOSS train 0.17088600180222321 valid 0.2126571538105403
LOSS train 0.17088600180222321 valid 0.2126296857992808
LOSS train 0.17088600180222321 valid 0.2123824113325493
LOSS train 0.17088600180222321 valid 0.2125696912907914
LOSS train 0.17088600180222321 valid 0.212554605503877
LOSS train 0.17088600180222321 valid 0.21244543317137965
LOSS train 0.17088600180222321 valid 0.2124530190699979
LOSS train 0.17088600180222321 valid 0.2121983176352931
LOSS train 0.17088600180222321 valid 0.21222651362806172
LOSS train 0.17088600180222321 valid 0.2120363421978489
LOSS train 0.17088600180222321 valid 0.21225589265426
LOSS train 0.17088600180222321 valid 0.21208421572758135
LOSS train 0.17088600180222321 valid 0.2120217051875742
LOSS train 0.17088600180222321 valid 0.2123860479713236
LOSS train 0.17088600180222321 valid 0.2123521470464766
LOSS train 0.17088600180222321 valid 0.2124679835501665
LOSS train 0.17088600180222321 valid 0.21239644831713336
LOSS train 0.17088600180222321 valid 0.21215194774551627
LOSS train 0.17088600180222321 valid 0.21220806895232783
LOSS train 0.17088600180222321 valid 0.2123184196876757
LOSS train 0.17088600180222321 valid 0.21219887225383735
LOSS train 0.17088600180222321 valid 0.21222591194920912
LOSS train 0.17088600180222321 valid 0.2119608653620595
LOSS train 0.17088600180222321 valid 0.21182685252830122
LOSS train 0.17088600180222321 valid 0.2116655121831333
LOSS train 0.17088600180222321 valid 0.21153286787850117
LOSS train 0.17088600180222321 valid 0.21159031389411107
LOSS train 0.17088600180222321 valid 0.21165174064953204
LOSS train 0.17088600180222321 valid 0.21164663812552376
LOSS train 0.17088600180222321 valid 0.21135283648967743
LOSS train 0.17088600180222321 valid 0.2112890105186538
LOSS train 0.17088600180222321 valid 0.2112185204096433
LOSS train 0.17088600180222321 valid 0.21126036881730798
LOSS train 0.17088600180222321 valid 0.21121234858835208
LOSS train 0.17088600180222321 valid 0.21126204174425867
LOSS train 0.17088600180222321 valid 0.2111580028553694
LOSS train 0.17088600180222321 valid 0.2110777599634705
LOSS train 0.17088600180222321 valid 0.21118897240344292
LOSS train 0.17088600180222321 valid 0.21124769350432832
LOSS train 0.17088600180222321 valid 0.21129841554809262
LOSS train 0.17088600180222321 valid 0.21140553529864997
LOSS train 0.17088600180222321 valid 0.21135496113070829
LOSS train 0.17088600180222321 valid 0.21141553781134018
LOSS train 0.17088600180222321 valid 0.21118465065956116
LOSS train 0.17088600180222321 valid 0.21117572282489977
LOSS train 0.17088600180222321 valid 0.21128131971933456
LOSS train 0.17088600180222321 valid 0.21138184688364467
LOSS train 0.17088600180222321 valid 0.21118455584802776
LOSS train 0.17088600180222321 valid 0.21122890864450908
LOSS train 0.17088600180222321 valid 0.21114517480899125
LOSS train 0.17088600180222321 valid 0.21136375334189864
LOSS train 0.17088600180222321 valid 0.21145272731478446
LOSS train 0.17088600180222321 valid 0.2115559424414779
LOSS train 0.17088600180222321 valid 0.21167161454207933
LOSS train 0.17088600180222321 valid 0.21182145185768605
LOSS train 0.17088600180222321 valid 0.21180232351099082
LOSS train 0.17088600180222321 valid 0.21198252389336578
LOSS train 0.17088600180222321 valid 0.2121017692711553
LOSS train 0.17088600180222321 valid 0.21197218221484446
LOSS train 0.17088600180222321 valid 0.21181572922846167
LOSS train 0.17088600180222321 valid 0.21177340407394668
LOSS train 0.17088600180222321 valid 0.21163596925528153
LOSS train 0.17088600180222321 valid 0.21140767390338275
LOSS train 0.17088600180222321 valid 0.21147947662184682
LOSS train 0.17088600180222321 valid 0.2116851054486774
LOSS train 0.17088600180222321 valid 0.21185374528310874
LOSS train 0.17088600180222321 valid 0.211718387620629
LOSS train 0.17088600180222321 valid 0.21169264229810295
LOSS train 0.17088600180222321 valid 0.21160795330722756
LOSS train 0.17088600180222321 valid 0.21133788470612017
LOSS train 0.17088600180222321 valid 0.2111029004747117
LOSS train 0.17088600180222321 valid 0.21111639187357942
LOSS train 0.17088600180222321 valid 0.211076645881211
LOSS train 0.17088600180222321 valid 0.21110771310656037
LOSS train 0.17088600180222321 valid 0.21107237792827865
LOSS train 0.17088600180222321 valid 0.21101939745618206
LOSS train 0.17088600180222321 valid 0.21112195167455589
LOSS train 0.17088600180222321 valid 0.21103420920436158
LOSS train 0.17088600180222321 valid 0.2108925965481571
LOSS train 0.17088600180222321 valid 0.21062757896052467
LOSS train 0.17088600180222321 valid 0.21045723630000004
LOSS train 0.17088600180222321 valid 0.2104578782283262
LOSS train 0.17088600180222321 valid 0.21046738725220948
LOSS train 0.17088600180222321 valid 0.21038330551318207
LOSS train 0.17088600180222321 valid 0.2105039063355197
LOSS train 0.17088600180222321 valid 0.21048855091328228
LOSS train 0.17088600180222321 valid 0.21054423285712456
LOSS train 0.17088600180222321 valid 0.2107154716110025
LOSS train 0.17088600180222321 valid 0.21068730281713682
LOSS train 0.17088600180222321 valid 0.21066201316549424
LOSS train 0.17088600180222321 valid 0.21050339697276133
LOSS train 0.17088600180222321 valid 0.21052488508606762
LOSS train 0.17088600180222321 valid 0.21054924899289587
LOSS train 0.17088600180222321 valid 0.2105122328303349
LOSS train 0.17088600180222321 valid 0.21016550964365402
LOSS train 0.17088600180222321 valid 0.21003295462655824
LOSS train 0.17088600180222321 valid 0.21007039601152594
LOSS train 0.17088600180222321 valid 0.2101009979905415
LOSS train 0.17088600180222321 valid 0.21014838128304872
LOSS train 0.17088600180222321 valid 0.2103535080442623
LOSS train 0.17088600180222321 valid 0.21043779682822344
LOSS train 0.17088600180222321 valid 0.21051700883790067
LOSS train 0.17088600180222321 valid 0.21043326831873385
LOSS train 0.17088600180222321 valid 0.21035131123889403
LOSS train 0.17088600180222321 valid 0.21046114164590835
LOSS train 0.17088600180222321 valid 0.21051628080259757
LOSS train 0.17088600180222321 valid 0.21058541187454785
LOSS train 0.17088600180222321 valid 0.2105050967498259
LOSS train 0.17088600180222321 valid 0.21054626778354796
LOSS train 0.17088600180222321 valid 0.21055698844732024
LOSS train 0.17088600180222321 valid 0.21056050329934806
LOSS train 0.17088600180222321 valid 0.21049774910689328
LOSS train 0.17088600180222321 valid 0.21056925511175348
LOSS train 0.17088600180222321 valid 0.21054108619229675
LOSS train 0.17088600180222321 valid 0.21041840051229183
LOSS train 0.17088600180222321 valid 0.21041024758897978
LOSS train 0.17088600180222321 valid 0.21064221654229492
LOSS train 0.17088600180222321 valid 0.21054241174527447
LOSS train 0.17088600180222321 valid 0.21060816983156133
LOSS train 0.17088600180222321 valid 0.2106376716550791
LOSS train 0.17088600180222321 valid 0.21068581182481652
LOSS train 0.17088600180222321 valid 0.2106957043266475
LOSS train 0.17088600180222321 valid 0.21076450535832947
LOSS train 0.17088600180222321 valid 0.2108127913510489
LOSS train 0.17088600180222321 valid 0.2106871646311548
LOSS train 0.17088600180222321 valid 0.21063046245337413
LOSS train 0.17088600180222321 valid 0.21046273541801117
LOSS train 0.17088600180222321 valid 0.21036948749433942
LOSS train 0.17088600180222321 valid 0.2103298091105301
LOSS train 0.17088600180222321 valid 0.21041823723099448
LOSS train 0.17088600180222321 valid 0.21053715811475462
LOSS train 0.17088600180222321 valid 0.21047519034426995
LOSS train 0.17088600180222321 valid 0.21045400529242247
LOSS train 0.17088600180222321 valid 0.2105269724216085
LOSS train 0.17088600180222321 valid 0.21035274155437947
LOSS train 0.17088600180222321 valid 0.21023507559426738
LOSS train 0.17088600180222321 valid 0.21022242297094765
LOSS train 0.17088600180222321 valid 0.21023381814729197
LOSS train 0.17088600180222321 valid 0.21009782192782617
LOSS train 0.17088600180222321 valid 0.21017891351591078
LOSS train 0.17088600180222321 valid 0.21021408535592206
LOSS train 0.17088600180222321 valid 0.21028765885464407
LOSS train 0.17088600180222321 valid 0.21022294358246857
LOSS train 0.17088600180222321 valid 0.21018488411259897
LOSS train 0.17088600180222321 valid 0.21011426382023712
LOSS train 0.17088600180222321 valid 0.21006618168755495
LOSS train 0.17088600180222321 valid 0.21003621321630805
LOSS train 0.17088600180222321 valid 0.21002633854390818
LOSS train 0.17088600180222321 valid 0.21001416130536268
LOSS train 0.17088600180222321 valid 0.21001809628333076
LOSS train 0.17088600180222321 valid 0.2100727556122316
LOSS train 0.17088600180222321 valid 0.21007864523415615
LOSS train 0.17088600180222321 valid 0.21014845836282575
LOSS train 0.17088600180222321 valid 0.21015095302093786
LOSS train 0.17088600180222321 valid 0.2101771855354309
LOSS train 0.17088600180222321 valid 0.21015598761084864
LOSS train 0.17088600180222321 valid 0.21016708929215047
LOSS train 0.17088600180222321 valid 0.2101912800431645
LOSS train 0.17088600180222321 valid 0.21016857400536537
LOSS train 0.17088600180222321 valid 0.21009349417491038
LOSS train 0.17088600180222321 valid 0.2102164700627327
LOSS train 0.17088600180222321 valid 0.2101272358179869
LOSS train 0.17088600180222321 valid 0.21022660691629758
LOSS train 0.17088600180222321 valid 0.21024445557671456
LOSS train 0.17088600180222321 valid 0.21026484081822058
LOSS train 0.17088600180222321 valid 0.21030491724658243
LOSS train 0.17088600180222321 valid 0.21025170581654096
LOSS train 0.17088600180222321 valid 0.21028436739414264
LOSS train 0.17088600180222321 valid 0.21029056115135267
LOSS train 0.17088600180222321 valid 0.2103067441119088
LOSS train 0.17088600180222321 valid 0.21031782993032963
LOSS train 0.17088600180222321 valid 0.21029439822356408
LOSS train 0.17088600180222321 valid 0.21034623837133623
LOSS train 0.17088600180222321 valid 0.2103612260852114
LOSS train 0.17088600180222321 valid 0.21025776420719922
LOSS train 0.17088600180222321 valid 0.21024400084003853
LOSS train 0.17088600180222321 valid 0.21021098958224244
LOSS train 0.17088600180222321 valid 0.21015422147678517
LOSS train 0.17088600180222321 valid 0.21014646325766304
LOSS train 0.17088600180222321 valid 0.20997455367675194
LOSS train 0.17088600180222321 valid 0.2100286834810409
LOSS train 0.17088600180222321 valid 0.209905810221255
LOSS train 0.17088600180222321 valid 0.2099143248051405
LOSS train 0.17088600180222321 valid 0.20996345987254725
LOSS train 0.17088600180222321 valid 0.21002595781376868
LOSS train 0.17088600180222321 valid 0.2099668932825417
LOSS train 0.17088600180222321 valid 0.20994550734758377
LOSS train 0.17088600180222321 valid 0.20996580391495792
LOSS train 0.17088600180222321 valid 0.20992728099673094
LOSS train 0.17088600180222321 valid 0.20982017663877403
LOSS train 0.17088600180222321 valid 0.20974591609445356
LOSS train 0.17088600180222321 valid 0.2099137432232101
LOSS train 0.17088600180222321 valid 0.209842086499612
LOSS train 0.17088600180222321 valid 0.20973791775450243
LOSS train 0.17088600180222321 valid 0.209762751135756
LOSS train 0.17088600180222321 valid 0.2098101237652239
LOSS train 0.17088600180222321 valid 0.20982844744160858
LOSS train 0.17088600180222321 valid 0.20972656517265142
LOSS train 0.17088600180222321 valid 0.20978283630900604
LOSS train 0.17088600180222321 valid 0.2098295180261999
LOSS train 0.17088600180222321 valid 0.20975385080872244
LOSS train 0.17088600180222321 valid 0.20972554158065093
LOSS train 0.17088600180222321 valid 0.20967482479995694
LOSS train 0.17088600180222321 valid 0.2096277439918764
LOSS train 0.17088600180222321 valid 0.20964779841048378
LOSS train 0.17088600180222321 valid 0.20968893667062125
LOSS train 0.17088600180222321 valid 0.2097138947891918
LOSS train 0.17088600180222321 valid 0.20975215880796524
LOSS train 0.17088600180222321 valid 0.20966190874239818
LOSS train 0.17088600180222321 valid 0.20955731671460917
LOSS train 0.17088600180222321 valid 0.20954238083422855
LOSS train 0.17088600180222321 valid 0.20956140844260945
LOSS train 0.17088600180222321 valid 0.20965547497545542
LOSS train 0.17088600180222321 valid 0.20965770580144313
LOSS train 0.17088600180222321 valid 0.2098896740625302
LOSS train 0.17088600180222321 valid 0.20983040592842156
LOSS train 0.17088600180222321 valid 0.20993118984264564
LOSS train 0.17088600180222321 valid 0.2099676499189424
LOSS train 0.17088600180222321 valid 0.20988386193965816
LOSS train 0.17088600180222321 valid 0.20988863057469667
LOSS train 0.17088600180222321 valid 0.2098805364526686
LOSS train 0.17088600180222321 valid 0.20992743461430885
LOSS train 0.17088600180222321 valid 0.2098293007634904
LOSS train 0.17088600180222321 valid 0.20993078966450884
EPOCH 3:
  batch 1 loss: 0.17516909539699554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.15344040095806122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1521629293759664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.15215758234262466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.15499804317951202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.15383048603932062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1539991455418723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.15084282495081425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.15243199467658997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.15072990953922272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.15159667351029135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.15338748941818872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.15336579199020678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1515457747238023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.15093977749347687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.15213008038699627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1536845591138391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.15443776133987638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1552097311145381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1556254431605339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.15615453961349668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.15580823407931763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.15418393294448438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1540147950872779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1542336729168892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.15558438547528708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.15616005410750708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1566151063889265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.15752399866950922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.157189496109883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.15670606854461855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1557864008937031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.15562742564714316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.156436019741437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1571487233042717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.15815074596967962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.15860237402690425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1578531243691319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.15709839054407218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1574781769886613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.15734989050684905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.15733859847698892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.15715569482986316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15736533989283172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.15715172274245157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.15692520967644194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1569145231487903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.15740066533908248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.15779785857516893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1579172445833683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.15766475758716172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.15719483950390264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1572127751303169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.15728849213984278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.15736235989765687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.15761875055198157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1578077267397914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.15749140173710627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1580379811131348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15865896306931973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15914910677515093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15882504743433767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1584721932572032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15856227849144489
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.15853992184767357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.15859433274829027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.15876620996799043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.15890449559425607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1585899834399638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15830467737146786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.15839321061339176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15804930393480593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15834955473060477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1587212561956934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.15861090709765752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1586413788364122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15877962180159308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15897772317895523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.15907846740152262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15911836335435509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1591430588820834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15926650629901304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.15903702333389994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15927863573389395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.15934838994460948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1589757948247499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.15906777558313018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1588451009751721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15892075211479423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1589922808110714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.15906341382107891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15894445623068706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1588676378771823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15888362750411034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.15870305856591777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.15867409614535669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1586341823345607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15853826435548918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15881396175333948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15882774017751217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15903464542462095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.15907400839176833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15914563726163605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1593468437830989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1592152475601151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15922178798970185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1592520555463907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15915386620219107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15914203998965953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15929755046963692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.15920591240262125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15897448939670408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.15884099683139177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15894110860270366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1590633332081463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1591252307568131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1592137455048724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.159036535769701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15915341831806326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.15904744335760673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15919992567832805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15929040379944395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15924072550321983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1591716344438253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.15938694471120834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.15939974625195777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15955220121802308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15942423819797114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15925867341516553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15910410944085854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.15902976276537845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.15877323027587298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15882028342413723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1586156916151296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15852373821867838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.15874804287929745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15878120733656154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15866384865797084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15872166102095472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1584291926452092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.15847564589047264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1585239092229118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15852415051076796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15837925444874498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15855876001818428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1586274506702815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1584191394298255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15852347610367312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15866116439336098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.15863873451948166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.15871233853283306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15880748109990045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1589246699038674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15882608188050135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1587259191659189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1586690822090858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1586025969427862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15855035893147504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.15853399094545617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1584620177745819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.15851468199528523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15867240441802108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15873140224649862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.15867965650267718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15849165663574682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15837785018136702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1583559073195486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15817575413911117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15806141345811314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1581553701968754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15822378123364253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.15811431971053744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15803963738369803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1580628495791863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15811794689723424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.15813519560139289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15849587060300643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15845008052132104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15846492133660023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1584440416759915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15840895539818547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.15843428106425883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15849246019548405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1584402657235446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1584233676259582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.15843322480558067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1585330508928248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.15842975382792188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15846422022928006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.15848262419826106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15841713992400944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.15859628088461855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.15861200267169143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15860560129291004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.15856133982157095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1585104938368408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.15829034206377068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15823335934317473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15811495636426023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15824551466852427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.1582073977411683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.15815797592006106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15819140149307956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15818968937531405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15814936869755025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.15823135326064905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15813937844429615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.15799608522166425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15784258480990332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1579412610403129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.15792139414786163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15811366415670458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15824075368508486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.15816607740577135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.158092356594496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1580696115270257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15808068693096186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15809655514195425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.158176309816097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15797695483673702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15798269111106839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1579751552225233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.15803945011087597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.15802612215546624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15810103668106926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.15799411867572144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.15810429669161724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15794656053185463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1578254275446896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1578774314859639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1579160685147042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.15790978581484022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.15786001636234986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1578392169287062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1577178605693452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15773322666853162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.15768432177068814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15777975151769252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.15767438578805165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1576267952720324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.15762274727781778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15762960048746472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15759971284081417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.15757185635996646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.15755366743827354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.15738569760346802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.15739451737900978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15748026897950518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15748406135772605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.15751809629797936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1574941332418605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.15748982778972104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1574386626540908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.15734724932181554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1574222230151588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.15744069198262878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15732898615329646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.15727965265165927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15732656842147982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.15732285583821626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.15748732274405344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15754508025204864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15742873270135416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.157470270003559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.15751103244300158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.15739457216814048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.15743174599239443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1574974400060835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.15740478783845901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15747923048006163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.15739533687319704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.15730625366353812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.15722525859381253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15726608883616697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.15716644008051264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.15713461469588935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.15723345057521057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15714764544110504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.15707689905572536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.15712281393685512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15705296012748604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1570960504418992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15704318962648986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.15704833466926932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15717318013571857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.15716581273224803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.15723428215818538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.15712487974411082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.15700099643023369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.15697501694847799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.15697484417367227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.15698179156098463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.15710152332184665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.15706601564069184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15712076327053168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15717581777858572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1571131664946023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.15722354448981732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.15724994354323799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.15716523530582588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.15709079898572048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15696992887961153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.15699880459521076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1570230771444346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15706008858368045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.15702476790722678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.15697749580350684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.15696306908866028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15704776182722505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15700922920819252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15695449973417633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1569030802601423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.15696639055832506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.15697182881034863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.15709190042245955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1570919558311565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.15712978848335496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.1571232240717366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1571296694510409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1570723335724324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15706783769843735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.15716906919242432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1570669723092218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15700855602820715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15709580352673164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15707124842822187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15707350533672065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.15707459100862828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.15714034936348356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.15707448635137441
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1570532904020609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1571011566613094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15715656552586826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.15714536535882664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.15709556550232331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15718288041119063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.15718926851933362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15713587835342926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15704555053450717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.15707561522722244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15706865305949516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1570373439823675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1570135740775061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15699358280141687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15705501415591308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15702310557654828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15707209878077769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.15707910664636512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.15699113700485504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1570221906900406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.15707030370194688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.15702563909475098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.15703301929549524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1571291770133595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15711172483336758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1570543058300286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15697040853380156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.15692238455544638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.15688071227671377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15687162801623344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.15683732811268677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15691236637079914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.15689109430004414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.156855888910346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.15687881936765696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.15689068727317404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.15681443694339461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15689759486881288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.15691534431807716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.15698796329466072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.15701562547780112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.15699238266034793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.15692248261326439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15698535804123803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.15695468612511954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1569509782848206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.15697606449418106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15697106336632732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15704874269723262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15709878206253053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15707940481153373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.157074605256163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.15707170013970556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.15706781740300357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.15700220825610223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.15702109204827194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.15691055078300087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.15695052653474292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1569165188081773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.15692680373023718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.15697646165823997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.15698612859586672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15697240797238798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.15693752522471593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.15698412239928788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1569366031937828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.156933009605714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.15692244307255027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.15692551338807084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15692528067156672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15698428602512934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.15702748630399727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1569997655879475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15697631341322224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1569441755796656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.15692145658170648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.156904865962428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.15683220945956075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1568565569945536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15683468236792378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.15691026847655465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1569079020539823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1568228275882707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.15672489943135764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1567203149738082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1567660287165871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1566641214940188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15660682484578858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15658837041166346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15654944525588127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1565561805300928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15661389412473165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.15656616540524412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15659892559051514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1565912139415741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.15653466443780442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15651883830510482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15655510597557665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15653215272304338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15655228173316912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1565412180952019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.15662664143989483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.15661512927838342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.15666452898270525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.15666100132739408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.15672166751475508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.156755885659148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.15678399634551785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15678127704014266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1567965815013105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.15679278101375044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1567575529015442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.15677972655936773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15673533132350123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1566875007715118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.1566433929274435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1566148153954171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.15661927691793867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1566104582264057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15654446442921957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15654366441946602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.15650777001164656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.15647168723952692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.15637714990172616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15630617551096193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.156288193473429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.15626933247959587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.15620932474276905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.15613493352544075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.15617146650734154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15617911507923019
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.15613355881078936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.15613706323034562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.15619236869930192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15614091834073426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15616706186353904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.15621205877185634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15621441793747437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15622173742190607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15613349042040237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1561716371977152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1561391669625448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1561391669625448 valid 0.1776207685470581
LOSS train 0.1561391669625448 valid 0.2000390738248825
LOSS train 0.1561391669625448 valid 0.21332088112831116
LOSS train 0.1561391669625448 valid 0.21042920276522636
LOSS train 0.1561391669625448 valid 0.21143434345722198
LOSS train 0.1561391669625448 valid 0.2160220369696617
LOSS train 0.1561391669625448 valid 0.21028512290545873
LOSS train 0.1561391669625448 valid 0.20955656096339226
LOSS train 0.1561391669625448 valid 0.20843094090620676
LOSS train 0.1561391669625448 valid 0.20976274013519286
LOSS train 0.1561391669625448 valid 0.20620886304161765
LOSS train 0.1561391669625448 valid 0.20811523124575615
LOSS train 0.1561391669625448 valid 0.20703612611843988
LOSS train 0.1561391669625448 valid 0.20772547594138555
LOSS train 0.1561391669625448 valid 0.20641610622406006
LOSS train 0.1561391669625448 valid 0.20697933435440063
LOSS train 0.1561391669625448 valid 0.20670435183188496
LOSS train 0.1561391669625448 valid 0.2064473016394509
LOSS train 0.1561391669625448 valid 0.20825700148155815
LOSS train 0.1561391669625448 valid 0.20791733935475348
LOSS train 0.1561391669625448 valid 0.20843189741883958
LOSS train 0.1561391669625448 valid 0.20696293156255374
LOSS train 0.1561391669625448 valid 0.20538097360859747
LOSS train 0.1561391669625448 valid 0.20629694623251757
LOSS train 0.1561391669625448 valid 0.2048269182443619
LOSS train 0.1561391669625448 valid 0.2044937679400811
LOSS train 0.1561391669625448 valid 0.20404022876863126
LOSS train 0.1561391669625448 valid 0.20408440647380693
LOSS train 0.1561391669625448 valid 0.2030220329761505
LOSS train 0.1561391669625448 valid 0.20232757727305095
LOSS train 0.1561391669625448 valid 0.2015843415452588
LOSS train 0.1561391669625448 valid 0.2022597142495215
LOSS train 0.1561391669625448 valid 0.20111864579446387
LOSS train 0.1561391669625448 valid 0.20063116313780055
LOSS train 0.1561391669625448 valid 0.20124112367630004
LOSS train 0.1561391669625448 valid 0.20141760839356315
LOSS train 0.1561391669625448 valid 0.20055622667879672
LOSS train 0.1561391669625448 valid 0.20027947974832436
LOSS train 0.1561391669625448 valid 0.19978051422498164
LOSS train 0.1561391669625448 valid 0.1996021829545498
LOSS train 0.1561391669625448 valid 0.19941425105420554
LOSS train 0.1561391669625448 valid 0.2007813042118436
LOSS train 0.1561391669625448 valid 0.20183725412501846
LOSS train 0.1561391669625448 valid 0.20095852186734026
LOSS train 0.1561391669625448 valid 0.20046362810664706
LOSS train 0.1561391669625448 valid 0.19906098557555157
LOSS train 0.1561391669625448 valid 0.19855650878967124
LOSS train 0.1561391669625448 valid 0.199902202313145
LOSS train 0.1561391669625448 valid 0.19978548616779093
LOSS train 0.1561391669625448 valid 0.20020983040332793
LOSS train 0.1561391669625448 valid 0.19989188278422637
LOSS train 0.1561391669625448 valid 0.19988107738586572
LOSS train 0.1561391669625448 valid 0.20075689790383824
LOSS train 0.1561391669625448 valid 0.20080236621476985
LOSS train 0.1561391669625448 valid 0.20102667049928144
LOSS train 0.1561391669625448 valid 0.20083996254418576
LOSS train 0.1561391669625448 valid 0.20026275829264992
LOSS train 0.1561391669625448 valid 0.20094106372060447
LOSS train 0.1561391669625448 valid 0.2010709789849944
LOSS train 0.1561391669625448 valid 0.20080214962363244
LOSS train 0.1561391669625448 valid 0.20070740946980772
LOSS train 0.1561391669625448 valid 0.20034567939658318
LOSS train 0.1561391669625448 valid 0.20040269692738852
LOSS train 0.1561391669625448 valid 0.20031089475378394
LOSS train 0.1561391669625448 valid 0.19950117079111246
LOSS train 0.1561391669625448 valid 0.1993610170302969
LOSS train 0.1561391669625448 valid 0.19999414882553157
LOSS train 0.1561391669625448 valid 0.19936859826831257
LOSS train 0.1561391669625448 valid 0.1996783093697783
LOSS train 0.1561391669625448 valid 0.2000189393758774
LOSS train 0.1561391669625448 valid 0.20006668924445836
LOSS train 0.1561391669625448 valid 0.20059800727499855
LOSS train 0.1561391669625448 valid 0.2009128842451801
LOSS train 0.1561391669625448 valid 0.20088969895968567
LOSS train 0.1561391669625448 valid 0.2006756019592285
LOSS train 0.1561391669625448 valid 0.20079571283177325
LOSS train 0.1561391669625448 valid 0.20095469599420374
LOSS train 0.1561391669625448 valid 0.20090593970738924
LOSS train 0.1561391669625448 valid 0.20118221357653412
LOSS train 0.1561391669625448 valid 0.20093486923724413
LOSS train 0.1561391669625448 valid 0.20105433298481834
LOSS train 0.1561391669625448 valid 0.20131994384091076
LOSS train 0.1561391669625448 valid 0.20114027502306972
LOSS train 0.1561391669625448 valid 0.20101722418552354
LOSS train 0.1561391669625448 valid 0.20129109603517195
LOSS train 0.1561391669625448 valid 0.20139018792745678
LOSS train 0.1561391669625448 valid 0.20099060357302084
LOSS train 0.1561391669625448 valid 0.2008914163505489
LOSS train 0.1561391669625448 valid 0.200978911492262
LOSS train 0.1561391669625448 valid 0.20085802028576533
LOSS train 0.1561391669625448 valid 0.20094810344360686
LOSS train 0.1561391669625448 valid 0.20096232142785322
LOSS train 0.1561391669625448 valid 0.20131993470012502
LOSS train 0.1561391669625448 valid 0.20159719377122026
LOSS train 0.1561391669625448 valid 0.20197831266804744
LOSS train 0.1561391669625448 valid 0.2028624148418506
LOSS train 0.1561391669625448 valid 0.20300614557315394
LOSS train 0.1561391669625448 valid 0.20345032914560668
LOSS train 0.1561391669625448 valid 0.20335291913061432
LOSS train 0.1561391669625448 valid 0.2035663628578186
LOSS train 0.1561391669625448 valid 0.20346598120609133
LOSS train 0.1561391669625448 valid 0.20380615165420607
LOSS train 0.1561391669625448 valid 0.2033731690020237
LOSS train 0.1561391669625448 valid 0.20320285613147113
LOSS train 0.1561391669625448 valid 0.20339643032777877
LOSS train 0.1561391669625448 valid 0.20341968522319254
LOSS train 0.1561391669625448 valid 0.20325953784947084
LOSS train 0.1561391669625448 valid 0.20329296050800216
LOSS train 0.1561391669625448 valid 0.20275841434614375
LOSS train 0.1561391669625448 valid 0.20254173509099266
LOSS train 0.1561391669625448 valid 0.20259692566888826
LOSS train 0.1561391669625448 valid 0.20267288825873817
LOSS train 0.1561391669625448 valid 0.20251738405333156
LOSS train 0.1561391669625448 valid 0.20269038802699038
LOSS train 0.1561391669625448 valid 0.20304686802884805
LOSS train 0.1561391669625448 valid 0.20301218086789394
LOSS train 0.1561391669625448 valid 0.20300889817567971
LOSS train 0.1561391669625448 valid 0.20297948814044564
LOSS train 0.1561391669625448 valid 0.20270994142824864
LOSS train 0.1561391669625448 valid 0.20265587915976843
LOSS train 0.1561391669625448 valid 0.20270708671286086
LOSS train 0.1561391669625448 valid 0.20281483648253268
LOSS train 0.1561391669625448 valid 0.20271734396616617
LOSS train 0.1561391669625448 valid 0.20280444598005665
LOSS train 0.1561391669625448 valid 0.202953946352005
LOSS train 0.1561391669625448 valid 0.20287911558435076
LOSS train 0.1561391669625448 valid 0.20278681650405794
LOSS train 0.1561391669625448 valid 0.20265275554265827
LOSS train 0.1561391669625448 valid 0.20251197951246602
LOSS train 0.1561391669625448 valid 0.20221665386970228
LOSS train 0.1561391669625448 valid 0.20230805441623426
LOSS train 0.1561391669625448 valid 0.20249915766445073
LOSS train 0.1561391669625448 valid 0.20247186003322887
LOSS train 0.1561391669625448 valid 0.20252456126818016
LOSS train 0.1561391669625448 valid 0.2025100142867477
LOSS train 0.1561391669625448 valid 0.20264049498912165
LOSS train 0.1561391669625448 valid 0.20246733420521673
LOSS train 0.1561391669625448 valid 0.20212197681699973
LOSS train 0.1561391669625448 valid 0.2020197970618447
LOSS train 0.1561391669625448 valid 0.20190681804503713
LOSS train 0.1561391669625448 valid 0.20215476034803592
LOSS train 0.1561391669625448 valid 0.20241743483593766
LOSS train 0.1561391669625448 valid 0.20247507918547916
LOSS train 0.1561391669625448 valid 0.20246660564508703
LOSS train 0.1561391669625448 valid 0.20227830594983595
LOSS train 0.1561391669625448 valid 0.2023205598943854
LOSS train 0.1561391669625448 valid 0.20228519599859407
LOSS train 0.1561391669625448 valid 0.20206105839964505
LOSS train 0.1561391669625448 valid 0.20225748279750747
LOSS train 0.1561391669625448 valid 0.20220799922943114
LOSS train 0.1561391669625448 valid 0.20211617086107367
LOSS train 0.1561391669625448 valid 0.20211508340741458
LOSS train 0.1561391669625448 valid 0.20184750864708345
LOSS train 0.1561391669625448 valid 0.20185035118808994
LOSS train 0.1561391669625448 valid 0.2016506069129513
LOSS train 0.1561391669625448 valid 0.20186555337829468
LOSS train 0.1561391669625448 valid 0.20169441762623513
LOSS train 0.1561391669625448 valid 0.20164026131358329
LOSS train 0.1561391669625448 valid 0.20196352158702394
LOSS train 0.1561391669625448 valid 0.2019161249510944
LOSS train 0.1561391669625448 valid 0.2020810230733445
LOSS train 0.1561391669625448 valid 0.20203493590708133
LOSS train 0.1561391669625448 valid 0.20179772413581426
LOSS train 0.1561391669625448 valid 0.2018327166939654
LOSS train 0.1561391669625448 valid 0.20193952574874416
LOSS train 0.1561391669625448 valid 0.201849062848522
LOSS train 0.1561391669625448 valid 0.2019037946077164
LOSS train 0.1561391669625448 valid 0.20162560852865377
LOSS train 0.1561391669625448 valid 0.2014624674821041
LOSS train 0.1561391669625448 valid 0.20131582354798036
LOSS train 0.1561391669625448 valid 0.20117292261263084
LOSS train 0.1561391669625448 valid 0.20123186378284943
LOSS train 0.1561391669625448 valid 0.20131428169377277
LOSS train 0.1561391669625448 valid 0.20132101721119608
LOSS train 0.1561391669625448 valid 0.2010363062790462
LOSS train 0.1561391669625448 valid 0.2010036837309599
LOSS train 0.1561391669625448 valid 0.20092722108471864
LOSS train 0.1561391669625448 valid 0.20097321765811255
LOSS train 0.1561391669625448 valid 0.20096639189973223
LOSS train 0.1561391669625448 valid 0.2009971794154909
LOSS train 0.1561391669625448 valid 0.20086821543248318
LOSS train 0.1561391669625448 valid 0.2007803894006289
LOSS train 0.1561391669625448 valid 0.2008969446674722
LOSS train 0.1561391669625448 valid 0.20094781725302988
LOSS train 0.1561391669625448 valid 0.20098977290295267
LOSS train 0.1561391669625448 valid 0.20110167474836432
LOSS train 0.1561391669625448 valid 0.20105759592298517
LOSS train 0.1561391669625448 valid 0.20107898298413196
LOSS train 0.1561391669625448 valid 0.2008127203536412
LOSS train 0.1561391669625448 valid 0.20082397288397738
LOSS train 0.1561391669625448 valid 0.20093070477715338
LOSS train 0.1561391669625448 valid 0.20097129116766155
LOSS train 0.1561391669625448 valid 0.20077810864992093
LOSS train 0.1561391669625448 valid 0.20083374215155533
LOSS train 0.1561391669625448 valid 0.20073157984476822
LOSS train 0.1561391669625448 valid 0.2009342720313948
LOSS train 0.1561391669625448 valid 0.20104943147770643
LOSS train 0.1561391669625448 valid 0.20114899599792982
LOSS train 0.1561391669625448 valid 0.20125025735428584
LOSS train 0.1561391669625448 valid 0.20137675538659094
LOSS train 0.1561391669625448 valid 0.20138418652228454
LOSS train 0.1561391669625448 valid 0.2015694500786243
LOSS train 0.1561391669625448 valid 0.20171008805923274
LOSS train 0.1561391669625448 valid 0.2015658998314072
LOSS train 0.1561391669625448 valid 0.20142256181414533
LOSS train 0.1561391669625448 valid 0.20138288723322953
LOSS train 0.1561391669625448 valid 0.20124815188456271
LOSS train 0.1561391669625448 valid 0.20104177539738324
LOSS train 0.1561391669625448 valid 0.2010993752182956
LOSS train 0.1561391669625448 valid 0.20131845516817912
LOSS train 0.1561391669625448 valid 0.20152909912486777
LOSS train 0.1561391669625448 valid 0.20139073039282043
LOSS train 0.1561391669625448 valid 0.20136805998047752
LOSS train 0.1561391669625448 valid 0.20132050015658975
LOSS train 0.1561391669625448 valid 0.2010468379702679
LOSS train 0.1561391669625448 valid 0.20081685304089827
LOSS train 0.1561391669625448 valid 0.2008614135640008
LOSS train 0.1561391669625448 valid 0.20081839033769905
LOSS train 0.1561391669625448 valid 0.20087874861068378
LOSS train 0.1561391669625448 valid 0.20083986818790436
LOSS train 0.1561391669625448 valid 0.200794354216006
LOSS train 0.1561391669625448 valid 0.2009123065047436
LOSS train 0.1561391669625448 valid 0.2008083534213994
LOSS train 0.1561391669625448 valid 0.20067413224439537
LOSS train 0.1561391669625448 valid 0.20040540675322216
LOSS train 0.1561391669625448 valid 0.2002401378961791
LOSS train 0.1561391669625448 valid 0.2002382259405657
LOSS train 0.1561391669625448 valid 0.20024021257433974
LOSS train 0.1561391669625448 valid 0.20017522729640444
LOSS train 0.1561391669625448 valid 0.2002902251870736
LOSS train 0.1561391669625448 valid 0.20030119434579627
LOSS train 0.1561391669625448 valid 0.20038975788087682
LOSS train 0.1561391669625448 valid 0.2005617442049182
LOSS train 0.1561391669625448 valid 0.20054226203097236
LOSS train 0.1561391669625448 valid 0.20051349350746642
LOSS train 0.1561391669625448 valid 0.20033997802411097
LOSS train 0.1561391669625448 valid 0.20035562076397587
LOSS train 0.1561391669625448 valid 0.20039554071777008
LOSS train 0.1561391669625448 valid 0.20036758806665572
LOSS train 0.1561391669625448 valid 0.20001018953820068
LOSS train 0.1561391669625448 valid 0.19988604427867906
LOSS train 0.1561391669625448 valid 0.19994602346223248
LOSS train 0.1561391669625448 valid 0.19999784568454992
LOSS train 0.1561391669625448 valid 0.20004939141332126
LOSS train 0.1561391669625448 valid 0.20025876152272126
LOSS train 0.1561391669625448 valid 0.2003540215574629
LOSS train 0.1561391669625448 valid 0.2004051088442204
LOSS train 0.1561391669625448 valid 0.20030377348584513
LOSS train 0.1561391669625448 valid 0.20020704438169318
LOSS train 0.1561391669625448 valid 0.20032123941183091
LOSS train 0.1561391669625448 valid 0.20039188784669595
LOSS train 0.1561391669625448 valid 0.20044382336357283
LOSS train 0.1561391669625448 valid 0.2003538180363508
LOSS train 0.1561391669625448 valid 0.20041134355105752
LOSS train 0.1561391669625448 valid 0.20039545204125198
LOSS train 0.1561391669625448 valid 0.2003936228575185
LOSS train 0.1561391669625448 valid 0.200314104441075
LOSS train 0.1561391669625448 valid 0.20040957467962606
LOSS train 0.1561391669625448 valid 0.2003649400698172
LOSS train 0.1561391669625448 valid 0.200257218342561
LOSS train 0.1561391669625448 valid 0.2002491076901498
LOSS train 0.1561391669625448 valid 0.20048766003082727
LOSS train 0.1561391669625448 valid 0.20036688424335233
LOSS train 0.1561391669625448 valid 0.20041365492524524
LOSS train 0.1561391669625448 valid 0.20044214005740182
LOSS train 0.1561391669625448 valid 0.20049792662599034
LOSS train 0.1561391669625448 valid 0.20048666290576092
LOSS train 0.1561391669625448 valid 0.2005447554054545
LOSS train 0.1561391669625448 valid 0.2006190527770599
LOSS train 0.1561391669625448 valid 0.20048958035530867
LOSS train 0.1561391669625448 valid 0.20042210305968775
LOSS train 0.1561391669625448 valid 0.20026883077533805
LOSS train 0.1561391669625448 valid 0.20014435107454712
LOSS train 0.1561391669625448 valid 0.20009772958111588
LOSS train 0.1561391669625448 valid 0.20019571867856112
LOSS train 0.1561391669625448 valid 0.20030510571339857
LOSS train 0.1561391669625448 valid 0.20024231780952495
LOSS train 0.1561391669625448 valid 0.20022427976774654
LOSS train 0.1561391669625448 valid 0.20029463653709725
LOSS train 0.1561391669625448 valid 0.20010724695665497
LOSS train 0.1561391669625448 valid 0.19999015150969562
LOSS train 0.1561391669625448 valid 0.19997566579081488
LOSS train 0.1561391669625448 valid 0.1999645488527554
LOSS train 0.1561391669625448 valid 0.19982357984277563
LOSS train 0.1561391669625448 valid 0.19991535915617356
LOSS train 0.1561391669625448 valid 0.1999369957126104
LOSS train 0.1561391669625448 valid 0.20003055607192607
LOSS train 0.1561391669625448 valid 0.19997518437190187
LOSS train 0.1561391669625448 valid 0.19993276100051444
LOSS train 0.1561391669625448 valid 0.19984366477563464
LOSS train 0.1561391669625448 valid 0.19979735527866074
LOSS train 0.1561391669625448 valid 0.19977086409926414
LOSS train 0.1561391669625448 valid 0.19974770241833384
LOSS train 0.1561391669625448 valid 0.19972962445142317
LOSS train 0.1561391669625448 valid 0.19973422662686494
LOSS train 0.1561391669625448 valid 0.1997870436391315
LOSS train 0.1561391669625448 valid 0.19979898119815673
LOSS train 0.1561391669625448 valid 0.19984364854609407
LOSS train 0.1561391669625448 valid 0.19985126367580133
LOSS train 0.1561391669625448 valid 0.19987044483423233
LOSS train 0.1561391669625448 valid 0.19984410029907163
LOSS train 0.1561391669625448 valid 0.19984329689220087
LOSS train 0.1561391669625448 valid 0.19986001970154224
LOSS train 0.1561391669625448 valid 0.19982731004098528
LOSS train 0.1561391669625448 valid 0.19974577304769736
LOSS train 0.1561391669625448 valid 0.19986164769510817
LOSS train 0.1561391669625448 valid 0.19976274562386814
LOSS train 0.1561391669625448 valid 0.19984574611117314
LOSS train 0.1561391669625448 valid 0.1998562521533287
LOSS train 0.1561391669625448 valid 0.19988553865301995
LOSS train 0.1561391669625448 valid 0.19990830525899622
LOSS train 0.1561391669625448 valid 0.19986843924300793
LOSS train 0.1561391669625448 valid 0.19989303616098703
LOSS train 0.1561391669625448 valid 0.1998958344197577
LOSS train 0.1561391669625448 valid 0.1999148423236514
LOSS train 0.1561391669625448 valid 0.19992529520694213
LOSS train 0.1561391669625448 valid 0.19990579520678295
LOSS train 0.1561391669625448 valid 0.1999653731502077
LOSS train 0.1561391669625448 valid 0.19997671470746725
LOSS train 0.1561391669625448 valid 0.19987369044683873
LOSS train 0.1561391669625448 valid 0.19987993313701724
LOSS train 0.1561391669625448 valid 0.19983910366615154
LOSS train 0.1561391669625448 valid 0.19979119526897052
LOSS train 0.1561391669625448 valid 0.1997831643179611
LOSS train 0.1561391669625448 valid 0.19962182581424714
LOSS train 0.1561391669625448 valid 0.19968419211225275
LOSS train 0.1561391669625448 valid 0.19956110764169546
LOSS train 0.1561391669625448 valid 0.1995812073895117
LOSS train 0.1561391669625448 valid 0.199634322053031
LOSS train 0.1561391669625448 valid 0.19970029041622625
LOSS train 0.1561391669625448 valid 0.19962737504449135
LOSS train 0.1561391669625448 valid 0.199619087738445
LOSS train 0.1561391669625448 valid 0.19964296381931762
LOSS train 0.1561391669625448 valid 0.19960080234411948
LOSS train 0.1561391669625448 valid 0.19948978428520373
LOSS train 0.1561391669625448 valid 0.19942006632863057
LOSS train 0.1561391669625448 valid 0.19956562940020237
LOSS train 0.1561391669625448 valid 0.19950783918418827
LOSS train 0.1561391669625448 valid 0.19940630375108184
LOSS train 0.1561391669625448 valid 0.19942907353534417
LOSS train 0.1561391669625448 valid 0.19947989962317728
LOSS train 0.1561391669625448 valid 0.19950020335048263
LOSS train 0.1561391669625448 valid 0.19938242031429668
LOSS train 0.1561391669625448 valid 0.19944733970387038
LOSS train 0.1561391669625448 valid 0.19949605672255807
LOSS train 0.1561391669625448 valid 0.19941166947687292
LOSS train 0.1561391669625448 valid 0.1994016307059901
LOSS train 0.1561391669625448 valid 0.19936064896227298
LOSS train 0.1561391669625448 valid 0.19932862044576247
LOSS train 0.1561391669625448 valid 0.19934715330600739
LOSS train 0.1561391669625448 valid 0.19938857407651395
LOSS train 0.1561391669625448 valid 0.19940662303600798
LOSS train 0.1561391669625448 valid 0.1994495802999556
LOSS train 0.1561391669625448 valid 0.19934975852568945
LOSS train 0.1561391669625448 valid 0.19925896877973853
LOSS train 0.1561391669625448 valid 0.1992413452669476
LOSS train 0.1561391669625448 valid 0.1992711681945651
LOSS train 0.1561391669625448 valid 0.19937274721581177
LOSS train 0.1561391669625448 valid 0.19938887531212776
LOSS train 0.1561391669625448 valid 0.19961570447517765
LOSS train 0.1561391669625448 valid 0.19956577199813072
LOSS train 0.1561391669625448 valid 0.19968524237693344
LOSS train 0.1561391669625448 valid 0.19971471203916985
LOSS train 0.1561391669625448 valid 0.19962719447173916
LOSS train 0.1561391669625448 valid 0.1996313073455471
LOSS train 0.1561391669625448 valid 0.19962354089881554
LOSS train 0.1561391669625448 valid 0.1996694800801758
LOSS train 0.1561391669625448 valid 0.19956646620741356
LOSS train 0.1561391669625448 valid 0.19965251997557437
EPOCH 4:
  batch 1 loss: 0.16304534673690796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.14368686079978943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.14263080060482025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.14158233255147934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14547737836837768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14447635412216187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1446774091039385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.14230709336698055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.14363715052604675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.14199692755937576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.14301178265701642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14479697247346243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1454919404708422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.14357023632952146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.14322507431109746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.144496934954077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14582324773073196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.14684501331713465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14779839821551977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.14818460904061795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.14886274614504405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1484253599562428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14666115168644034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14664732199162245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14657217770814895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.14832939408146417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14876385888567678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1492625493556261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1501418318727921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1498157319923242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1492219685066131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.14827745710499585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14829748710899643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14924510283505216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14993778403316224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.15079311260746586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1513621547334903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.15051433640091041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14967602625107154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.15027177203446626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.15018979678066766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1502571441233158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1501648039665333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.15031995268707926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.15008283406496048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14994686663798665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14995326181041432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1504507587912182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.15067815887076513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.15070312216877937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.15053140138294183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14996428386523172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.15003459830329102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1500217691063881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1501237936995246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1503796542861632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.15068733587599636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1503365951879271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.15090091652789359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.15164764747023582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.15217682132955457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.15186785257631732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.15146813955571917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.15157687314786017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.151566353898782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.15154397645683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.15174226151473486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.15191840205122442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1515259822641594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.15121614720140183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1513238018247443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.15101150071455371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.15129014135223545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.15166603089184374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1515834730863571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.15162198186704987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.15188753701649704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.15206128435257155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.15215673552283757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.15225550029426813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.15223233070638445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.15229842484724232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.15211585576993875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.15231840685009956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1524508916279849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1520465025028517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1521295740686614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.15188178741796451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.15199963614512024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.15209879659944112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1521162883593486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.15192300988280255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1518896689978979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.15194955943746769
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1517678052186966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.15175705247869095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.15165894916377118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.15152440493812366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.15185638148375233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.15190352886915207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.15201601548360125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1520601148698844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.15205764539033464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.15227780505441701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.15221218367417652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.15219626041515819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1522077961224262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.15207816136104088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.15208653914272238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.15227925330400466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.15219044322903091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.15193572507372924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1518482939068195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.15195539802835697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.15206986238127168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.15220564590959712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.15228559573491415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.15214427156468568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.15230946818820568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1522041483471791
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.15233663574230571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.15245492951791795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.15242845704400443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.15234817912982357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.15259577453136444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.15258666470883384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.15275172537236703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.15268050238955766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.15247422329677168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.15232799981649106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1523201788654764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.15201553064539577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.15202612418653375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.15183899512709076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.15172956039508184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1519107633982511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.15193983417575377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.15183287346060725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.15193172308395234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.15170050029243742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1517440069017681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.15179570491465044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.15185886444328547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.15167135895333356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.15184643869769984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.15187936822233133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1516816898876307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.15180441243825732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.15194439857998152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1519191125035286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1520015193729211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.15206895611788096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.15218776026192835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.15209794905665633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.15200270673921032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.15196136079537562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.15192023090496184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.15191297336846968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.15193661213296014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.15188128808513285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1519018152485723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.15207394065312396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.15215287490125082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1520732597606938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.15187844218629779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.15172259913510586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.15173830431021615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.15159489152332148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.15146511458080902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.15155778886640772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.15160198910543096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1514972677930843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.15141478740755535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1514729904546135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.15152419422354016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.15159421160139822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.15195505295769643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.15192921752675195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.15195311605930328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.15194501794046825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.15192840985171702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1519580697947806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.15200053057709678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.15194232105884864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.15191252730988167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.151915738419179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1520395308414245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1519276186665322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.15198187537924954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.1520028019421979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.15194190781153932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.15212082482563952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1521317074799167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.15213083460466148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1520922432343165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.15204386063376252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.15180945865393894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.15175781248494832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.15162112769769065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.15174618124961853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.15169647810471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1516852155327797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.15176008362781826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.15173853685458502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.15167822554344085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1517720948000556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.15164230162826714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1515019448259129
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.15136258771117225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.15145995031510082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.15144490457682813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.15164072245781152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.15175473105879456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1517171132439208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.15165503229512725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1516391451091126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.15168247351311318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.15167168900370598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.15174022184251107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.15152958888899196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.15154355164297026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.15154665795800923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.15162780219396668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1515755166432687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.15162848095099132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1515432318623087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.15168386362985367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.15154631180982842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.15144545692283515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.15149826640668124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.15150538751315246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1515059345883542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1514514002293476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.15141315560819757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.15130052052913828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.15130461424084032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.15125275230357416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.15134473289010905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.15124752101289676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.15119101274758578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.15113832229647894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.15116289590508486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.15116256892435836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1511491659845485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1511151061374314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1509462660405694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1509686468342538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.15106927831807443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.15106329142329206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1510940455198288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1511027951283284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.15112346718235622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.15105498767653003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.15100469704218736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.15110314009236356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.15110404277220368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.15100060298980905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.1509588845362959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.15100471755942782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1510258885530325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1511820100847332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.15125815490729935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.15111486384283906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1511466164198337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1511694518181513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1510549656262523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.15109061639518773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.15117612461656776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1510987526366702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.15116286148075705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.15108339639717364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1510036194642239
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.150929967043819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.15095548693389788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1508326920325106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.15077516968375532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.15087651611988295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.15078953538331197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.15072302886127997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1507782863985215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.15072124191556538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.15074998644966606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.15069035651300483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1507092295345706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.15084277791412254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.15084177477063831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1509256735963273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1508422627682901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.15071270656111332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1506953994004891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.15068325003500246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.15068959717779126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1508118148905832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1507733925720867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.15081818166930797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.15087513421737664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.15080535981289866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.15092633837121444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.15096909328547609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1508849119891723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1508268658257402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.15069506633163288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1507380790639632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.15076816650597671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.15080750359863532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.15078088514555513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1507247817438666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1507153259469317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.15080027388330416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.15075471016668504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.15068630955610243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.15064842607348394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1507104719504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1507283739603249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1508824666341146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.15087419635132898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.150909110621699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.15089957772185966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1509140697672823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.15085384752601386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.15086286280571115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.15098604999528908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.15089002895189144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.15086014065201636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.15095685539337306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.15093545462547636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.15093937135708807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.15094613404263083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.15099213734314434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1509386320683089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.15090430469401292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.15097009500555963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.15100148049471257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.15100068088152452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1509505901096472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.15104965260252357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1510450914944315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.15097750399327842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.15090363705668125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1509512674063444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.15100013919152822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1510156741834175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.15101839221861899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.15101251627730075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.15110914920983107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.15109118326120294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.15116548965489143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.15117810797160383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1511360004885832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.15118075726287705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1512690527189491
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1512792552609674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.15127777607937373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.15137508021151952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.15135996952443057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.15133854870297267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.15128399400400513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1512808866132904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1512491708221874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.15125902322017484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.15122833144218967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.15130066305645923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1513021551585723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.15129400742659857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1513216365485975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.15135119576516048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.15127983638554893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.15137166511672345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1513898262607696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1514422049997626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1514729252361223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1514432243001397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.15137993220749232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.15142946608643482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1514129590392113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.15141545749328872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.15144145411664042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.15145434539705988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.15154168007792143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.15158526813121218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.15157536448534392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.15158223265206627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1515813535833172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1515852403632986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.15151498455118823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.15154359186637587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1514336588077767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.15148175419452264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.15146226104550006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.15147163229875074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.15153434628720783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.151550541650884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.15153274395083652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1515045340744977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1515557905163946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1515103548087857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1515008993728335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1514792299300582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.15150508463830875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.15149698197841643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.15156369468666372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1516226233534552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.15160054730126638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.15158439822273678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1515698180522448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1515455574824892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.15152439628830705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.15144875590853832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1514806745250534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.15145605075650098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.15154332807174276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.15154075405551393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1514583657930896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1513740595628098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.15137833550751928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.15142774700115508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1513227331552574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.15126980891068015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.15127163339486271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.15125069586294038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1512591406313654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.15131319628507606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.15125953960925975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.15130159448621408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.15130701987182393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1512535963679703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.15122571273486565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.15127868717101134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.15125623491260556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.15127670543138372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1512657605163459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1513595968760826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1513666422826864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1514147456546533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1514034062966533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1514711771331249
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.15148908397698566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1515199754336109
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.15151176786639967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.15153217969292945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.151537896486637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.15152558873142052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1515675086536472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.15153101009425815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1514757414547245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.15142526962030095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.15140160321522614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1514084997719952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.15139040739207066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.15132687538862227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.15132043502415363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.151276737385619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.15123921711713273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1511575253064937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.15107917526921072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.15107824166485093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1510549465141359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1509997196267786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.15092233078217454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.1509544117781131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.15095781554304338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1509176336099833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1509052036293897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1509799718824697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.15092126070171274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.15094941326860706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1509935336398874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.15098493574903563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.15098878137592567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.15089692872889499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.15092744490017052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.15087035268355728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.15087035268355728 valid 0.1816110610961914
LOSS train 0.15087035268355728 valid 0.20786413550376892
LOSS train 0.15087035268355728 valid 0.2199128121137619
LOSS train 0.15087035268355728 valid 0.2162575051188469
LOSS train 0.15087035268355728 valid 0.21542503833770751
LOSS train 0.15087035268355728 valid 0.2205636352300644
LOSS train 0.15087035268355728 valid 0.21487714563097274
LOSS train 0.15087035268355728 valid 0.21417911909520626
LOSS train 0.15087035268355728 valid 0.21294662853082022
LOSS train 0.15087035268355728 valid 0.2142556592822075
LOSS train 0.15087035268355728 valid 0.21064287424087524
LOSS train 0.15087035268355728 valid 0.21284510816136995
LOSS train 0.15087035268355728 valid 0.21162389448055854
LOSS train 0.15087035268355728 valid 0.212172337940761
LOSS train 0.15087035268355728 valid 0.21086062987645468
LOSS train 0.15087035268355728 valid 0.21117012668401003
LOSS train 0.15087035268355728 valid 0.2112801031154745
LOSS train 0.15087035268355728 valid 0.2117014941242006
LOSS train 0.15087035268355728 valid 0.21354938180823074
LOSS train 0.15087035268355728 valid 0.21288279742002486
LOSS train 0.15087035268355728 valid 0.21328312087626683
LOSS train 0.15087035268355728 valid 0.21175509149377997
LOSS train 0.15087035268355728 valid 0.21001952109129532
LOSS train 0.15087035268355728 valid 0.21096187892059484
LOSS train 0.15087035268355728 valid 0.20942145347595215
LOSS train 0.15087035268355728 valid 0.20895198789926675
LOSS train 0.15087035268355728 valid 0.20860851583657442
LOSS train 0.15087035268355728 valid 0.20851211728794233
LOSS train 0.15087035268355728 valid 0.20728041128865604
LOSS train 0.15087035268355728 valid 0.2065023809671402
LOSS train 0.15087035268355728 valid 0.20561294978664768
LOSS train 0.15087035268355728 valid 0.20633763913065195
LOSS train 0.15087035268355728 valid 0.20517625682281726
LOSS train 0.15087035268355728 valid 0.2046946685980348
LOSS train 0.15087035268355728 valid 0.20539447792938778
LOSS train 0.15087035268355728 valid 0.20544877068863976
LOSS train 0.15087035268355728 valid 0.20452616101986654
LOSS train 0.15087035268355728 valid 0.20443387290364817
LOSS train 0.15087035268355728 valid 0.20396814399804825
LOSS train 0.15087035268355728 valid 0.20367162749171258
LOSS train 0.15087035268355728 valid 0.20344944298267365
LOSS train 0.15087035268355728 valid 0.20466291868970507
LOSS train 0.15087035268355728 valid 0.2056668224029763
LOSS train 0.15087035268355728 valid 0.2047889337620952
LOSS train 0.15087035268355728 valid 0.20425477325916291
LOSS train 0.15087035268355728 valid 0.20270140501468079
LOSS train 0.15087035268355728 valid 0.20201612787043793
LOSS train 0.15087035268355728 valid 0.20340377961595854
LOSS train 0.15087035268355728 valid 0.20328032848786334
LOSS train 0.15087035268355728 valid 0.20359599500894546
LOSS train 0.15087035268355728 valid 0.203329679720542
LOSS train 0.15087035268355728 valid 0.20325474469707563
LOSS train 0.15087035268355728 valid 0.20424776454016846
LOSS train 0.15087035268355728 valid 0.20422337011054711
LOSS train 0.15087035268355728 valid 0.20438704571940683
LOSS train 0.15087035268355728 valid 0.20402696329568112
LOSS train 0.15087035268355728 valid 0.2033710898014537
LOSS train 0.15087035268355728 valid 0.20407945277362033
LOSS train 0.15087035268355728 valid 0.20420903523089523
LOSS train 0.15087035268355728 valid 0.20389326065778732
LOSS train 0.15087035268355728 valid 0.20376091242813674
LOSS train 0.15087035268355728 valid 0.20335138613177883
LOSS train 0.15087035268355728 valid 0.20343369671276637
LOSS train 0.15087035268355728 valid 0.20332323294132948
LOSS train 0.15087035268355728 valid 0.2024949910549017
LOSS train 0.15087035268355728 valid 0.20237504149025137
LOSS train 0.15087035268355728 valid 0.2029760181903839
LOSS train 0.15087035268355728 valid 0.2023207784137305
LOSS train 0.15087035268355728 valid 0.2027294784784317
LOSS train 0.15087035268355728 valid 0.20311829171010426
LOSS train 0.15087035268355728 valid 0.2031789501787911
LOSS train 0.15087035268355728 valid 0.2037005896369616
LOSS train 0.15087035268355728 valid 0.2039944880629239
LOSS train 0.15087035268355728 valid 0.20393904781824834
LOSS train 0.15087035268355728 valid 0.20369163393974304
LOSS train 0.15087035268355728 valid 0.20378202592071734
LOSS train 0.15087035268355728 valid 0.20390684631737796
LOSS train 0.15087035268355728 valid 0.20384960029369745
LOSS train 0.15087035268355728 valid 0.20414324463168276
LOSS train 0.15087035268355728 valid 0.20386635661125183
LOSS train 0.15087035268355728 valid 0.2039871350114728
LOSS train 0.15087035268355728 valid 0.2042417248211256
LOSS train 0.15087035268355728 valid 0.20401308209781188
LOSS train 0.15087035268355728 valid 0.20388229563832283
LOSS train 0.15087035268355728 valid 0.20412871364284965
LOSS train 0.15087035268355728 valid 0.20422834764386333
LOSS train 0.15087035268355728 valid 0.20383120747818345
LOSS train 0.15087035268355728 valid 0.20371495390480215
LOSS train 0.15087035268355728 valid 0.2038199193357082
LOSS train 0.15087035268355728 valid 0.20366722461250092
LOSS train 0.15087035268355728 valid 0.20375692631517137
LOSS train 0.15087035268355728 valid 0.20381745226357295
LOSS train 0.15087035268355728 valid 0.204139894054782
LOSS train 0.15087035268355728 valid 0.2044454778128482
LOSS train 0.15087035268355728 valid 0.20485064716715562
LOSS train 0.15087035268355728 valid 0.20569880663727722
LOSS train 0.15087035268355728 valid 0.20585985850427568
LOSS train 0.15087035268355728 valid 0.20635136004005158
LOSS train 0.15087035268355728 valid 0.20623861614501837
LOSS train 0.15087035268355728 valid 0.20648621425032615
LOSS train 0.15087035268355728 valid 0.2064427531296664
LOSS train 0.15087035268355728 valid 0.20681527008612952
LOSS train 0.15087035268355728 valid 0.20635683067793986
LOSS train 0.15087035268355728 valid 0.20620997336048347
LOSS train 0.15087035268355728 valid 0.20643349176361447
LOSS train 0.15087035268355728 valid 0.20646381040789047
LOSS train 0.15087035268355728 valid 0.206261948308098
LOSS train 0.15087035268355728 valid 0.20630265151460966
LOSS train 0.15087035268355728 valid 0.20577642745381103
LOSS train 0.15087035268355728 valid 0.20556166023015976
LOSS train 0.15087035268355728 valid 0.2056170963757747
LOSS train 0.15087035268355728 valid 0.20569252741656133
LOSS train 0.15087035268355728 valid 0.20549973621305112
LOSS train 0.15087035268355728 valid 0.20571078242439972
LOSS train 0.15087035268355728 valid 0.20609517771264782
LOSS train 0.15087035268355728 valid 0.20607909457436924
LOSS train 0.15087035268355728 valid 0.20608029508183145
LOSS train 0.15087035268355728 valid 0.2060793679902109
LOSS train 0.15087035268355728 valid 0.20580755424599687
LOSS train 0.15087035268355728 valid 0.20575113259255887
LOSS train 0.15087035268355728 valid 0.20581656472742066
LOSS train 0.15087035268355728 valid 0.2058535610310367
LOSS train 0.15087035268355728 valid 0.2057333575273917
LOSS train 0.15087035268355728 valid 0.205814422018105
LOSS train 0.15087035268355728 valid 0.20596858513355254
LOSS train 0.15087035268355728 valid 0.20584139346130312
LOSS train 0.15087035268355728 valid 0.20570390712557815
LOSS train 0.15087035268355728 valid 0.20560850424226373
LOSS train 0.15087035268355728 valid 0.20549540833909383
LOSS train 0.15087035268355728 valid 0.2052026309646093
LOSS train 0.15087035268355728 valid 0.20522559746986127
LOSS train 0.15087035268355728 valid 0.20540670271624217
LOSS train 0.15087035268355728 valid 0.20539395600781404
LOSS train 0.15087035268355728 valid 0.20542763874157152
LOSS train 0.15087035268355728 valid 0.2054571278669216
LOSS train 0.15087035268355728 valid 0.20560313103830113
LOSS train 0.15087035268355728 valid 0.20540620440984295
LOSS train 0.15087035268355728 valid 0.2050575133467066
LOSS train 0.15087035268355728 valid 0.20499352156687126
LOSS train 0.15087035268355728 valid 0.20485575986760005
LOSS train 0.15087035268355728 valid 0.20511909114553573
LOSS train 0.15087035268355728 valid 0.2053865407344321
LOSS train 0.15087035268355728 valid 0.2054954170138686
LOSS train 0.15087035268355728 valid 0.20544828050252464
LOSS train 0.15087035268355728 valid 0.2052217591425468
LOSS train 0.15087035268355728 valid 0.20526777105788663
LOSS train 0.15087035268355728 valid 0.20525216873811217
LOSS train 0.15087035268355728 valid 0.2050078323966748
LOSS train 0.15087035268355728 valid 0.20520852766181
LOSS train 0.15087035268355728 valid 0.205160161058108
LOSS train 0.15087035268355728 valid 0.2050362584606701
LOSS train 0.15087035268355728 valid 0.2050941723741983
LOSS train 0.15087035268355728 valid 0.20484304895587996
LOSS train 0.15087035268355728 valid 0.20486562108838713
LOSS train 0.15087035268355728 valid 0.20467181417249863
LOSS train 0.15087035268355728 valid 0.20486001507976118
LOSS train 0.15087035268355728 valid 0.20466484879232516
LOSS train 0.15087035268355728 valid 0.20458511536634422
LOSS train 0.15087035268355728 valid 0.20488704694142132
LOSS train 0.15087035268355728 valid 0.20482801776379347
LOSS train 0.15087035268355728 valid 0.20498339248740155
LOSS train 0.15087035268355728 valid 0.20495571637595142
LOSS train 0.15087035268355728 valid 0.2047052608311542
LOSS train 0.15087035268355728 valid 0.20476187556618597
LOSS train 0.15087035268355728 valid 0.2049120919270949
LOSS train 0.15087035268355728 valid 0.20480877376464476
LOSS train 0.15087035268355728 valid 0.20485700961358533
LOSS train 0.15087035268355728 valid 0.20454865073164305
LOSS train 0.15087035268355728 valid 0.2044040800551691
LOSS train 0.15087035268355728 valid 0.2042522230569054
LOSS train 0.15087035268355728 valid 0.2041020178132587
LOSS train 0.15087035268355728 valid 0.2041799394023973
LOSS train 0.15087035268355728 valid 0.20424231361447043
LOSS train 0.15087035268355728 valid 0.20424224427719226
LOSS train 0.15087035268355728 valid 0.20395857104233334
LOSS train 0.15087035268355728 valid 0.20393962319940329
LOSS train 0.15087035268355728 valid 0.2038317348829097
LOSS train 0.15087035268355728 valid 0.20383628148041413
LOSS train 0.15087035268355728 valid 0.2038392076612185
LOSS train 0.15087035268355728 valid 0.20386167814334233
LOSS train 0.15087035268355728 valid 0.2037429549417443
LOSS train 0.15087035268355728 valid 0.20364381684051766
LOSS train 0.15087035268355728 valid 0.20377652078378397
LOSS train 0.15087035268355728 valid 0.20382442479224308
LOSS train 0.15087035268355728 valid 0.20389117904611537
LOSS train 0.15087035268355728 valid 0.20405125009116307
LOSS train 0.15087035268355728 valid 0.2040110796211875
LOSS train 0.15087035268355728 valid 0.2040150301887634
LOSS train 0.15087035268355728 valid 0.20373072794505528
LOSS train 0.15087035268355728 valid 0.20371914160879034
LOSS train 0.15087035268355728 valid 0.2038124030485203
LOSS train 0.15087035268355728 valid 0.20386231853626668
LOSS train 0.15087035268355728 valid 0.20364373806535888
LOSS train 0.15087035268355728 valid 0.20371816038470908
LOSS train 0.15087035268355728 valid 0.20362640023231507
LOSS train 0.15087035268355728 valid 0.20383854271197804
LOSS train 0.15087035268355728 valid 0.20395662699859154
LOSS train 0.15087035268355728 valid 0.20405476141457607
LOSS train 0.15087035268355728 valid 0.20416036293135217
LOSS train 0.15087035268355728 valid 0.2043468076735735
LOSS train 0.15087035268355728 valid 0.20436820172848394
LOSS train 0.15087035268355728 valid 0.2045586219813564
LOSS train 0.15087035268355728 valid 0.20469060819137272
LOSS train 0.15087035268355728 valid 0.20451450479381225
LOSS train 0.15087035268355728 valid 0.2043546712253152
LOSS train 0.15087035268355728 valid 0.20429195535993114
LOSS train 0.15087035268355728 valid 0.20413735091398302
LOSS train 0.15087035268355728 valid 0.20392806978466418
LOSS train 0.15087035268355728 valid 0.203970345583829
LOSS train 0.15087035268355728 valid 0.20418530390376136
LOSS train 0.15087035268355728 valid 0.2043923129685117
LOSS train 0.15087035268355728 valid 0.20426030625712197
LOSS train 0.15087035268355728 valid 0.2042408673696115
LOSS train 0.15087035268355728 valid 0.20419033115433755
LOSS train 0.15087035268355728 valid 0.20392422336478566
LOSS train 0.15087035268355728 valid 0.20370275720402045
LOSS train 0.15087035268355728 valid 0.20375663804293778
LOSS train 0.15087035268355728 valid 0.20370524786754485
LOSS train 0.15087035268355728 valid 0.20377113605470962
LOSS train 0.15087035268355728 valid 0.2037350098517808
LOSS train 0.15087035268355728 valid 0.2037130547055292
LOSS train 0.15087035268355728 valid 0.20382088550307728
LOSS train 0.15087035268355728 valid 0.20370016123414575
LOSS train 0.15087035268355728 valid 0.2035738897642919
LOSS train 0.15087035268355728 valid 0.20327348907788595
LOSS train 0.15087035268355728 valid 0.2030833902875934
LOSS train 0.15087035268355728 valid 0.20307368499592013
LOSS train 0.15087035268355728 valid 0.20310976526193453
LOSS train 0.15087035268355728 valid 0.20303138856283962
LOSS train 0.15087035268355728 valid 0.2031858737701955
LOSS train 0.15087035268355728 valid 0.2032162875453115
LOSS train 0.15087035268355728 valid 0.20333871662873645
LOSS train 0.15087035268355728 valid 0.20347500595667845
LOSS train 0.15087035268355728 valid 0.20343802410822648
LOSS train 0.15087035268355728 valid 0.20339738117887618
LOSS train 0.15087035268355728 valid 0.20321620015774744
LOSS train 0.15087035268355728 valid 0.20323596699831356
LOSS train 0.15087035268355728 valid 0.203271795784225
LOSS train 0.15087035268355728 valid 0.20326616848861823
LOSS train 0.15087035268355728 valid 0.2028875503068169
LOSS train 0.15087035268355728 valid 0.20277859097447137
LOSS train 0.15087035268355728 valid 0.2028379273439242
LOSS train 0.15087035268355728 valid 0.2028820839070489
LOSS train 0.15087035268355728 valid 0.20293372686280578
LOSS train 0.15087035268355728 valid 0.20313778689929418
LOSS train 0.15087035268355728 valid 0.20324064954751875
LOSS train 0.15087035268355728 valid 0.2032665569289976
LOSS train 0.15087035268355728 valid 0.20317619546286522
LOSS train 0.15087035268355728 valid 0.2030862825941369
LOSS train 0.15087035268355728 valid 0.20320515650510787
LOSS train 0.15087035268355728 valid 0.2032767465390057
LOSS train 0.15087035268355728 valid 0.20331209803384448
LOSS train 0.15087035268355728 valid 0.20321018387206458
LOSS train 0.15087035268355728 valid 0.2032616918129245
LOSS train 0.15087035268355728 valid 0.20325051657125062
LOSS train 0.15087035268355728 valid 0.2032645646831952
LOSS train 0.15087035268355728 valid 0.20317522253739695
LOSS train 0.15087035268355728 valid 0.2032486770388692
LOSS train 0.15087035268355728 valid 0.2032163551070055
LOSS train 0.15087035268355728 valid 0.20310349624890547
LOSS train 0.15087035268355728 valid 0.20307294733222875
LOSS train 0.15087035268355728 valid 0.20332153243872955
LOSS train 0.15087035268355728 valid 0.2031954577327228
LOSS train 0.15087035268355728 valid 0.20322841067205777
LOSS train 0.15087035268355728 valid 0.20330923199653625
LOSS train 0.15087035268355728 valid 0.2033750986246238
LOSS train 0.15087035268355728 valid 0.2033637787184019
LOSS train 0.15087035268355728 valid 0.20341494985258401
LOSS train 0.15087035268355728 valid 0.20348319267251677
LOSS train 0.15087035268355728 valid 0.20336492464498238
LOSS train 0.15087035268355728 valid 0.20328425629772384
LOSS train 0.15087035268355728 valid 0.20311645149965496
LOSS train 0.15087035268355728 valid 0.20299197400445904
LOSS train 0.15087035268355728 valid 0.20295296587648182
LOSS train 0.15087035268355728 valid 0.20304449737071992
LOSS train 0.15087035268355728 valid 0.20317905952317128
LOSS train 0.15087035268355728 valid 0.2031153609929102
LOSS train 0.15087035268355728 valid 0.20309594932863181
LOSS train 0.15087035268355728 valid 0.20317174344720806
LOSS train 0.15087035268355728 valid 0.2029920349695853
LOSS train 0.15087035268355728 valid 0.20286088232680147
LOSS train 0.15087035268355728 valid 0.2028454290108478
LOSS train 0.15087035268355728 valid 0.20285827937690615
LOSS train 0.15087035268355728 valid 0.20271049329722432
LOSS train 0.15087035268355728 valid 0.2028039642070469
LOSS train 0.15087035268355728 valid 0.20281345954724958
LOSS train 0.15087035268355728 valid 0.20293429893691364
LOSS train 0.15087035268355728 valid 0.20288062659609649
LOSS train 0.15087035268355728 valid 0.20284924808258004
LOSS train 0.15087035268355728 valid 0.2027476550176226
LOSS train 0.15087035268355728 valid 0.20269187501410849
LOSS train 0.15087035268355728 valid 0.20266318918295104
LOSS train 0.15087035268355728 valid 0.20262161838113243
LOSS train 0.15087035268355728 valid 0.20260636320933192
LOSS train 0.15087035268355728 valid 0.20259395090200133
LOSS train 0.15087035268355728 valid 0.2026352155450228
LOSS train 0.15087035268355728 valid 0.20263391898738015
LOSS train 0.15087035268355728 valid 0.20267086956124977
LOSS train 0.15087035268355728 valid 0.2026883185308514
LOSS train 0.15087035268355728 valid 0.202725499967734
LOSS train 0.15087035268355728 valid 0.20269866337213802
LOSS train 0.15087035268355728 valid 0.20269339048112464
LOSS train 0.15087035268355728 valid 0.20271184022473818
LOSS train 0.15087035268355728 valid 0.20267470328039244
LOSS train 0.15087035268355728 valid 0.2025733217841289
LOSS train 0.15087035268355728 valid 0.20268399049254024
LOSS train 0.15087035268355728 valid 0.20257619795077011
LOSS train 0.15087035268355728 valid 0.20266887806839756
LOSS train 0.15087035268355728 valid 0.2026823072371745
LOSS train 0.15087035268355728 valid 0.20272050956564566
LOSS train 0.15087035268355728 valid 0.20276485268517705
LOSS train 0.15087035268355728 valid 0.20272955666176784
LOSS train 0.15087035268355728 valid 0.2027487315880224
LOSS train 0.15087035268355728 valid 0.20275587460417657
LOSS train 0.15087035268355728 valid 0.20277793440553876
LOSS train 0.15087035268355728 valid 0.20277234100842778
LOSS train 0.15087035268355728 valid 0.20275042528045667
LOSS train 0.15087035268355728 valid 0.20282074495119118
LOSS train 0.15087035268355728 valid 0.20282945737569683
LOSS train 0.15087035268355728 valid 0.2027129523921758
LOSS train 0.15087035268355728 valid 0.2027257911605627
LOSS train 0.15087035268355728 valid 0.20266863851813796
LOSS train 0.15087035268355728 valid 0.2026219140542181
LOSS train 0.15087035268355728 valid 0.20259879009775172
LOSS train 0.15087035268355728 valid 0.2024323184673603
LOSS train 0.15087035268355728 valid 0.2025016324560335
LOSS train 0.15087035268355728 valid 0.2023853691072639
LOSS train 0.15087035268355728 valid 0.20242910078022539
LOSS train 0.15087035268355728 valid 0.20248727536672517
LOSS train 0.15087035268355728 valid 0.20255742515578415
LOSS train 0.15087035268355728 valid 0.2024873278562396
LOSS train 0.15087035268355728 valid 0.20246881277984882
LOSS train 0.15087035268355728 valid 0.20247846533049335
LOSS train 0.15087035268355728 valid 0.20243418185475343
LOSS train 0.15087035268355728 valid 0.2023203891604694
LOSS train 0.15087035268355728 valid 0.20223625944483847
LOSS train 0.15087035268355728 valid 0.20238235504407912
LOSS train 0.15087035268355728 valid 0.20231955185444397
LOSS train 0.15087035268355728 valid 0.20221295856093235
LOSS train 0.15087035268355728 valid 0.20225251829799484
LOSS train 0.15087035268355728 valid 0.202300418288477
LOSS train 0.15087035268355728 valid 0.20232537397515704
LOSS train 0.15087035268355728 valid 0.20220418165793572
LOSS train 0.15087035268355728 valid 0.20229044123444445
LOSS train 0.15087035268355728 valid 0.20235861729884494
LOSS train 0.15087035268355728 valid 0.20227857872929877
LOSS train 0.15087035268355728 valid 0.20224325994421494
LOSS train 0.15087035268355728 valid 0.20220549378929467
LOSS train 0.15087035268355728 valid 0.20217869428645574
LOSS train 0.15087035268355728 valid 0.2022121695961271
LOSS train 0.15087035268355728 valid 0.20225595506677602
LOSS train 0.15087035268355728 valid 0.20228216632015325
LOSS train 0.15087035268355728 valid 0.20233752411755895
LOSS train 0.15087035268355728 valid 0.20222802893758493
LOSS train 0.15087035268355728 valid 0.20211529681380366
LOSS train 0.15087035268355728 valid 0.2021014445534583
LOSS train 0.15087035268355728 valid 0.20213196253409238
LOSS train 0.15087035268355728 valid 0.2022359923110994
LOSS train 0.15087035268355728 valid 0.20223964078346668
LOSS train 0.15087035268355728 valid 0.2024897305915753
LOSS train 0.15087035268355728 valid 0.20244353113907526
LOSS train 0.15087035268355728 valid 0.20256419772107298
LOSS train 0.15087035268355728 valid 0.20258902592107284
LOSS train 0.15087035268355728 valid 0.2024888252573354
LOSS train 0.15087035268355728 valid 0.20250363582617617
LOSS train 0.15087035268355728 valid 0.20249626410952032
LOSS train 0.15087035268355728 valid 0.20255519697386823
LOSS train 0.15087035268355728 valid 0.20243753310616897
LOSS train 0.15087035268355728 valid 0.2025357673000191
EPOCH 5:
  batch 1 loss: 0.16364790499210358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13879413530230522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1376733457048734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13760335557162762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14256390184164047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.14196719353397688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.14111278418983733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.138484719209373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1393718214498626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13746589794754982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13887895169583234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14072979303697744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1408294101174061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13878283383590834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13824241707722346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13941484550014138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.14095281546606736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1423379857507017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.14328534783501373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1435053411871195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1442193165421486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14371676370501518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.14168125974095386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1415118481963873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1416250467300415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1434575869486882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14400671477671023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14462466804044588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.14551752086343436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1456287274758021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1451006935488793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1443469524383545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14441396296024323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14533077793962815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1459188461303711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14689524927073055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14752439912912008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14668304963331474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14578321136725256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14622429776936768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14603620822109828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14603288489438238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14594960299342177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14610373381186614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14593960129552416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14571069618282112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14558506059519788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14609244636570415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14636261654751642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14633449658751488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1461266362491776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14555815421044827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1455467516240084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14572091414420693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14587980684908955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14615828397550754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14640755415485615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1461065076291561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1466962863075531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14735583203534286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14801519779396838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14754996076226234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14714442607429293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14712868153583258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1471228573184747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1469729548376618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14720333834637456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14738909122260177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.147095645143502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14685512238315174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14686909391426703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14655216400408083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14682488990564868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.147204370212716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14718769162893294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14715446482755637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14729807429112396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14745849208571973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14758324349605584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14778422666713595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14776355938778984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14785195132944642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1477169368460954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14799620840875877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1481491463149295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14778947752229002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1478723368939312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14771549649197946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14787199498897188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14795682339204683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.1479589425764241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14777317343522672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14773712419374016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14776894220329345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14757771876297499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14751525572501123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14742948936740147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14734976290135968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1476695876982477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14769019223749638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1477482821239103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1477539940353702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14774800019646153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14799729569886738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14797952011937188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14796108787633339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1479154398368898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1477936684947323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14774611790519243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14797880371863192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14788146766724888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14769008722422378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14758699519180618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14764766140203728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14775132232386132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14795740533234744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14801007273614916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14790491803975428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14808791932188162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14798846772561472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.148128947012188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14819646254181862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14815530841185795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14807278560774942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14826269048452378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14827864209101313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1484513054565182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1483784422161989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14822622954614403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14814327731728555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14811390575789313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14780047338343028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14776330553275302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14757829309621853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14750991413990658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14776009359561346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14778828267415944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1477312922369743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14786686180092448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1477285017392465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14786652422754476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14796126985424002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.147984115862763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1478010832539035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14800360022947706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1480437054311576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1478209428641261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14795279512937004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1481389724368217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.1481108890970548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14817564959162907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1482738299589408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1483489773631875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14828492730081855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14817506238337486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14813221160035867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14811429381370544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14815232242587245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14815475393391256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1480755666270852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.148048834771103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14821790480687294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14828706765833077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1482002649728845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14797097385832758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14781079490680293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1478021117652248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14765227484028964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14755644023242082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14767443594687124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14771888082661824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.147589924853555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14747775423561219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14754661493774118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14758152923413687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14762179964137348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14800302772703816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14799897000193596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.1479919380066115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14801912725799612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14801393082951972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14806789974426174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14813272935933755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14803710713496673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14802541543503064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14803575656266624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14807950290926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14797179571333083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14799541238912198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14803012009514005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14797389292748186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.148155708059979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1481421986314917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14814186691469752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14812091699777505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14805691077240876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14780788740076994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14778340333188422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14765275501875422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14777641374617814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14770954124518296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14769338377484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.147773477574581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1477497899722235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14766254036164866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14774050191044807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1475963798770006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1474586337661514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14731503153246556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1474315615636962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14744162382955234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14764134951357571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14775222805744045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14772744609095226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1476702677649121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14765737605867563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.147728571831356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1477614284381954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14790553494131184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14769201014529576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1477354286887527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14771584038798874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1478230461411412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1478109750231462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14787257333596548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14777810251818294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14787677599995147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14772929042054897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14764923560046733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.1476844379435415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14773270126545068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1477598431680737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14769140301600034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1476326451724411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1475342275297388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14756676002200378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14751588999596327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14761024985869392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14750247903822855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14744201311841607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14738012055762081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14740234222417034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14737636001144416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14735707246744242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14735893731822772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14719980406930777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1472051689921603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14740244842945568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14746889002232189
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14753306815028192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14758980265295363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14759998665087753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14753498151721692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14750205863296517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14761817335498098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14763977317488752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14756020500618197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14750269901498345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14755964523691928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1475561331384457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14774235749039158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14783536833319955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1477013563191936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1477174942635677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14775859643265887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14765169373468348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14768230850888547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14775470407929883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14766402658919864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14773723582426707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1476662126524422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14759303339044838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14751248012532245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14755506291441675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14741965058175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14736201854395695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14745790220877755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14736835009974542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14730455723714658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.147357532701322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14729247377436355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1472990131335901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14726570496078936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14728983056167483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14743644307579912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14743203461378604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14753015515903978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14745551508126986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14733299331990904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14731809175733862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1473570672660759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14739599290673863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1475206796880875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14750706847934497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1475509930212619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14762978207924077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1475687719605587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14770914726709358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1477152911267153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14763371986647447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14757844436901352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1474528782168366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14749796105788485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14751251907038845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1475331793798775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14750466595580375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14745086763988488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14744929691123498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14752549658797706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14748957462368473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14742977953129643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14738637927728584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1474526341968832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.1474782193589742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14762152251270083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14761027356586123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14764315479843398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14763075228101052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14763535897840152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14758149820845573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1475774276990014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14769293832945526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1475747695694398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.147521295780201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14762353454644864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14759368447164084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1475983212163689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14759983329056967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14766259403123683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14761407689163178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14757834417672316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14763084433254708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1476649310630005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1476662202464963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14762508302037394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14770407116572773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14770377210675786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14765982636245045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14759607035062305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1476423988228335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1476701920813829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14764779315967308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.147646164689919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14762924964604682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14769140959217927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14764881011292424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14770834502208474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1477068798595119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14761456900561779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14764723754354886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14772524998823122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14768782704645259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1476888709171298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14780880883336067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14777210335916197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14772463579442394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1476504089994257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1475954873060714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14759600691725616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14758718309717045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14755359929767012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14765099541011437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14763749674085744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14761811316750206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14764180610032931
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14765972104636046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1475686651403313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14765814039856195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1476741726724759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1477366665327871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.147770937602475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14776069210261428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14768992598111125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14775000144055184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1477335208058357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14772707911485689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14776000813321663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14778278722728372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14788051372231784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1479406727575942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14792839300835853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14793310429491296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1479285998420678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14794291992438957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1478750575866018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14789197993479244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.147786519766932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14785921640847763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.147851395978566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1478680170308321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14793562342215072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14794262135591435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14795002544123403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14790966643521627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14798185416037524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1479433769491887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1479418699439587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1479078443093815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14792562784034208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1479318424127996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1480095762378557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1480692024365883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14804153390247235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14802895211568562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1480170675449901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14799889545942763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14799086440180678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14791788135235215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14794847653372656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1479355208757447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1480228953842989
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14802183691882392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14794068837642094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14784640768443905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14784675427230007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1478800973138557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14777798364631278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14773967972640215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14773663320091857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14768781360416186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14770377269833218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14775702975231325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14770201857842452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14773668431378198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14775216216550155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14769926794887708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14768479057790523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1477436892096406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14771542962128187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14774092208853987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14773327926402854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14783942597676758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14784965708886227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14790738598885625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14790352892944184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14796365577078194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1479848052973878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.1480300866203493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1480239135020143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1480414509603923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14804823556386965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14803399083234067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14806732463042838
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14803513644581978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1479876062675808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14794602143189833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1479206130708624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14791192910966597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14790096552109133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14782997409502666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1478447858285481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14779231364352513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14775650216385755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14766890432537916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14759704370747556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14758517928094717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14756996315013396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1475087712685614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14742994191599826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14746304442701133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14746075871838923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14741784424492807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1474018204418166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14746321525810094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14743386263488442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14746942476653235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14750703831045736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14751734864762706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14751078595103487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14741343136163468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14745383945366886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14740040724686646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14740040724686646 valid 0.18320488929748535
LOSS train 0.14740040724686646 valid 0.20831432938575745
LOSS train 0.14740040724686646 valid 0.21877080698808035
LOSS train 0.14740040724686646 valid 0.21537751331925392
LOSS train 0.14740040724686646 valid 0.21463477611541748
LOSS train 0.14740040724686646 valid 0.2201122815410296
LOSS train 0.14740040724686646 valid 0.2145969888993672
LOSS train 0.14740040724686646 valid 0.21414890326559544
LOSS train 0.14740040724686646 valid 0.21353691154056126
LOSS train 0.14740040724686646 valid 0.21469305455684662
LOSS train 0.14740040724686646 valid 0.21118737079880454
LOSS train 0.14740040724686646 valid 0.21363908300797144
LOSS train 0.14740040724686646 valid 0.21221260267954606
LOSS train 0.14740040724686646 valid 0.21232823921101435
LOSS train 0.14740040724686646 valid 0.21108459333578747
LOSS train 0.14740040724686646 valid 0.21156094316393137
LOSS train 0.14740040724686646 valid 0.2117376941091874
LOSS train 0.14740040724686646 valid 0.21228719419903225
LOSS train 0.14740040724686646 valid 0.2140588187857678
LOSS train 0.14740040724686646 valid 0.21342431604862214
LOSS train 0.14740040724686646 valid 0.2138311905520303
LOSS train 0.14740040724686646 valid 0.21219385212117975
LOSS train 0.14740040724686646 valid 0.2104468883379646
LOSS train 0.14740040724686646 valid 0.2114013507962227
LOSS train 0.14740040724686646 valid 0.21001425504684448
LOSS train 0.14740040724686646 valid 0.20949753259236997
LOSS train 0.14740040724686646 valid 0.20920398665799034
LOSS train 0.14740040724686646 valid 0.20923595715846335
LOSS train 0.14740040724686646 valid 0.2078998022038361
LOSS train 0.14740040724686646 valid 0.20702248166004816
LOSS train 0.14740040724686646 valid 0.2060852425713693
LOSS train 0.14740040724686646 valid 0.20685219718143344
LOSS train 0.14740040724686646 valid 0.20571051267060367
LOSS train 0.14740040724686646 valid 0.20520661420681896
LOSS train 0.14740040724686646 valid 0.20574470460414887
LOSS train 0.14740040724686646 valid 0.20579321392708355
LOSS train 0.14740040724686646 valid 0.20484099919731552
LOSS train 0.14740040724686646 valid 0.20480183825681084
LOSS train 0.14740040724686646 valid 0.2044622229459958
LOSS train 0.14740040724686646 valid 0.2041472762823105
LOSS train 0.14740040724686646 valid 0.20394145024985802
LOSS train 0.14740040724686646 valid 0.20506345906427928
LOSS train 0.14740040724686646 valid 0.20597334933835407
LOSS train 0.14740040724686646 valid 0.20510329949584874
LOSS train 0.14740040724686646 valid 0.2045458803574244
LOSS train 0.14740040724686646 valid 0.20304445501254953
LOSS train 0.14740040724686646 valid 0.20233368080981234
LOSS train 0.14740040724686646 valid 0.20374953467398882
LOSS train 0.14740040724686646 valid 0.2036030049226722
LOSS train 0.14740040724686646 valid 0.20389586806297302
LOSS train 0.14740040724686646 valid 0.20368505225462072
LOSS train 0.14740040724686646 valid 0.20355737467224783
LOSS train 0.14740040724686646 valid 0.20449224704841398
LOSS train 0.14740040724686646 valid 0.20438416053851446
LOSS train 0.14740040724686646 valid 0.20455985854972492
LOSS train 0.14740040724686646 valid 0.2042068030153002
LOSS train 0.14740040724686646 valid 0.20353719801233525
LOSS train 0.14740040724686646 valid 0.20430679722078915
LOSS train 0.14740040724686646 valid 0.20446280023809207
LOSS train 0.14740040724686646 valid 0.20414580578605335
LOSS train 0.14740040724686646 valid 0.203978459121751
LOSS train 0.14740040724686646 valid 0.20361961400316608
LOSS train 0.14740040724686646 valid 0.20368997918234932
LOSS train 0.14740040724686646 valid 0.2035753494128585
LOSS train 0.14740040724686646 valid 0.20277731647858252
LOSS train 0.14740040724686646 valid 0.20264334457390237
LOSS train 0.14740040724686646 valid 0.20323146990875698
LOSS train 0.14740040724686646 valid 0.2025809033828623
LOSS train 0.14740040724686646 valid 0.20293192435865817
LOSS train 0.14740040724686646 valid 0.20338570922613144
LOSS train 0.14740040724686646 valid 0.20348471990773376
LOSS train 0.14740040724686646 valid 0.20395862166252401
LOSS train 0.14740040724686646 valid 0.2042117376033574
LOSS train 0.14740040724686646 valid 0.2041864513948157
LOSS train 0.14740040724686646 valid 0.20397978802522024
LOSS train 0.14740040724686646 valid 0.20408116300639353
LOSS train 0.14740040724686646 valid 0.20416249257403535
LOSS train 0.14740040724686646 valid 0.20403350775058454
LOSS train 0.14740040724686646 valid 0.20430015517941005
LOSS train 0.14740040724686646 valid 0.20401606764644384
LOSS train 0.14740040724686646 valid 0.20415247277713117
LOSS train 0.14740040724686646 valid 0.204386863403204
LOSS train 0.14740040724686646 valid 0.20413331992654915
LOSS train 0.14740040724686646 valid 0.20402478462173826
LOSS train 0.14740040724686646 valid 0.2042388891472536
LOSS train 0.14740040724686646 valid 0.20433133944522502
LOSS train 0.14740040724686646 valid 0.20394263544986987
LOSS train 0.14740040724686646 valid 0.20381826484067875
LOSS train 0.14740040724686646 valid 0.2039471511425597
LOSS train 0.14740040724686646 valid 0.20379371129804189
LOSS train 0.14740040724686646 valid 0.20385223610715553
LOSS train 0.14740040724686646 valid 0.20391746448433917
LOSS train 0.14740040724686646 valid 0.2041948926384731
LOSS train 0.14740040724686646 valid 0.20453218307266843
LOSS train 0.14740040724686646 valid 0.20494921505451202
LOSS train 0.14740040724686646 valid 0.20575766560311118
LOSS train 0.14740040724686646 valid 0.20592293189358465
LOSS train 0.14740040724686646 valid 0.20639179053963447
LOSS train 0.14740040724686646 valid 0.20627369919810631
LOSS train 0.14740040724686646 valid 0.20655080184340477
LOSS train 0.14740040724686646 valid 0.20648102181972844
LOSS train 0.14740040724686646 valid 0.20689534366715187
LOSS train 0.14740040724686646 valid 0.20647190569095242
LOSS train 0.14740040724686646 valid 0.20632618608383033
LOSS train 0.14740040724686646 valid 0.20658470690250397
LOSS train 0.14740040724686646 valid 0.20659682787251923
LOSS train 0.14740040724686646 valid 0.2063951979730731
LOSS train 0.14740040724686646 valid 0.20646136778372307
LOSS train 0.14740040724686646 valid 0.2059399701586557
LOSS train 0.14740040724686646 valid 0.20572460320862856
LOSS train 0.14740040724686646 valid 0.20578662581271953
LOSS train 0.14740040724686646 valid 0.20588517029370582
LOSS train 0.14740040724686646 valid 0.20569546520709991
LOSS train 0.14740040724686646 valid 0.20590939987124057
LOSS train 0.14740040724686646 valid 0.206232968620632
LOSS train 0.14740040724686646 valid 0.206194418514597
LOSS train 0.14740040724686646 valid 0.20623311680606288
LOSS train 0.14740040724686646 valid 0.20624078620793457
LOSS train 0.14740040724686646 valid 0.20599381750872156
LOSS train 0.14740040724686646 valid 0.205959373836716
LOSS train 0.14740040724686646 valid 0.2060194088161484
LOSS train 0.14740040724686646 valid 0.2060636466399568
LOSS train 0.14740040724686646 valid 0.20598214792042244
LOSS train 0.14740040724686646 valid 0.20605558517479128
LOSS train 0.14740040724686646 valid 0.2061948791742325
LOSS train 0.14740040724686646 valid 0.2060552705374975
LOSS train 0.14740040724686646 valid 0.20590907804609285
LOSS train 0.14740040724686646 valid 0.20579645270481706
LOSS train 0.14740040724686646 valid 0.20570727697638577
LOSS train 0.14740040724686646 valid 0.20543436396580475
LOSS train 0.14740040724686646 valid 0.20544095098517323
LOSS train 0.14740040724686646 valid 0.20558885055961032
LOSS train 0.14740040724686646 valid 0.20557235046885067
LOSS train 0.14740040724686646 valid 0.20560907372343007
LOSS train 0.14740040724686646 valid 0.20565577855816594
LOSS train 0.14740040724686646 valid 0.2058170483831097
LOSS train 0.14740040724686646 valid 0.20564470075777846
LOSS train 0.14740040724686646 valid 0.20531005120795706
LOSS train 0.14740040724686646 valid 0.20527617665503523
LOSS train 0.14740040724686646 valid 0.20514430350490978
LOSS train 0.14740040724686646 valid 0.20541699241239128
LOSS train 0.14740040724686646 valid 0.20565583966147732
LOSS train 0.14740040724686646 valid 0.2057690161925096
LOSS train 0.14740040724686646 valid 0.20570527182685006
LOSS train 0.14740040724686646 valid 0.20548718890239454
LOSS train 0.14740040724686646 valid 0.2055259327570053
LOSS train 0.14740040724686646 valid 0.20551493155712985
LOSS train 0.14740040724686646 valid 0.20527310790242376
LOSS train 0.14740040724686646 valid 0.2054737060262053
LOSS train 0.14740040724686646 valid 0.20541243354479471
LOSS train 0.14740040724686646 valid 0.20529827791333988
LOSS train 0.14740040724686646 valid 0.20535246253405748
LOSS train 0.14740040724686646 valid 0.20511886292423298
LOSS train 0.14740040724686646 valid 0.20514426602945698
LOSS train 0.14740040724686646 valid 0.2049519665779606
LOSS train 0.14740040724686646 valid 0.20514856880673996
LOSS train 0.14740040724686646 valid 0.2049621454659541
LOSS train 0.14740040724686646 valid 0.20489180172923244
LOSS train 0.14740040724686646 valid 0.20517196192306542
LOSS train 0.14740040724686646 valid 0.2051465035416186
LOSS train 0.14740040724686646 valid 0.20530691493001785
LOSS train 0.14740040724686646 valid 0.20530567575751998
LOSS train 0.14740040724686646 valid 0.20506366675982446
LOSS train 0.14740040724686646 valid 0.20510445753248727
LOSS train 0.14740040724686646 valid 0.2052685797214508
LOSS train 0.14740040724686646 valid 0.2051625141357801
LOSS train 0.14740040724686646 valid 0.20522658865965768
LOSS train 0.14740040724686646 valid 0.20489166659258662
LOSS train 0.14740040724686646 valid 0.20475377521571322
LOSS train 0.14740040724686646 valid 0.2046107464853455
LOSS train 0.14740040724686646 valid 0.20444257278539982
LOSS train 0.14740040724686646 valid 0.20451078520611274
LOSS train 0.14740040724686646 valid 0.20456707503409743
LOSS train 0.14740040724686646 valid 0.20458976419150143
LOSS train 0.14740040724686646 valid 0.20431703780378613
LOSS train 0.14740040724686646 valid 0.2043042533438314
LOSS train 0.14740040724686646 valid 0.20418074078815804
LOSS train 0.14740040724686646 valid 0.20419492229317013
LOSS train 0.14740040724686646 valid 0.2042047639465865
LOSS train 0.14740040724686646 valid 0.20421890939275425
LOSS train 0.14740040724686646 valid 0.20409860680116473
LOSS train 0.14740040724686646 valid 0.20399481934178007
LOSS train 0.14740040724686646 valid 0.2041155805841821
LOSS train 0.14740040724686646 valid 0.2041626131243032
LOSS train 0.14740040724686646 valid 0.2042311294658764
LOSS train 0.14740040724686646 valid 0.20440543154554983
LOSS train 0.14740040724686646 valid 0.2043473983512205
LOSS train 0.14740040724686646 valid 0.20432714158867268
LOSS train 0.14740040724686646 valid 0.2040086463487968
LOSS train 0.14740040724686646 valid 0.2040000648090714
LOSS train 0.14740040724686646 valid 0.20410476741990494
LOSS train 0.14740040724686646 valid 0.20415263545388976
LOSS train 0.14740040724686646 valid 0.20392363553220128
LOSS train 0.14740040724686646 valid 0.20398861607632687
LOSS train 0.14740040724686646 valid 0.20389411457073994
LOSS train 0.14740040724686646 valid 0.20411354348975785
LOSS train 0.14740040724686646 valid 0.20422738003851798
LOSS train 0.14740040724686646 valid 0.20430915704881303
LOSS train 0.14740040724686646 valid 0.2044023058222766
LOSS train 0.14740040724686646 valid 0.20460047177970409
LOSS train 0.14740040724686646 valid 0.20462380779619835
LOSS train 0.14740040724686646 valid 0.20482204374995563
LOSS train 0.14740040724686646 valid 0.20494016542516905
LOSS train 0.14740040724686646 valid 0.2047528251275128
LOSS train 0.14740040724686646 valid 0.20459150395742276
LOSS train 0.14740040724686646 valid 0.20454204617773444
LOSS train 0.14740040724686646 valid 0.2043918573482025
LOSS train 0.14740040724686646 valid 0.20418602607857722
LOSS train 0.14740040724686646 valid 0.20421542640793267
LOSS train 0.14740040724686646 valid 0.20444149098226003
LOSS train 0.14740040724686646 valid 0.2046788879339164
LOSS train 0.14740040724686646 valid 0.20454651918613687
LOSS train 0.14740040724686646 valid 0.20452219317776496
LOSS train 0.14740040724686646 valid 0.20448006515469508
LOSS train 0.14740040724686646 valid 0.20421999831532323
LOSS train 0.14740040724686646 valid 0.2040100770967978
LOSS train 0.14740040724686646 valid 0.20404545248653483
LOSS train 0.14740040724686646 valid 0.20400331449618034
LOSS train 0.14740040724686646 valid 0.2040821684005598
LOSS train 0.14740040724686646 valid 0.2040366900238124
LOSS train 0.14740040724686646 valid 0.20402397464842817
LOSS train 0.14740040724686646 valid 0.2041341481847806
LOSS train 0.14740040724686646 valid 0.20401444235992003
LOSS train 0.14740040724686646 valid 0.2038961363557194
LOSS train 0.14740040724686646 valid 0.2036118002070321
LOSS train 0.14740040724686646 valid 0.20344671237785206
LOSS train 0.14740040724686646 valid 0.20343613762424906
LOSS train 0.14740040724686646 valid 0.20347398552193977
LOSS train 0.14740040724686646 valid 0.2034216204324664
LOSS train 0.14740040724686646 valid 0.20357009612995644
LOSS train 0.14740040724686646 valid 0.20361960134206913
LOSS train 0.14740040724686646 valid 0.20377196384401158
LOSS train 0.14740040724686646 valid 0.2039008980912712
LOSS train 0.14740040724686646 valid 0.20386354727113348
LOSS train 0.14740040724686646 valid 0.20381983658100697
LOSS train 0.14740040724686646 valid 0.2036362484855167
LOSS train 0.14740040724686646 valid 0.2036692172540391
LOSS train 0.14740040724686646 valid 0.20370419054221706
LOSS train 0.14740040724686646 valid 0.20370590237892822
LOSS train 0.14740040724686646 valid 0.2033238396048546
LOSS train 0.14740040724686646 valid 0.2032244282763034
LOSS train 0.14740040724686646 valid 0.20328946614807303
LOSS train 0.14740040724686646 valid 0.20333066711455217
LOSS train 0.14740040724686646 valid 0.20338012540682418
LOSS train 0.14740040724686646 valid 0.20356800586593393
LOSS train 0.14740040724686646 valid 0.2036771165525041
LOSS train 0.14740040724686646 valid 0.203697357945114
LOSS train 0.14740040724686646 valid 0.20362570004597788
LOSS train 0.14740040724686646 valid 0.2035345566799363
LOSS train 0.14740040724686646 valid 0.20365964442491533
LOSS train 0.14740040724686646 valid 0.20372760076209367
LOSS train 0.14740040724686646 valid 0.2037631164467524
LOSS train 0.14740040724686646 valid 0.2036686165059508
LOSS train 0.14740040724686646 valid 0.203730625547762
LOSS train 0.14740040724686646 valid 0.2037262492904476
LOSS train 0.14740040724686646 valid 0.20374602946685627
LOSS train 0.14740040724686646 valid 0.20365667580862454
LOSS train 0.14740040724686646 valid 0.20373714166556217
LOSS train 0.14740040724686646 valid 0.20368664915957507
LOSS train 0.14740040724686646 valid 0.20356313626353556
LOSS train 0.14740040724686646 valid 0.20352642597823306
LOSS train 0.14740040724686646 valid 0.20377659570169812
LOSS train 0.14740040724686646 valid 0.20364822056356946
LOSS train 0.14740040724686646 valid 0.20367166366089473
LOSS train 0.14740040724686646 valid 0.20376085189153564
LOSS train 0.14740040724686646 valid 0.20383735135533756
LOSS train 0.14740040724686646 valid 0.20382402563809457
LOSS train 0.14740040724686646 valid 0.20385429724606116
LOSS train 0.14740040724686646 valid 0.20389797530209708
LOSS train 0.14740040724686646 valid 0.20378695146905051
LOSS train 0.14740040724686646 valid 0.20370199138168038
LOSS train 0.14740040724686646 valid 0.20353060845723925
LOSS train 0.14740040724686646 valid 0.20340598846748198
LOSS train 0.14740040724686646 valid 0.20336888752279492
LOSS train 0.14740040724686646 valid 0.2034694720398296
LOSS train 0.14740040724686646 valid 0.20360554403800896
LOSS train 0.14740040724686646 valid 0.20353859394035614
LOSS train 0.14740040724686646 valid 0.20352241016334768
LOSS train 0.14740040724686646 valid 0.203587578349216
LOSS train 0.14740040724686646 valid 0.20341735295951366
LOSS train 0.14740040724686646 valid 0.20329258312534182
LOSS train 0.14740040724686646 valid 0.20328115228008717
LOSS train 0.14740040724686646 valid 0.20329920631626047
LOSS train 0.14740040724686646 valid 0.2031207370086455
LOSS train 0.14740040724686646 valid 0.20322049317652718
LOSS train 0.14740040724686646 valid 0.20322279892601333
LOSS train 0.14740040724686646 valid 0.203354074564545
LOSS train 0.14740040724686646 valid 0.20330850976622766
LOSS train 0.14740040724686646 valid 0.2032842923303789
LOSS train 0.14740040724686646 valid 0.20318235991329983
LOSS train 0.14740040724686646 valid 0.20314203054224914
LOSS train 0.14740040724686646 valid 0.2031203134827418
LOSS train 0.14740040724686646 valid 0.2030684272075269
LOSS train 0.14740040724686646 valid 0.20305243149703864
LOSS train 0.14740040724686646 valid 0.2030433206234948
LOSS train 0.14740040724686646 valid 0.2030813341104501
LOSS train 0.14740040724686646 valid 0.2030886100097136
LOSS train 0.14740040724686646 valid 0.20311964053235598
LOSS train 0.14740040724686646 valid 0.20313348175091886
LOSS train 0.14740040724686646 valid 0.2031700370212396
LOSS train 0.14740040724686646 valid 0.20313886040666967
LOSS train 0.14740040724686646 valid 0.2031326669434838
LOSS train 0.14740040724686646 valid 0.20316392463622707
LOSS train 0.14740040724686646 valid 0.20312675338630615
LOSS train 0.14740040724686646 valid 0.2030204521339448
LOSS train 0.14740040724686646 valid 0.20313522779668858
LOSS train 0.14740040724686646 valid 0.20303642264406532
LOSS train 0.14740040724686646 valid 0.20313004665560536
LOSS train 0.14740040724686646 valid 0.20315315369456333
LOSS train 0.14740040724686646 valid 0.20318577674127394
LOSS train 0.14740040724686646 valid 0.20323117125264317
LOSS train 0.14740040724686646 valid 0.20320238110919794
LOSS train 0.14740040724686646 valid 0.20321532703055362
LOSS train 0.14740040724686646 valid 0.2032182050073982
LOSS train 0.14740040724686646 valid 0.20323233722694337
LOSS train 0.14740040724686646 valid 0.20322795226415502
LOSS train 0.14740040724686646 valid 0.20320296028999124
LOSS train 0.14740040724686646 valid 0.20327498229607097
LOSS train 0.14740040724686646 valid 0.2032742227506488
LOSS train 0.14740040724686646 valid 0.2031611363403499
LOSS train 0.14740040724686646 valid 0.20316755469900055
LOSS train 0.14740040724686646 valid 0.20309860819244976
LOSS train 0.14740040724686646 valid 0.20305018385312876
LOSS train 0.14740040724686646 valid 0.20299182731060333
LOSS train 0.14740040724686646 valid 0.20282371227557844
LOSS train 0.14740040724686646 valid 0.20290152961673913
LOSS train 0.14740040724686646 valid 0.20279008769843193
LOSS train 0.14740040724686646 valid 0.2028511917082275
LOSS train 0.14740040724686646 valid 0.2028990945464572
LOSS train 0.14740040724686646 valid 0.20297543221350872
LOSS train 0.14740040724686646 valid 0.2028942501616982
LOSS train 0.14740040724686646 valid 0.2028779577329216
LOSS train 0.14740040724686646 valid 0.20289692873353357
LOSS train 0.14740040724686646 valid 0.20284258458250298
LOSS train 0.14740040724686646 valid 0.20272477887459656
LOSS train 0.14740040724686646 valid 0.20264809636310452
LOSS train 0.14740040724686646 valid 0.20279447530957287
LOSS train 0.14740040724686646 valid 0.2027315185708407
LOSS train 0.14740040724686646 valid 0.20263426770678664
LOSS train 0.14740040724686646 valid 0.20268151974853346
LOSS train 0.14740040724686646 valid 0.20272847909556804
LOSS train 0.14740040724686646 valid 0.2027528362664563
LOSS train 0.14740040724686646 valid 0.2026251292089679
LOSS train 0.14740040724686646 valid 0.20271993441463904
LOSS train 0.14740040724686646 valid 0.20278864965058754
LOSS train 0.14740040724686646 valid 0.20270347328199817
LOSS train 0.14740040724686646 valid 0.2026486926704044
LOSS train 0.14740040724686646 valid 0.20262265569348445
LOSS train 0.14740040724686646 valid 0.20259898683232358
LOSS train 0.14740040724686646 valid 0.20263916041169847
LOSS train 0.14740040724686646 valid 0.20268997401423602
LOSS train 0.14740040724686646 valid 0.20272788435051387
LOSS train 0.14740040724686646 valid 0.20279695473548034
LOSS train 0.14740040724686646 valid 0.20268140762707607
LOSS train 0.14740040724686646 valid 0.20256980716342657
LOSS train 0.14740040724686646 valid 0.20256047059645813
LOSS train 0.14740040724686646 valid 0.20259929195839485
LOSS train 0.14740040724686646 valid 0.20269712556007854
LOSS train 0.14740040724686646 valid 0.20269895878342864
LOSS train 0.14740040724686646 valid 0.2029488033718533
LOSS train 0.14740040724686646 valid 0.20291312443417525
LOSS train 0.14740040724686646 valid 0.20303219173988585
LOSS train 0.14740040724686646 valid 0.2030557629654887
LOSS train 0.14740040724686646 valid 0.2029553352558351
LOSS train 0.14740040724686646 valid 0.202965432487122
LOSS train 0.14740040724686646 valid 0.2029589395054051
LOSS train 0.14740040724686646 valid 0.20301670631855645
LOSS train 0.14740040724686646 valid 0.20290093468097242
LOSS train 0.14740040724686646 valid 0.2030032625608651
EPOCH 6:
  batch 1 loss: 0.1611129641532898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13471532985568047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1344629650314649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13429523073136806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13971439152956008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13852009053031603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13810933807066508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13507876452058554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13633175608184603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13437802940607071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13590431213378906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13782703503966331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13777674734592438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13555936834641866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13487920463085173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13603499624878168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13687845889259787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1381051176124149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13934014423897392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13923017978668212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1402759509427207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13967787406661294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13793524030757986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13782003490875164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1377991458773613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13957688527611586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14022624189103092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14081367344728538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1418042591419713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14150156900286676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14098550355242145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.140096454648301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14010745490139181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.14116599301204963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14166534734623773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.14300356577667925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.143894356046174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14317916706204414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14216070775038156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1425950461998582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14249970436823078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1424217154937131
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14241774501495583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14255069077692248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14222967906130685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14207080443916115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14204709025773596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14259291269506016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14283397656922436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1426758898794651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14243261092433743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14182369539944026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14186112568625864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.1420745793040152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14235352941534735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14271125343761273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1430111696061335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1427605921595261
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14324173672219453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.1438926273336013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14447732086552947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1441291855467904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14370004868223554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14366251567844301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.1437147678090976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14352463851823952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14372112913363017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14393304825267372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14359817513521167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14331713180456843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1433874067496246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14305331951214206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1433006669968775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14370968071995555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14368828197320302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14367446558255897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14386103079690563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14406322917112938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14421323658544807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1443925380706787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14444358426111717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1446821357055408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14454261312283664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14487357100560552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14501265743199518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1446478422297988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14473897013170967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14459212293679063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14471972859307622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1448164087202814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14482673888023084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1445474850580744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14455670975549248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1445487031435713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14429669615469481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14425860966245332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14423600607311604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14414211377805594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14444852537579006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14446355730295182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14453389857075002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14460590014270708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14462160673534985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14486385853244707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14486524377550397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14482875941496975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14477287330360056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1447216526225761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14472886181752617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1449420854449272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14485584870652035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1446857818269304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14456621656375648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14463538375862858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14464264188123785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14481620580471796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1448889606528812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14474500033815027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14489985276170136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14478158056735993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14488605403703106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14492186085611095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14493627458568512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14492048034744878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1450765415430069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14507031511692775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14523147386828744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14515042665880173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14501266123712525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14488535053454912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1448290978224223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14453473427530492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14453285170677013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1443131601632531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.144246334389404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14452446471242344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1446121453586286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14454369009404944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14466478155671264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14444775160934245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14449534924529123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14456609368953907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14453974329716676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14434052512256634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14454272065697046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14457339951640938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14435306520891839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14457158537933956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14477316080123787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14476130440831184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14485322042608892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1449555413013226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14508079104368982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1450099999738204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14492427355820134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14490092722460246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1448455594812229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14492403909186774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14492543196340776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14483935120515526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14479835835858162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14493841792882226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.145006404835388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14490559483628448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14469445469704542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14453972400311965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14455361824906515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14441857828448215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1443008061432274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1444014184176922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14442832127474903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14431949198072733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14422025049180653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1442459331418591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14429569342306683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14438557586717335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14473422886119724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14473982318566087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14477431670081017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14481750937799612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14481962694482908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1448901133959765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14494460010951984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14488661576710318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14487561821132092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14491922188029494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14501004482813698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14491491523036298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1449733784510976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14501023280777428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14495527264491426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14517718479813388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1451794556462703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14520560485339656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1452040644410329
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14511949709635608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1448895952801414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14487927290375788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14475569761727922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14487049352377654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.144832033756657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14481079168986566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14486527299792895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14486237524040774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1447901668708499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1448603978073134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1447290795436804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14457142610962576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.144416611017793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14453551882789248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14452790260597428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1447356511258854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1448225071872344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1448152358426112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1447486816450607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14474989942930364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14476981079248788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.144753804515808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1448525095094829
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1446508238600059
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1446875444701894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1446721307799086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14474886117895622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1447184156692986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14477982617086835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14471124636784063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14485018555944712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14475302568130327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14466091899372086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14471370130777358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14473309029232373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14476829696575116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14470465423723147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14469448228677115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14458991799582827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14464358647622294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1445916624826218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14469915149467333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14458724653745794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14454142255708574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14449545323725063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14451219126089546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.144486920854437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14447822143919156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14449578800371715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14432458182781693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14436877947225263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14448773833893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14449760219059796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14453418096899986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14455248810261845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.144561078369854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.144523580525468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14446336556020684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1445651709156878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14456893180613406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14446762949228287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14440745851674744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14446899568137056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.144482295988844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14467551016830393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14475660136979046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1446181775102597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14466306231351514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14469438456701783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14456682352531225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1445977481061153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14466197585770443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1445606534808985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14462579014124693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14456127627969229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14449756154242685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1444248277079928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14446883349523057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14433972486040808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14428977368642454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14440403370327898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14432666590102286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14426030325419587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14430645188050611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14422368183997178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14425321147902637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14420212860558143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14424194287980946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1443822154611872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14438144198351807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.144449705524104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1443727739776174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14424791963042685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1442282046737342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.144232078823437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14426025554333646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14438572978607217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14435252653700964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14442036798444846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14450062312991233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14444486310185006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14459262393264963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14460928299554615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14452067290743192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1444693637943743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14434179814150003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14437822454144852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14441348948074798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1444611342471154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14443188099787127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14438420774396934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14438338101200468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14449073911967014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1444789784329553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14442530324700562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14437274355441332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14441565219491434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14446001881911497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14459841542300725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1445952265087185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.1446429081378675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14462417713220013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14463802142109616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14459164205472916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1445877392111909
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14469963957711776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14459892140914782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14454839702833583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1446458211541176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14461957916983068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1446169865650868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14461944181835507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14468295942414494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14463535730134358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1445979755765722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14467265903500906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14471916252517844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14473532401248365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14468382292274218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14474988110097392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14475301931714446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14468831875355992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14461685240180794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14465887518051793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1446836861478618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1446538468855515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14466551791214038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1446502943588204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1447362035728883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14469572869425565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14476998800932503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14478784412059975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14471890802967513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14476338341832162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1448563417775339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14480591859583827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1448020035712307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14495207821834558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14492072141926055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14487528949557396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1448022739893916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.144755492746497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14474623162600322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14473617701894706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14471569554132108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14481302433086363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1448062691970962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14476596138307027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14479967069952454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1448266275756346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1447479575263382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14482463748477722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14484705259079533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1449235102413474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14496143489594088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14494591973401527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1448942415476165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14494647370541797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1449501563111941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14494536351412535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14497219411463574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14499628892730151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14508399636654237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1451481442506376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1451407557358266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1451574258350265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14515503130211221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14517455394767845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14510596744812929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14512061302603219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1450136044382741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14507611249525523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14506070771683152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14507626994298053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14515250338160474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1451649875285066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14516469908276283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14514296948153355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14522593640073944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14519263040086236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14520503479227312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14518923443465975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1452020026164545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14521516483277083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1453003904171418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.1453741209868768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14534944410507494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1453482319075282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1453380035765377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14532570841864412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1453124818183866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14525152330158972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1452810476855136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14526635124915985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14536356436510156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14536134970998302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14526265634102048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14517220766576017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14518213808895594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14522433651682848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14512342688062493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14508560750829547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14508143538578028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1450456109962293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.145064120921981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1451155226765933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14507786018980873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14512008644710733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14512483353123945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14507797760932659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1450551963417815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14511525441204834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1450938224827215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1451337838068951
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14511505918892798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14521658612001273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14521942948427816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14527319263546698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14525718882508662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14532343405854264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14534328852351797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14540136940607198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14540563253919192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14542708828706633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14542479683009404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14542046595550231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14546880188227776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14543164268068903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1453758621651135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14533125393660615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14531041160059188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14531496112301415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1453131403948257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14525693385137453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14526111348439213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1452123842045533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14518180668814556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14509598887833206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1450165202001949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14501423444272132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.145001224537647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14495598573611812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14487903401223143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14490794798602227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14490367585304242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14487095831921606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14486505850731168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.144933866240598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14488989050670337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14493263103547527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14496059891243307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14496864531284723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14497352501095484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14488400114660568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1449184576701966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14486897675225796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14486897675225796 valid 0.17579367756843567
LOSS train 0.14486897675225796 valid 0.2017146646976471
LOSS train 0.14486897675225796 valid 0.2119984875122706
LOSS train 0.14486897675225796 valid 0.20874061062932014
LOSS train 0.14486897675225796 valid 0.20845263004302977
LOSS train 0.14486897675225796 valid 0.21404759337504706
LOSS train 0.14486897675225796 valid 0.20860748844487326
LOSS train 0.14486897675225796 valid 0.20820903591811657
LOSS train 0.14486897675225796 valid 0.20794888503021663
LOSS train 0.14486897675225796 valid 0.20909298658370973
LOSS train 0.14486897675225796 valid 0.20556765523823825
LOSS train 0.14486897675225796 valid 0.20809303099910417
LOSS train 0.14486897675225796 valid 0.20673144322175246
LOSS train 0.14486897675225796 valid 0.207048522574561
LOSS train 0.14486897675225796 valid 0.20574678480625153
LOSS train 0.14486897675225796 valid 0.20611759182065725
LOSS train 0.14486897675225796 valid 0.20628383142106674
LOSS train 0.14486897675225796 valid 0.20663344694508445
LOSS train 0.14486897675225796 valid 0.20851973640291313
LOSS train 0.14486897675225796 valid 0.20798326283693314
LOSS train 0.14486897675225796 valid 0.20845374039241246
LOSS train 0.14486897675225796 valid 0.2068596685474569
LOSS train 0.14486897675225796 valid 0.20506858372169992
LOSS train 0.14486897675225796 valid 0.20601529938479266
LOSS train 0.14486897675225796 valid 0.20456117510795593
LOSS train 0.14486897675225796 valid 0.2040954169172507
LOSS train 0.14486897675225796 valid 0.2037337416851962
LOSS train 0.14486897675225796 valid 0.20381133630871773
LOSS train 0.14486897675225796 valid 0.2024722613137344
LOSS train 0.14486897675225796 valid 0.2015870268146197
LOSS train 0.14486897675225796 valid 0.2007808675689082
LOSS train 0.14486897675225796 valid 0.2015656353905797
LOSS train 0.14486897675225796 valid 0.20033710020961185
LOSS train 0.14486897675225796 valid 0.1998688306878595
LOSS train 0.14486897675225796 valid 0.2002909928560257
LOSS train 0.14486897675225796 valid 0.2003681187828382
LOSS train 0.14486897675225796 valid 0.19940575232376923
LOSS train 0.14486897675225796 valid 0.19939446213998294
LOSS train 0.14486897675225796 valid 0.199104993389203
LOSS train 0.14486897675225796 valid 0.1987628757953644
LOSS train 0.14486897675225796 valid 0.19860636606449034
LOSS train 0.14486897675225796 valid 0.19977458956695737
LOSS train 0.14486897675225796 valid 0.20062350603037102
LOSS train 0.14486897675225796 valid 0.19973499429496852
LOSS train 0.14486897675225796 valid 0.1992172913418876
LOSS train 0.14486897675225796 valid 0.19770292549029642
LOSS train 0.14486897675225796 valid 0.19699337634634465
LOSS train 0.14486897675225796 valid 0.19832265935838223
LOSS train 0.14486897675225796 valid 0.19814679999740756
LOSS train 0.14486897675225796 valid 0.19849678099155427
LOSS train 0.14486897675225796 valid 0.19824859003225961
LOSS train 0.14486897675225796 valid 0.19809981063008308
LOSS train 0.14486897675225796 valid 0.19904603795060571
LOSS train 0.14486897675225796 valid 0.1989646860294872
LOSS train 0.14486897675225796 valid 0.1991194248199463
LOSS train 0.14486897675225796 valid 0.1987743547984532
LOSS train 0.14486897675225796 valid 0.19810539248742556
LOSS train 0.14486897675225796 valid 0.19882293482278957
LOSS train 0.14486897675225796 valid 0.1989861375699609
LOSS train 0.14486897675225796 valid 0.1986690031985442
LOSS train 0.14486897675225796 valid 0.19853423535823822
LOSS train 0.14486897675225796 valid 0.1981733962893486
LOSS train 0.14486897675225796 valid 0.19823355830851055
LOSS train 0.14486897675225796 valid 0.1981760209891945
LOSS train 0.14486897675225796 valid 0.19736371132043692
LOSS train 0.14486897675225796 valid 0.19723653477249722
LOSS train 0.14486897675225796 valid 0.19785507468145286
LOSS train 0.14486897675225796 valid 0.19722502503324957
LOSS train 0.14486897675225796 valid 0.1975766748621844
LOSS train 0.14486897675225796 valid 0.1980202295950481
LOSS train 0.14486897675225796 valid 0.1981063424281671
LOSS train 0.14486897675225796 valid 0.19858183008101252
LOSS train 0.14486897675225796 valid 0.19875712998925824
LOSS train 0.14486897675225796 valid 0.19873861102639018
LOSS train 0.14486897675225796 valid 0.19853488266468047
LOSS train 0.14486897675225796 valid 0.19866227711501874
LOSS train 0.14486897675225796 valid 0.19874687550903916
LOSS train 0.14486897675225796 valid 0.19856948042527223
LOSS train 0.14486897675225796 valid 0.19887413838996162
LOSS train 0.14486897675225796 valid 0.1985945025458932
LOSS train 0.14486897675225796 valid 0.19872357503131585
LOSS train 0.14486897675225796 valid 0.19896002186507714
LOSS train 0.14486897675225796 valid 0.19868276600378104
LOSS train 0.14486897675225796 valid 0.19857206585861387
LOSS train 0.14486897675225796 valid 0.19882515914299909
LOSS train 0.14486897675225796 valid 0.19891847843347593
LOSS train 0.14486897675225796 valid 0.19853951191080027
LOSS train 0.14486897675225796 valid 0.19840134714137425
LOSS train 0.14486897675225796 valid 0.19849944449542614
LOSS train 0.14486897675225796 valid 0.19833854618999694
LOSS train 0.14486897675225796 valid 0.19839721999980592
LOSS train 0.14486897675225796 valid 0.19847473081039346
LOSS train 0.14486897675225796 valid 0.19874386761778146
LOSS train 0.14486897675225796 valid 0.1990958217610704
LOSS train 0.14486897675225796 valid 0.1994551757448598
LOSS train 0.14486897675225796 valid 0.2003178309338788
LOSS train 0.14486897675225796 valid 0.20048101323167072
LOSS train 0.14486897675225796 valid 0.20096286918435777
LOSS train 0.14486897675225796 valid 0.20081325627938665
LOSS train 0.14486897675225796 valid 0.2010991258919239
LOSS train 0.14486897675225796 valid 0.20103229508541598
LOSS train 0.14486897675225796 valid 0.2014581663351433
LOSS train 0.14486897675225796 valid 0.20102985738550574
LOSS train 0.14486897675225796 valid 0.20088395949166554
LOSS train 0.14486897675225796 valid 0.20113788630281176
LOSS train 0.14486897675225796 valid 0.20117512514006416
LOSS train 0.14486897675225796 valid 0.20099184641214174
LOSS train 0.14486897675225796 valid 0.20105537841165508
LOSS train 0.14486897675225796 valid 0.20052218601244307
LOSS train 0.14486897675225796 valid 0.20030828497626565
LOSS train 0.14486897675225796 valid 0.2003502621575519
LOSS train 0.14486897675225796 valid 0.20045783610216208
LOSS train 0.14486897675225796 valid 0.20025180082405564
LOSS train 0.14486897675225796 valid 0.2004836961104159
LOSS train 0.14486897675225796 valid 0.20080095477726148
LOSS train 0.14486897675225796 valid 0.20076820614009067
LOSS train 0.14486897675225796 valid 0.20080984453869682
LOSS train 0.14486897675225796 valid 0.20081208646297455
LOSS train 0.14486897675225796 valid 0.2005678033377944
LOSS train 0.14486897675225796 valid 0.20054585387309393
LOSS train 0.14486897675225796 valid 0.2006053478757212
LOSS train 0.14486897675225796 valid 0.2006525501120286
LOSS train 0.14486897675225796 valid 0.20057022389842244
LOSS train 0.14486897675225796 valid 0.20066609966658777
LOSS train 0.14486897675225796 valid 0.2007944074869156
LOSS train 0.14486897675225796 valid 0.20064841045273674
LOSS train 0.14486897675225796 valid 0.20049327295126879
LOSS train 0.14486897675225796 valid 0.20039439585525542
LOSS train 0.14486897675225796 valid 0.20029676595861598
LOSS train 0.14486897675225796 valid 0.2000425377717385
LOSS train 0.14486897675225796 valid 0.2000749264737122
LOSS train 0.14486897675225796 valid 0.20023976503448052
LOSS train 0.14486897675225796 valid 0.20021495559161767
LOSS train 0.14486897675225796 valid 0.20025291905474307
LOSS train 0.14486897675225796 valid 0.20030176164927305
LOSS train 0.14486897675225796 valid 0.20048697566723123
LOSS train 0.14486897675225796 valid 0.20031776937255025
LOSS train 0.14486897675225796 valid 0.19997664562601974
LOSS train 0.14486897675225796 valid 0.1999460840396744
LOSS train 0.14486897675225796 valid 0.19981348578419003
LOSS train 0.14486897675225796 valid 0.20008901607060264
LOSS train 0.14486897675225796 valid 0.20033434022900085
LOSS train 0.14486897675225796 valid 0.20044443938698803
LOSS train 0.14486897675225796 valid 0.20038373199188048
LOSS train 0.14486897675225796 valid 0.200177156205835
LOSS train 0.14486897675225796 valid 0.2002272098644139
LOSS train 0.14486897675225796 valid 0.20021129151185355
LOSS train 0.14486897675225796 valid 0.1999686229470614
LOSS train 0.14486897675225796 valid 0.20018354338287508
LOSS train 0.14486897675225796 valid 0.20012374520301818
LOSS train 0.14486897675225796 valid 0.2000321728109524
LOSS train 0.14486897675225796 valid 0.20006798982228102
LOSS train 0.14486897675225796 valid 0.1998320520898096
LOSS train 0.14486897675225796 valid 0.19986560040867174
LOSS train 0.14486897675225796 valid 0.1996771361558668
LOSS train 0.14486897675225796 valid 0.19988485931968078
LOSS train 0.14486897675225796 valid 0.19969958826235146
LOSS train 0.14486897675225796 valid 0.19963181839336322
LOSS train 0.14486897675225796 valid 0.19994159230271225
LOSS train 0.14486897675225796 valid 0.19992499640211464
LOSS train 0.14486897675225796 valid 0.20011065974368814
LOSS train 0.14486897675225796 valid 0.200107684565915
LOSS train 0.14486897675225796 valid 0.1998589551704793
LOSS train 0.14486897675225796 valid 0.19988820792698278
LOSS train 0.14486897675225796 valid 0.20004545359900502
LOSS train 0.14486897675225796 valid 0.19994640628616495
LOSS train 0.14486897675225796 valid 0.2000163118104021
LOSS train 0.14486897675225796 valid 0.199677535820575
LOSS train 0.14486897675225796 valid 0.19954331968663006
LOSS train 0.14486897675225796 valid 0.19939294562620274
LOSS train 0.14486897675225796 valid 0.19920953393679613
LOSS train 0.14486897675225796 valid 0.19927926907359167
LOSS train 0.14486897675225796 valid 0.19933935336639427
LOSS train 0.14486897675225796 valid 0.1993522621776866
LOSS train 0.14486897675225796 valid 0.19908191638333456
LOSS train 0.14486897675225796 valid 0.19907559064978902
LOSS train 0.14486897675225796 valid 0.19895377645721543
LOSS train 0.14486897675225796 valid 0.19896860814161516
LOSS train 0.14486897675225796 valid 0.19896770705723896
LOSS train 0.14486897675225796 valid 0.19897090478075874
LOSS train 0.14486897675225796 valid 0.19883589759386705
LOSS train 0.14486897675225796 valid 0.19872726573721394
LOSS train 0.14486897675225796 valid 0.1988460556731198
LOSS train 0.14486897675225796 valid 0.19889987543549226
LOSS train 0.14486897675225796 valid 0.19897170622606536
LOSS train 0.14486897675225796 valid 0.19911456781048928
LOSS train 0.14486897675225796 valid 0.19906100989344286
LOSS train 0.14486897675225796 valid 0.19903492341016202
LOSS train 0.14486897675225796 valid 0.1987038723060063
LOSS train 0.14486897675225796 valid 0.19868556174792742
LOSS train 0.14486897675225796 valid 0.1987843717111967
LOSS train 0.14486897675225796 valid 0.19881016248837113
LOSS train 0.14486897675225796 valid 0.1985813723635797
LOSS train 0.14486897675225796 valid 0.19865258552671708
LOSS train 0.14486897675225796 valid 0.19855811771674034
LOSS train 0.14486897675225796 valid 0.19877873338302787
LOSS train 0.14486897675225796 valid 0.19891176331164268
LOSS train 0.14486897675225796 valid 0.19899280850935464
LOSS train 0.14486897675225796 valid 0.19909009921490847
LOSS train 0.14486897675225796 valid 0.1992920822650194
LOSS train 0.14486897675225796 valid 0.1993135422171645
LOSS train 0.14486897675225796 valid 0.19950349437128198
LOSS train 0.14486897675225796 valid 0.1996247332377974
LOSS train 0.14486897675225796 valid 0.1994277786682634
LOSS train 0.14486897675225796 valid 0.19926393911605927
LOSS train 0.14486897675225796 valid 0.19922140638515787
LOSS train 0.14486897675225796 valid 0.19907976816529813
LOSS train 0.14486897675225796 valid 0.19886904181196138
LOSS train 0.14486897675225796 valid 0.19890995088376498
LOSS train 0.14486897675225796 valid 0.19914931740079608
LOSS train 0.14486897675225796 valid 0.19939405531114876
LOSS train 0.14486897675225796 valid 0.19926137519332598
LOSS train 0.14486897675225796 valid 0.19924785553569524
LOSS train 0.14486897675225796 valid 0.199203151408757
LOSS train 0.14486897675225796 valid 0.1989334387834682
LOSS train 0.14486897675225796 valid 0.19871144714178862
LOSS train 0.14486897675225796 valid 0.19874954319769336
LOSS train 0.14486897675225796 valid 0.1986946801253415
LOSS train 0.14486897675225796 valid 0.19877609392823695
LOSS train 0.14486897675225796 valid 0.19871154183691198
LOSS train 0.14486897675225796 valid 0.19870014495439658
LOSS train 0.14486897675225796 valid 0.19882623670068947
LOSS train 0.14486897675225796 valid 0.19870962062224146
LOSS train 0.14486897675225796 valid 0.19858473645789282
LOSS train 0.14486897675225796 valid 0.19830086145136092
LOSS train 0.14486897675225796 valid 0.1981257855628444
LOSS train 0.14486897675225796 valid 0.19813684981299917
LOSS train 0.14486897675225796 valid 0.19818385409419997
LOSS train 0.14486897675225796 valid 0.19814254805510742
LOSS train 0.14486897675225796 valid 0.19828998977723328
LOSS train 0.14486897675225796 valid 0.19833203427719348
LOSS train 0.14486897675225796 valid 0.19848702565349383
LOSS train 0.14486897675225796 valid 0.19862488142410573
LOSS train 0.14486897675225796 valid 0.19859684659884527
LOSS train 0.14486897675225796 valid 0.19854935382274871
LOSS train 0.14486897675225796 valid 0.19836414643263411
LOSS train 0.14486897675225796 valid 0.1984026176768516
LOSS train 0.14486897675225796 valid 0.19844926599444462
LOSS train 0.14486897675225796 valid 0.198441728700155
LOSS train 0.14486897675225796 valid 0.1980557767363886
LOSS train 0.14486897675225796 valid 0.19796628845059525
LOSS train 0.14486897675225796 valid 0.1980390752827333
LOSS train 0.14486897675225796 valid 0.19807994289040076
LOSS train 0.14486897675225796 valid 0.19813463467432826
LOSS train 0.14486897675225796 valid 0.19833340775601718
LOSS train 0.14486897675225796 valid 0.19843592207000507
LOSS train 0.14486897675225796 valid 0.19846348304618225
LOSS train 0.14486897675225796 valid 0.19839086074141726
LOSS train 0.14486897675225796 valid 0.1982911172581006
LOSS train 0.14486897675225796 valid 0.1984170682132244
LOSS train 0.14486897675225796 valid 0.19848759071641708
LOSS train 0.14486897675225796 valid 0.19852944957240234
LOSS train 0.14486897675225796 valid 0.19844110289463412
LOSS train 0.14486897675225796 valid 0.19850469313151253
LOSS train 0.14486897675225796 valid 0.19850254903237025
LOSS train 0.14486897675225796 valid 0.1985298240033444
LOSS train 0.14486897675225796 valid 0.19843709193430986
LOSS train 0.14486897675225796 valid 0.19852916991641356
LOSS train 0.14486897675225796 valid 0.19845524047677582
LOSS train 0.14486897675225796 valid 0.1983428044674488
LOSS train 0.14486897675225796 valid 0.19831955110318816
LOSS train 0.14486897675225796 valid 0.19856844259468653
LOSS train 0.14486897675225796 valid 0.19843821859291752
LOSS train 0.14486897675225796 valid 0.19846418288282372
LOSS train 0.14486897675225796 valid 0.1985521637887325
LOSS train 0.14486897675225796 valid 0.19863317142191686
LOSS train 0.14486897675225796 valid 0.19861934043829807
LOSS train 0.14486897675225796 valid 0.19865068290104618
LOSS train 0.14486897675225796 valid 0.19869456357118365
LOSS train 0.14486897675225796 valid 0.19858386800796898
LOSS train 0.14486897675225796 valid 0.1985018394365082
LOSS train 0.14486897675225796 valid 0.19833473579081543
LOSS train 0.14486897675225796 valid 0.19820606945962696
LOSS train 0.14486897675225796 valid 0.19816293341726282
LOSS train 0.14486897675225796 valid 0.19827608094973997
LOSS train 0.14486897675225796 valid 0.19841214444866215
LOSS train 0.14486897675225796 valid 0.1983362298848827
LOSS train 0.14486897675225796 valid 0.19831900408478093
LOSS train 0.14486897675225796 valid 0.19838213164853366
LOSS train 0.14486897675225796 valid 0.1982091843283602
LOSS train 0.14486897675225796 valid 0.19808335562511695
LOSS train 0.14486897675225796 valid 0.19806705745505104
LOSS train 0.14486897675225796 valid 0.19808167153344136
LOSS train 0.14486897675225796 valid 0.19790891209013864
LOSS train 0.14486897675225796 valid 0.19801022428692433
LOSS train 0.14486897675225796 valid 0.1980133740478879
LOSS train 0.14486897675225796 valid 0.19814547109790795
LOSS train 0.14486897675225796 valid 0.1980981031794929
LOSS train 0.14486897675225796 valid 0.19807262257205574
LOSS train 0.14486897675225796 valid 0.1979718572363771
LOSS train 0.14486897675225796 valid 0.19793650648745476
LOSS train 0.14486897675225796 valid 0.19791510555740088
LOSS train 0.14486897675225796 valid 0.19786448575835994
LOSS train 0.14486897675225796 valid 0.19784324106817341
LOSS train 0.14486897675225796 valid 0.19783611431465312
LOSS train 0.14486897675225796 valid 0.19788263690330693
LOSS train 0.14486897675225796 valid 0.19789786127580938
LOSS train 0.14486897675225796 valid 0.1979273904940026
LOSS train 0.14486897675225796 valid 0.19793059587179618
LOSS train 0.14486897675225796 valid 0.19796445441742738
LOSS train 0.14486897675225796 valid 0.19793120474613385
LOSS train 0.14486897675225796 valid 0.19792123545972717
LOSS train 0.14486897675225796 valid 0.1979671519237383
LOSS train 0.14486897675225796 valid 0.19792648975884444
LOSS train 0.14486897675225796 valid 0.197814443683038
LOSS train 0.14486897675225796 valid 0.19793746827377212
LOSS train 0.14486897675225796 valid 0.19783416849102958
LOSS train 0.14486897675225796 valid 0.19793195971711117
LOSS train 0.14486897675225796 valid 0.1979562977515764
LOSS train 0.14486897675225796 valid 0.1979902511402484
LOSS train 0.14486897675225796 valid 0.19803128655029648
LOSS train 0.14486897675225796 valid 0.19800590420476136
LOSS train 0.14486897675225796 valid 0.19801087989308203
LOSS train 0.14486897675225796 valid 0.1980100561431639
LOSS train 0.14486897675225796 valid 0.19802629131646382
LOSS train 0.14486897675225796 valid 0.1980240518297953
LOSS train 0.14486897675225796 valid 0.19799472619588443
LOSS train 0.14486897675225796 valid 0.1980645056087641
LOSS train 0.14486897675225796 valid 0.19806621555344064
LOSS train 0.14486897675225796 valid 0.19795264129061252
LOSS train 0.14486897675225796 valid 0.19795760222220346
LOSS train 0.14486897675225796 valid 0.1978891003900063
LOSS train 0.14486897675225796 valid 0.19784210329362115
LOSS train 0.14486897675225796 valid 0.19779530556205613
LOSS train 0.14486897675225796 valid 0.19762675228027196
LOSS train 0.14486897675225796 valid 0.19770366842487108
LOSS train 0.14486897675225796 valid 0.1975893527952903
LOSS train 0.14486897675225796 valid 0.1976523608105575
LOSS train 0.14486897675225796 valid 0.1977040549587334
LOSS train 0.14486897675225796 valid 0.19779142866080457
LOSS train 0.14486897675225796 valid 0.19771472841861384
LOSS train 0.14486897675225796 valid 0.19770549823451472
LOSS train 0.14486897675225796 valid 0.19772474456567307
LOSS train 0.14486897675225796 valid 0.19766489628278566
LOSS train 0.14486897675225796 valid 0.1975432653258096
LOSS train 0.14486897675225796 valid 0.19747001263091252
LOSS train 0.14486897675225796 valid 0.19761282079562942
LOSS train 0.14486897675225796 valid 0.1975512586798541
LOSS train 0.14486897675225796 valid 0.19745378198437283
LOSS train 0.14486897675225796 valid 0.1974988626425757
LOSS train 0.14486897675225796 valid 0.19754588942636142
LOSS train 0.14486897675225796 valid 0.1975685110784065
LOSS train 0.14486897675225796 valid 0.19744191770564015
LOSS train 0.14486897675225796 valid 0.19753770133870285
LOSS train 0.14486897675225796 valid 0.19760650307803915
LOSS train 0.14486897675225796 valid 0.19752206922524926
LOSS train 0.14486897675225796 valid 0.19747096813291912
LOSS train 0.14486897675225796 valid 0.19744297680070347
LOSS train 0.14486897675225796 valid 0.19742356806481123
LOSS train 0.14486897675225796 valid 0.1974618037896497
LOSS train 0.14486897675225796 valid 0.19750825561943897
LOSS train 0.14486897675225796 valid 0.19754184145395728
LOSS train 0.14486897675225796 valid 0.19760482479678335
LOSS train 0.14486897675225796 valid 0.19748958770585598
LOSS train 0.14486897675225796 valid 0.19739017954594654
LOSS train 0.14486897675225796 valid 0.1973818504785219
LOSS train 0.14486897675225796 valid 0.19741842649480543
LOSS train 0.14486897675225796 valid 0.1975168254247258
LOSS train 0.14486897675225796 valid 0.19751761108636856
LOSS train 0.14486897675225796 valid 0.19777289560685554
LOSS train 0.14486897675225796 valid 0.19773426654018525
LOSS train 0.14486897675225796 valid 0.19786166850456877
LOSS train 0.14486897675225796 valid 0.19788080406665146
LOSS train 0.14486897675225796 valid 0.19778259575448848
LOSS train 0.14486897675225796 valid 0.1977920386276833
LOSS train 0.14486897675225796 valid 0.19778805623044732
LOSS train 0.14486897675225796 valid 0.19784173753102405
LOSS train 0.14486897675225796 valid 0.19771657886145555
LOSS train 0.14486897675225796 valid 0.19782803252056685
EPOCH 7:
  batch 1 loss: 0.16181902587413788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1362268626689911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13446751236915588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1339796483516693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1385130137205124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.1377989004055659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13634352598871505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13352003414183855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1349534731772211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13346509411931037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13463620096445084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13639683462679386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13664111208457214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13454260783536093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1342193772395452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13536835089325905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13673273517804987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1380636625819736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13910014770532908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1391795091331005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1401848587251845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13968818228353153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13772941153982413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13749453673760095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13743779361248015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1391327650501178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1395839292694021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14015010371804237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1411641481621512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14103002746899923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14071950508702186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13976305560208857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13974309757803427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1405627690255642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1410082214644977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.142113600547115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14295077182956645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14212120324373245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14113349410203788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14162862561643125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1415314612592139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1415748887118839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1415729917759119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14165471663529222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14144066141711342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1412061171039291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1410546363034147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1415797077740232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14169414736786667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14161835283041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14140325638593412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14086040367300695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.140921913227945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14114193038807976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1413213076916608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14161842076906137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14193317712398998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14151848916863574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14212870307392994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14280614691476026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14332680318687782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14287502414757206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1424810356563992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14246638189069927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14245561223763686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1423311714421619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14259948592577407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14278561161721454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1424064256142879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1421250935111727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1421761821273347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.1419042187432448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14219827970413312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14256759793371768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14253964285055795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1424969754328853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14267533727280504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14290210948540613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14303389399112026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14317147638648747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14319650994406807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1432945208578575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14315087411058955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14339666068553925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14352161007768968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14312950850919234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14325386491315117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14311831817030907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14327139047424445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1433462412820922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14341402430455763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14315435889622438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14318439909206923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1431547704529255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1429532481651557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.1429648691943536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1428981044704152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14275499684165935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14310251018314651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14311489082872866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14312317856762669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1431621651293016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14317934193368098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14337211653876764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14332368153901326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14329249967100485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14321820249902867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1430953612068185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1430953588644299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1432911417023702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14318668553689579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14295485196635127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1428290273895306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.142900535477358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14295751692160316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14317849477560357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14323573088289326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14310368485117364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.143252293665369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14315153540422518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1432910346664673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14334805767799988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1433345093353977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14327318023049063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14344788748025894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1434172135851686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14359111690849768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14353320462396368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14336348359667977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14322878609483058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.1431679477104704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1428748727070563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14288478047776043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14269191243532878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14263075628766306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14288367051631212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14299283992417539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1429380516967048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14305309956665518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14287392172430244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1429321716757531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.143040659194681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1430221330765244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14282163889664742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14301921874284745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1430782710954751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.142820200749806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14297055664497452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14318694634325552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14313921123743056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14326778508180024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14339616875115194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14351753160065295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14342423189770093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14333854875256938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1432952181651042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14324635362169544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14332252174993104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14335700410342067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14326824806630611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14320558994453145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14338659071995888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1434595162517454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1433620435617319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14316876033941905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14302760106791934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14304608146765987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14290038633736826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14279496427294772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14290351346135138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14296251513630326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14286487120701824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14275944982305427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1428213918517376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1428807818038123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14292357286269014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.143287548658538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1432897862925958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14333780842453409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14333873101406627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14333660456027775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14342404148735843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14352200102936374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14344472365210886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1434361779206508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14347893888911895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.143529126032151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1434020043766879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14346023396682486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14351272798682513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14347682483689322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14368243472805867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14363589840386198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14363905737541385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14362580588994883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14355086174090298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14331809106060697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14329487344983852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14316816452010792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14330466274172068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14325752027740526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14324973409276198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14331943519743792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14328189519252263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14318717667242375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1432870330596433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1431541870109701
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14300147225507176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14285781501868125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14297582230397632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14296633026328698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1431642141685171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1432616826793957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14324280480358087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14320234767226286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14320935329629314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.1431929719750233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14318866084475035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.1433022965307105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14310940984975207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14312314232010648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1431376729328353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14325227999366452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14323476342750446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14329814473787944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14321854914975377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14335206978121517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14323101561974017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14314827840270955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14320279110385023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1432277520562147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14324306179608765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14317023546271057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14312010045107612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14302175824946545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14307431428361747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14305179927671005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.143173531950021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14307814088326617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1430208693568905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.1429600793543693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14298295439028544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.1429555140159748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1429628840968257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14296405053868586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14281759323264526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14286602487568914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14297709142368648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1429811035952415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14303331419825555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14305152536269677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1430564417784649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14301339071612112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14296535335070504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14304827156604505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1430423870042432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14294149094634484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14287870056753935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14294843746773525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14294895039728053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1431337154516772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14322222834666268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14309144821797035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14315060647486738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14319249591175115
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1430753487393372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14310025638632112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14317337823892706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14307223642625774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14319475602220605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14313369483525462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14305572762318394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14297120696131563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14301761965790805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1428788734024221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1428318099267241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14294851337314082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.142868487981798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1427846641737073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14283555682216373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14276536437434234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14278945355867664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14274150841332997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14276352076148482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14291678382116452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1429100509446401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14301337916988113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14293236740761334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14280679383698633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14278318707285256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14280004200247146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14282516670757778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14296412788356938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14293252808504364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14298523290682646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1430853400882837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14301093715409238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14316564198308343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14316044114903861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14306815860172112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1430199751970776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14289006918964797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14292151628449412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14295849603551783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1429866608781893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14297492838665551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14292384812711503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14293577335774899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14301388484085262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1429931701912034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.1429473552433624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14289713610345736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14294727996420176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14298398543600063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14313319602182933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14313121231956571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14315699043240082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14315462006994015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1431714493699582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14313983528409152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14315200156047708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14328171120759864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14317546053659805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14312729172776512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14321489322644013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14321280309667617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.143224685996862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14321409949533095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.1432795775289956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14323136483629545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14318988265137658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14326879155474254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14330119666484026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14331641313647794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1432879505540008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14337141459275568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.143366630623001
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14330346430458965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14324113427713558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14330041553167736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1433367580612384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14332090553484464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14332346057752826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14331397513837316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14337379975595335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14333412628297862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1433769592462424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14337940873770877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14329280153471965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14333518200687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14343192963263926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1433871976226907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14337242529767094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14349368831279588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14344928921528266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14340859105329165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1433350373299516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1432999377060869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14329384802775794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14329817162619696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14329228816435277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14338748070581184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14337583604594564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1433624335370221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1433736174073938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14339612827437823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1433112538072004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1433966599566781
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14342430460097666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14351126342206388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14354794839964402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14354576611070222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1435058883943123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14357236940114892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14355997920036315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14356861778713287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14360583093343426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14362361968036683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14372405854095568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14379815953342537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14378331617263984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14378491516044628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1437880661406654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14380328283489993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14372680402034288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1437376418146136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14364515160251343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14370222996497892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1436911792252548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14371082805670224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14379424809494898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1437993341942831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14380407902120634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14378106673508126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14385158562962014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14381488954479044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14381018205914148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14379377651903497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14381091886743866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14380849197506904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14390965958039956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14398345484662411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14395918040802105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14393954012211008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14392681213808647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14392147275614622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14391100725172778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14384740749921868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14388063023536304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14387366250157357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14396427787495936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14396417558699556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14386376918949745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14378184225896132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14379207299775387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14384831088738373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14374238275867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14369803447828908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14373618693297688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14370046745808351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1437320433680915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14380390283550132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1437658639910937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14382529921197104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1438334554258515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14378773174445394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.143779633143607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1438402606818442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14381381311825106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14386420251671658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14385547952212202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14395583819391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1439617173810754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14400372827588687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14400166326213157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.1440747937170464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.144108798994216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14415306415936174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14415488527888318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1441803884946487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14419108688358276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1441814084431974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14422118507376105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1441978518725247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14414188314354823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14409596984161924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14406316409041683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14405519578472845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1440618051452467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14400903264681497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14401730610896107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14397797054421585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14394712250753744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14386476889777813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14379151109810714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14378587568276807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14378196927076878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14373343301261876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14365312970022232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14366550157251565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14367165867392773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1436324350826152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14362569378596401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.143714249936928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1436688642187785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14370016441900332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14375577836718242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14376828101519337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14376881049830775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1436707169293089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14371680136129356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14366551878530595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14366551878530595 valid 0.17882348597049713
LOSS train 0.14366551878530595 valid 0.20526977628469467
LOSS train 0.14366551878530595 valid 0.215591698884964
LOSS train 0.14366551878530595 valid 0.21143093332648277
LOSS train 0.14366551878530595 valid 0.21099942028522492
LOSS train 0.14366551878530595 valid 0.21645337094863257
LOSS train 0.14366551878530595 valid 0.2109303836311613
LOSS train 0.14366551878530595 valid 0.21054366789758205
LOSS train 0.14366551878530595 valid 0.21014710432953304
LOSS train 0.14366551878530595 valid 0.21121632307767868
LOSS train 0.14366551878530595 valid 0.2077246525070884
LOSS train 0.14366551878530595 valid 0.21036653965711594
LOSS train 0.14366551878530595 valid 0.20889403842962706
LOSS train 0.14366551878530595 valid 0.20899298574243272
LOSS train 0.14366551878530595 valid 0.20774472157160442
LOSS train 0.14366551878530595 valid 0.20818351954221725
LOSS train 0.14366551878530595 valid 0.20836285370237687
LOSS train 0.14366551878530595 valid 0.2088786040743192
LOSS train 0.14366551878530595 valid 0.21081328313601644
LOSS train 0.14366551878530595 valid 0.2101991057395935
LOSS train 0.14366551878530595 valid 0.21073261329105922
LOSS train 0.14366551878530595 valid 0.20907528359781613
LOSS train 0.14366551878530595 valid 0.2072473783855853
LOSS train 0.14366551878530595 valid 0.2081734538078308
LOSS train 0.14366551878530595 valid 0.2066447216272354
LOSS train 0.14366551878530595 valid 0.20622129623706525
LOSS train 0.14366551878530595 valid 0.2058427267604404
LOSS train 0.14366551878530595 valid 0.20594146155885287
LOSS train 0.14366551878530595 valid 0.2045402516578806
LOSS train 0.14366551878530595 valid 0.20365607341130573
LOSS train 0.14366551878530595 valid 0.20271570067251882
LOSS train 0.14366551878530595 valid 0.20350026991218328
LOSS train 0.14366551878530595 valid 0.202223565993887
LOSS train 0.14366551878530595 valid 0.2017638004001449
LOSS train 0.14366551878530595 valid 0.20226180638585772
LOSS train 0.14366551878530595 valid 0.20231995234886804
LOSS train 0.14366551878530595 valid 0.20131991562005636
LOSS train 0.14366551878530595 valid 0.2013282226888757
LOSS train 0.14366551878530595 valid 0.20099945977712289
LOSS train 0.14366551878530595 valid 0.2006910290569067
LOSS train 0.14366551878530595 valid 0.20050130275691427
LOSS train 0.14366551878530595 valid 0.20167731387274607
LOSS train 0.14366551878530595 valid 0.2025225283794625
LOSS train 0.14366551878530595 valid 0.2016392218118364
LOSS train 0.14366551878530595 valid 0.20107236272758908
LOSS train 0.14366551878530595 valid 0.19951921215523843
LOSS train 0.14366551878530595 valid 0.19876431879845072
LOSS train 0.14366551878530595 valid 0.20024341562141976
LOSS train 0.14366551878530595 valid 0.20012286518301284
LOSS train 0.14366551878530595 valid 0.20042818069458007
LOSS train 0.14366551878530595 valid 0.20018981367933983
LOSS train 0.14366551878530595 valid 0.19999419754514328
LOSS train 0.14366551878530595 valid 0.20100732766232401
LOSS train 0.14366551878530595 valid 0.20088244367528846
LOSS train 0.14366551878530595 valid 0.2010910548947074
LOSS train 0.14366551878530595 valid 0.2007197594004018
LOSS train 0.14366551878530595 valid 0.20003738032098403
LOSS train 0.14366551878530595 valid 0.2007821806032082
LOSS train 0.14366551878530595 valid 0.20091943493333914
LOSS train 0.14366551878530595 valid 0.2006087397535642
LOSS train 0.14366551878530595 valid 0.20046005385820984
LOSS train 0.14366551878530595 valid 0.20010367300241225
LOSS train 0.14366551878530595 valid 0.2001820086013703
LOSS train 0.14366551878530595 valid 0.20009840955026448
LOSS train 0.14366551878530595 valid 0.19929853150477775
LOSS train 0.14366551878530595 valid 0.19916684970711218
LOSS train 0.14366551878530595 valid 0.19977993395791124
LOSS train 0.14366551878530595 valid 0.19910519329064033
LOSS train 0.14366551878530595 valid 0.19946243516776874
LOSS train 0.14366551878530595 valid 0.19992503970861436
LOSS train 0.14366551878530595 valid 0.20002296315112583
LOSS train 0.14366551878530595 valid 0.2005013736585776
LOSS train 0.14366551878530595 valid 0.2007962604091592
LOSS train 0.14366551878530595 valid 0.20077818511305628
LOSS train 0.14366551878530595 valid 0.20054221510887146
LOSS train 0.14366551878530595 valid 0.20066502435426964
LOSS train 0.14366551878530595 valid 0.2007348382627809
LOSS train 0.14366551878530595 valid 0.20063746300263283
LOSS train 0.14366551878530595 valid 0.20092039941986906
LOSS train 0.14366551878530595 valid 0.2006074871867895
LOSS train 0.14366551878530595 valid 0.20074911967471795
LOSS train 0.14366551878530595 valid 0.20098355158073147
LOSS train 0.14366551878530595 valid 0.20067955212420727
LOSS train 0.14366551878530595 valid 0.20056222716257685
LOSS train 0.14366551878530595 valid 0.20081401292015524
LOSS train 0.14366551878530595 valid 0.20094147879023885
LOSS train 0.14366551878530595 valid 0.20054123730495058
LOSS train 0.14366551878530595 valid 0.20042591999200257
LOSS train 0.14366551878530595 valid 0.20056080952119293
LOSS train 0.14366551878530595 valid 0.20036577300892935
LOSS train 0.14366551878530595 valid 0.2004204164822023
LOSS train 0.14366551878530595 valid 0.20048107449775157
LOSS train 0.14366551878530595 valid 0.2007330442949008
LOSS train 0.14366551878530595 valid 0.2010860357512819
LOSS train 0.14366551878530595 valid 0.20152707554792104
LOSS train 0.14366551878530595 valid 0.20237876862908402
LOSS train 0.14366551878530595 valid 0.20253843975435828
LOSS train 0.14366551878530595 valid 0.2030214913645569
LOSS train 0.14366551878530595 valid 0.20287642605376965
LOSS train 0.14366551878530595 valid 0.20316316679120064
LOSS train 0.14366551878530595 valid 0.20309424474097715
LOSS train 0.14366551878530595 valid 0.20352974899259269
LOSS train 0.14366551878530595 valid 0.20309595691347584
LOSS train 0.14366551878530595 valid 0.20294128000163114
LOSS train 0.14366551878530595 valid 0.20321988009271166
LOSS train 0.14366551878530595 valid 0.20324079959460026
LOSS train 0.14366551878530595 valid 0.20304646099282203
LOSS train 0.14366551878530595 valid 0.20310762027899423
LOSS train 0.14366551878530595 valid 0.20257240697878218
LOSS train 0.14366551878530595 valid 0.2023327506401322
LOSS train 0.14366551878530595 valid 0.2023838143627923
LOSS train 0.14366551878530595 valid 0.20248654351702758
LOSS train 0.14366551878530595 valid 0.20225172219550716
LOSS train 0.14366551878530595 valid 0.20248235121630787
LOSS train 0.14366551878530595 valid 0.20279481385065162
LOSS train 0.14366551878530595 valid 0.20277642227452378
LOSS train 0.14366551878530595 valid 0.20284251193714958
LOSS train 0.14366551878530595 valid 0.2028491044953718
LOSS train 0.14366551878530595 valid 0.20259071673665727
LOSS train 0.14366551878530595 valid 0.202558051298062
LOSS train 0.14366551878530595 valid 0.20262130319086974
LOSS train 0.14366551878530595 valid 0.20265376604482777
LOSS train 0.14366551878530595 valid 0.20257221780172208
LOSS train 0.14366551878530595 valid 0.2026580949464152
LOSS train 0.14366551878530595 valid 0.20279153203964234
LOSS train 0.14366551878530595 valid 0.20265000230736202
LOSS train 0.14366551878530595 valid 0.20249277775681865
LOSS train 0.14366551878530595 valid 0.20237958943471313
LOSS train 0.14366551878530595 valid 0.20229799537233603
LOSS train 0.14366551878530595 valid 0.202021728685269
LOSS train 0.14366551878530595 valid 0.20203723502523116
LOSS train 0.14366551878530595 valid 0.20218952700044168
LOSS train 0.14366551878530595 valid 0.20217943662091306
LOSS train 0.14366551878530595 valid 0.20221466737896648
LOSS train 0.14366551878530595 valid 0.20226637147091053
LOSS train 0.14366551878530595 valid 0.2024524314876865
LOSS train 0.14366551878530595 valid 0.20226897473317862
LOSS train 0.14366551878530595 valid 0.2019110772272815
LOSS train 0.14366551878530595 valid 0.20187994601915207
LOSS train 0.14366551878530595 valid 0.20173408122999328
LOSS train 0.14366551878530595 valid 0.20201036217787588
LOSS train 0.14366551878530595 valid 0.20225435065131792
LOSS train 0.14366551878530595 valid 0.20237955013355174
LOSS train 0.14366551878530595 valid 0.20231404755678442
LOSS train 0.14366551878530595 valid 0.2021020400113073
LOSS train 0.14366551878530595 valid 0.20215569334487393
LOSS train 0.14366551878530595 valid 0.20214158326995615
LOSS train 0.14366551878530595 valid 0.20189499261008725
LOSS train 0.14366551878530595 valid 0.2021160553765777
LOSS train 0.14366551878530595 valid 0.2020500910282135
LOSS train 0.14366551878530595 valid 0.20194223463930042
LOSS train 0.14366551878530595 valid 0.20202790965375148
LOSS train 0.14366551878530595 valid 0.2017825882808835
LOSS train 0.14366551878530595 valid 0.20181826115041585
LOSS train 0.14366551878530595 valid 0.20163129193167534
LOSS train 0.14366551878530595 valid 0.20183973062114838
LOSS train 0.14366551878530595 valid 0.20165443752601647
LOSS train 0.14366551878530595 valid 0.2015683771688727
LOSS train 0.14366551878530595 valid 0.20186010912154456
LOSS train 0.14366551878530595 valid 0.20183903500437736
LOSS train 0.14366551878530595 valid 0.20201132312324477
LOSS train 0.14366551878530595 valid 0.20202782510975262
LOSS train 0.14366551878530595 valid 0.20176842296781716
LOSS train 0.14366551878530595 valid 0.2017980018040029
LOSS train 0.14366551878530595 valid 0.20195163813504305
LOSS train 0.14366551878530595 valid 0.20183979486485562
LOSS train 0.14366551878530595 valid 0.20192110806168195
LOSS train 0.14366551878530595 valid 0.20157147465007647
LOSS train 0.14366551878530595 valid 0.20144626489405096
LOSS train 0.14366551878530595 valid 0.20129306482918122
LOSS train 0.14366551878530595 valid 0.2011188578884504
LOSS train 0.14366551878530595 valid 0.20118222402971844
LOSS train 0.14366551878530595 valid 0.201246476242308
LOSS train 0.14366551878530595 valid 0.20125904833448344
LOSS train 0.14366551878530595 valid 0.20099227488040924
LOSS train 0.14366551878530595 valid 0.20099012020298027
LOSS train 0.14366551878530595 valid 0.200870806551249
LOSS train 0.14366551878530595 valid 0.20088056575381355
LOSS train 0.14366551878530595 valid 0.200877732821017
LOSS train 0.14366551878530595 valid 0.20088320448994637
LOSS train 0.14366551878530595 valid 0.20075956192793767
LOSS train 0.14366551878530595 valid 0.200651398563123
LOSS train 0.14366551878530595 valid 0.2007647786146956
LOSS train 0.14366551878530595 valid 0.2008147255881973
LOSS train 0.14366551878530595 valid 0.20087909601830148
LOSS train 0.14366551878530595 valid 0.2010657555794203
LOSS train 0.14366551878530595 valid 0.20100623656084193
LOSS train 0.14366551878530595 valid 0.20096993707913033
LOSS train 0.14366551878530595 valid 0.20062922911038475
LOSS train 0.14366551878530595 valid 0.20060407864420038
LOSS train 0.14366551878530595 valid 0.20070800715716097
LOSS train 0.14366551878530595 valid 0.20074628472017744
LOSS train 0.14366551878530595 valid 0.2005053581042611
LOSS train 0.14366551878530595 valid 0.20057087583640187
LOSS train 0.14366551878530595 valid 0.200471808665838
LOSS train 0.14366551878530595 valid 0.2007062923239202
LOSS train 0.14366551878530595 valid 0.2008414601311466
LOSS train 0.14366551878530595 valid 0.20093546100337095
LOSS train 0.14366551878530595 valid 0.20104043709872357
LOSS train 0.14366551878530595 valid 0.20125846952199936
LOSS train 0.14366551878530595 valid 0.20128212846926788
LOSS train 0.14366551878530595 valid 0.20148500077205128
LOSS train 0.14366551878530595 valid 0.20160088324781708
LOSS train 0.14366551878530595 valid 0.2013978391301398
LOSS train 0.14366551878530595 valid 0.20122956277393714
LOSS train 0.14366551878530595 valid 0.2011801146911186
LOSS train 0.14366551878530595 valid 0.2010201305007013
LOSS train 0.14366551878530595 valid 0.20079722814261913
LOSS train 0.14366551878530595 valid 0.20082293800189735
LOSS train 0.14366551878530595 valid 0.20105610135055724
LOSS train 0.14366551878530595 valid 0.20130291383413343
LOSS train 0.14366551878530595 valid 0.20116576018198482
LOSS train 0.14366551878530595 valid 0.20113403899289073
LOSS train 0.14366551878530595 valid 0.20109324498432818
LOSS train 0.14366551878530595 valid 0.20082630424998527
LOSS train 0.14366551878530595 valid 0.20060357892954792
LOSS train 0.14366551878530595 valid 0.20064628687322414
LOSS train 0.14366551878530595 valid 0.20058456890353368
LOSS train 0.14366551878530595 valid 0.2006653930362501
LOSS train 0.14366551878530595 valid 0.20060746297240256
LOSS train 0.14366551878530595 valid 0.20059590769838964
LOSS train 0.14366551878530595 valid 0.20071260523688686
LOSS train 0.14366551878530595 valid 0.20058754981900545
LOSS train 0.14366551878530595 valid 0.20046231390110084
LOSS train 0.14366551878530595 valid 0.2001626323329078
LOSS train 0.14366551878530595 valid 0.19998342840544944
LOSS train 0.14366551878530595 valid 0.19997979378910316
LOSS train 0.14366551878530595 valid 0.20003460870500198
LOSS train 0.14366551878530595 valid 0.1999953673526189
LOSS train 0.14366551878530595 valid 0.20015563906534858
LOSS train 0.14366551878530595 valid 0.20020523273841642
LOSS train 0.14366551878530595 valid 0.20037165431883827
LOSS train 0.14366551878530595 valid 0.20049888880467723
LOSS train 0.14366551878530595 valid 0.20046938255301908
LOSS train 0.14366551878530595 valid 0.2004136655558931
LOSS train 0.14366551878530595 valid 0.20023322124349868
LOSS train 0.14366551878530595 valid 0.20027238124282049
LOSS train 0.14366551878530595 valid 0.2003200695294292
LOSS train 0.14366551878530595 valid 0.20031355764077796
LOSS train 0.14366551878530595 valid 0.19991887680565318
LOSS train 0.14366551878530595 valid 0.19984088000296557
LOSS train 0.14366551878530595 valid 0.19990300427167867
LOSS train 0.14366551878530595 valid 0.1999413108322846
LOSS train 0.14366551878530595 valid 0.199999585228621
LOSS train 0.14366551878530595 valid 0.20020056959925867
LOSS train 0.14366551878530595 valid 0.2003118125343226
LOSS train 0.14366551878530595 valid 0.20033835022435015
LOSS train 0.14366551878530595 valid 0.20026275368347282
LOSS train 0.14366551878530595 valid 0.20015354050570225
LOSS train 0.14366551878530595 valid 0.20027519926428794
LOSS train 0.14366551878530595 valid 0.200341897448933
LOSS train 0.14366551878530595 valid 0.20038194005333243
LOSS train 0.14366551878530595 valid 0.20028877396710776
LOSS train 0.14366551878530595 valid 0.20034678394752226
LOSS train 0.14366551878530595 valid 0.20034526396615832
LOSS train 0.14366551878530595 valid 0.20037285357830115
LOSS train 0.14366551878530595 valid 0.20028287923173682
LOSS train 0.14366551878530595 valid 0.20036003504728161
LOSS train 0.14366551878530595 valid 0.20030140787593187
LOSS train 0.14366551878530595 valid 0.2001812033641797
LOSS train 0.14366551878530595 valid 0.20015475111103606
LOSS train 0.14366551878530595 valid 0.20041066012878456
LOSS train 0.14366551878530595 valid 0.2002807258143171
LOSS train 0.14366551878530595 valid 0.20030492744549658
LOSS train 0.14366551878530595 valid 0.20040299833383202
LOSS train 0.14366551878530595 valid 0.20049167460852996
LOSS train 0.14366551878530595 valid 0.20047888664828703
LOSS train 0.14366551878530595 valid 0.20050848311563926
LOSS train 0.14366551878530595 valid 0.20055666031336697
LOSS train 0.14366551878530595 valid 0.2004375412784241
LOSS train 0.14366551878530595 valid 0.2003532878202266
LOSS train 0.14366551878530595 valid 0.20017729855745153
LOSS train 0.14366551878530595 valid 0.20004626794721617
LOSS train 0.14366551878530595 valid 0.1999969607776534
LOSS train 0.14366551878530595 valid 0.2000984760035168
LOSS train 0.14366551878530595 valid 0.20024432822306087
LOSS train 0.14366551878530595 valid 0.2001690441466841
LOSS train 0.14366551878530595 valid 0.20015714668648707
LOSS train 0.14366551878530595 valid 0.20023023390534958
LOSS train 0.14366551878530595 valid 0.2000494357464569
LOSS train 0.14366551878530595 valid 0.19991897579507895
LOSS train 0.14366551878530595 valid 0.1999040019057744
LOSS train 0.14366551878530595 valid 0.1999222889226654
LOSS train 0.14366551878530595 valid 0.1997431484903668
LOSS train 0.14366551878530595 valid 0.19984999965680272
LOSS train 0.14366551878530595 valid 0.19984859908064762
LOSS train 0.14366551878530595 valid 0.1999816679840304
LOSS train 0.14366551878530595 valid 0.19993465515370998
LOSS train 0.14366551878530595 valid 0.19991415375553612
LOSS train 0.14366551878530595 valid 0.19980942938348342
LOSS train 0.14366551878530595 valid 0.19977612954746818
LOSS train 0.14366551878530595 valid 0.1997544562112387
LOSS train 0.14366551878530595 valid 0.19969271553152657
LOSS train 0.14366551878530595 valid 0.19967880609388255
LOSS train 0.14366551878530595 valid 0.19966373769408566
LOSS train 0.14366551878530595 valid 0.19970817493929252
LOSS train 0.14366551878530595 valid 0.19971333255972526
LOSS train 0.14366551878530595 valid 0.19974822573513792
LOSS train 0.14366551878530595 valid 0.19974739941765232
LOSS train 0.14366551878530595 valid 0.19978371160725752
LOSS train 0.14366551878530595 valid 0.19975328165728387
LOSS train 0.14366551878530595 valid 0.19974192204755664
LOSS train 0.14366551878530595 valid 0.19978065889562316
LOSS train 0.14366551878530595 valid 0.19974159687071255
LOSS train 0.14366551878530595 valid 0.1996291621053805
LOSS train 0.14366551878530595 valid 0.19975861690305416
LOSS train 0.14366551878530595 valid 0.1996575225787753
LOSS train 0.14366551878530595 valid 0.19975039563686042
LOSS train 0.14366551878530595 valid 0.19977671865217123
LOSS train 0.14366551878530595 valid 0.19981721675684375
LOSS train 0.14366551878530595 valid 0.19986159172376253
LOSS train 0.14366551878530595 valid 0.19983429306497177
LOSS train 0.14366551878530595 valid 0.1998447036257567
LOSS train 0.14366551878530595 valid 0.19984511221954776
LOSS train 0.14366551878530595 valid 0.19986372786381887
LOSS train 0.14366551878530595 valid 0.19986054973228823
LOSS train 0.14366551878530595 valid 0.19983415181046404
LOSS train 0.14366551878530595 valid 0.19990361390529937
LOSS train 0.14366551878530595 valid 0.1999083594141709
LOSS train 0.14366551878530595 valid 0.19979046003427356
LOSS train 0.14366551878530595 valid 0.19980074653280117
LOSS train 0.14366551878530595 valid 0.19973042665153556
LOSS train 0.14366551878530595 valid 0.19968458218932522
LOSS train 0.14366551878530595 valid 0.19962598620281544
LOSS train 0.14366551878530595 valid 0.19945464893029285
LOSS train 0.14366551878530595 valid 0.19953647130197544
LOSS train 0.14366551878530595 valid 0.19941822380192054
LOSS train 0.14366551878530595 valid 0.19948290954003248
LOSS train 0.14366551878530595 valid 0.19953931032433697
LOSS train 0.14366551878530595 valid 0.19961936629631302
LOSS train 0.14366551878530595 valid 0.19953861191103467
LOSS train 0.14366551878530595 valid 0.19951822058892393
LOSS train 0.14366551878530595 valid 0.1995288363403386
LOSS train 0.14366551878530595 valid 0.199475812720146
LOSS train 0.14366551878530595 valid 0.1993548399699268
LOSS train 0.14366551878530595 valid 0.1992743691385147
LOSS train 0.14366551878530595 valid 0.1994244775759714
LOSS train 0.14366551878530595 valid 0.19936479550935107
LOSS train 0.14366551878530595 valid 0.1992650636221807
LOSS train 0.14366551878530595 valid 0.1993133936515626
LOSS train 0.14366551878530595 valid 0.1993585917388589
LOSS train 0.14366551878530595 valid 0.19938207759756094
LOSS train 0.14366551878530595 valid 0.19925035455918522
LOSS train 0.14366551878530595 valid 0.19935564569002667
LOSS train 0.14366551878530595 valid 0.19942750181408897
LOSS train 0.14366551878530595 valid 0.19934437611285663
LOSS train 0.14366551878530595 valid 0.1992873301037107
LOSS train 0.14366551878530595 valid 0.19926346857743016
LOSS train 0.14366551878530595 valid 0.19924419121363102
LOSS train 0.14366551878530595 valid 0.1992907419162137
LOSS train 0.14366551878530595 valid 0.19933411578761887
LOSS train 0.14366551878530595 valid 0.19937388198873537
LOSS train 0.14366551878530595 valid 0.19944865961503713
LOSS train 0.14366551878530595 valid 0.1993305681241771
LOSS train 0.14366551878530595 valid 0.19921993758057205
LOSS train 0.14366551878530595 valid 0.1992123895691017
LOSS train 0.14366551878530595 valid 0.199245968223119
LOSS train 0.14366551878530595 valid 0.19934521347034576
LOSS train 0.14366551878530595 valid 0.19934827518911416
LOSS train 0.14366551878530595 valid 0.19961564224213363
LOSS train 0.14366551878530595 valid 0.19958435711025202
LOSS train 0.14366551878530595 valid 0.1997145493180383
LOSS train 0.14366551878530595 valid 0.19973478256917196
LOSS train 0.14366551878530595 valid 0.1996393877767272
LOSS train 0.14366551878530595 valid 0.19966093838623125
LOSS train 0.14366551878530595 valid 0.19966375320905544
LOSS train 0.14366551878530595 valid 0.19972008152050283
LOSS train 0.14366551878530595 valid 0.19960235296141193
LOSS train 0.14366551878530595 valid 0.19971954364198333
EPOCH 8:
  batch 1 loss: 0.1568501591682434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.13258098810911179
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13179166118303934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13101238757371902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13674751818180084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13614147156476974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13508284943444387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13253768254071474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13364091598325306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13182380124926568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13335805386304855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1358103808015585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13596817679130113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13367579079100064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13324621468782424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13465217361226678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13595296573989532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1371945949892203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1382380798459053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13823613189160824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13922361568326042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13858141309835695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13663495655940927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13644133787602186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13660985320806504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13848673237057832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13906842718521753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13952259745981013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1405576716723113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14027095660567285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1397688895944626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1388752469792962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1389092897826975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13997835753595128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.14055414795875548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1418086683584584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1426801874830916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14175686436264137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14077705259506518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1412781961262226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1410590624663888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.14099181904679253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14105235975842143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14104940606789154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1407747831609514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1406068869906923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14046989000858146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14111242443323135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14125586255472533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1410969665646553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14091473408773833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14039397741166446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14031652164346767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.140526264216061
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14077845933762465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14109864445137127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14143111399914088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1410174171986251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14157011625120194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14243051211039226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14292894987786403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14245315220567487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1419695085949368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.141854029847309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14185527150447552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14167333416866534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1418987145619606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14210107440457626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14180618146623392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.14149326841746057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14156068410252182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14118790595481792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14138167412721947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14178306418093475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1416785388191541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14168287215656356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14187495381414117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14207600582486543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14222976386169844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1423740248195827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14233722353791012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14246704129547608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1422784591475165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14257263401079745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.142658850988921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1422413819751074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1424289426241798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14228924821046265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14243325959430653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14244813952181073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14248195650813344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1422340090022139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14232982110272172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14236853295501242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1421364236034845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14210048667155206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14199277843089447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14189683197408307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14221898029849986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14224073447287083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1422702266585709
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14225652382946483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14225019753268622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1424716481079276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14247257148935683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14249707821686314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14246387814528474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14238531425319337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14242031028784743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14261855300177229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14251455456555426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14228413120976516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14221519324104342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14229180990603932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1423510765251906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1425175461275824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14263110525078243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14253567133919667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.1426470874487853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1425490869830052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14271815256638962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1427359624964292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14270174733506955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14263958339729615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14279274094104766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14279134417810138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.1429325118074267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14292553870473057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14277245254479637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14270304819712273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14264742848072343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1423112510167288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14230417673076903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14209190433594718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14203833440939587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14225747162366614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1423387991903472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.1422782599710036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14244096431371978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14225959868303367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1423435907097573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1424732620132641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14246020966268086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14221796460656655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14239794288215965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.1424299484144335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14217276207241072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14229917118476854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14250597312746432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14245838875571887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.1425813166609663
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14266484029787152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14280021994137296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14270557390598507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.1425994633186248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14254800750850102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1425149699400185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1425597518210924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.1426079086256477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1425260639283806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.142507249662965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1426582480378357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14276195710604908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14269042055963016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14246570693724084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14230773526142879
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14231137503050045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14216880185440892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14206795090225321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14216039737357813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14218937727617242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1421049616277911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14199295652911842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.142046215505093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1420938134619168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14212101498957386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14250133910589974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1425268565084827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14257136440976373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14259015342427625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1426205488940629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14269588359600896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1427600350780565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14265958738067877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14264133129570936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14268396746727727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14276962555984762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14265901167342004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14272863224700647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14281092442964252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14277263851690042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14301376060272256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14300853662540258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14301452593705088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14300720607623077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14292937646410903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1426952632928863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14267703962295947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14253849558644557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14265862111002206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14261333989118463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14260317056928531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14267762500811093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14264593094441236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14253680695120882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.1426405865084199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14249859267962728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14235680772421452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.1421976232072383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14235426286856334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14235368653496294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.1425619033991166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14264250305336965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14264124381207974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1425968333039173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1425821821860693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14259716511322057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14255549533104678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14264751528496067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14244029501622374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14245711765947386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.142455920912661
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1425588647479968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14253295958042145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14262103418509164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1425560495231004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14268293563227297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14257414067131385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1424852685116264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14254870382340057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14258836829043053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1426162589341402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14253584803941424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14249661825915688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14239884567387562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1424443946867171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14240972864351192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14252269327515313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1424168090503585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14236542951936523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14232402950277961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14233874883656658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14234186791711384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14232076634271223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14231845131333992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14215465217101864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.1421992526604579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.1423266328150226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14230902403234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14237618273496627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14240868110580748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14240368502953696
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14234688642467905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14230453088058262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14235926878218558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1423543497803621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14226541085465874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14222053652123887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1422926570112641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14230754530200593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14247605781217187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14255800240367422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1424181299007891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14245337363558286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14249011623971866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1423792688358099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1424002947097414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14246785095823344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1423658084359754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14246540163402205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14240427535178476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14232706273084178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14225078592479448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14228556813658588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1421505523540757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14209336806358636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14220427802431024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14212568522357255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14204714684930753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1421085167144026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.1420298225090597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14205745708012413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14199891665377803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14204522678759737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14220287052162905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1421965979091771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.142268944388898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1421765893852959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14204736371044469
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14203543896818982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14202219096758112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14204365788155224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14218668377745275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14215159525169807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14221508071079092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14230607306534374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14224942633098223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14240243018373547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14241148688422398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14232106817265353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14227644170934575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14214609926011387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14218588628292872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1422326189073685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14226343736785357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14222774355142725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.1421799160030455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14218314705634272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14227964897854042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14226796749618745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14221398496455318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1421560908500583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14222949893234638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14226045034778345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14240829559072615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14240985964955408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14244655397506167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14244446751175435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.1424552905475458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14239391018636524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1423907950752621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14252454866163478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14242382645145657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14237910120483535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14246190449366203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1424531108168371
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14246530035675847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.1424659784292666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14252611091162296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1424739880769542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.1424460108321599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14253116652638798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1425666804383467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1425716834377029
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14253257408515732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14261992238018484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1426109028455058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14255349913380555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1424936137016544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14253152512452183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14255829019979996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14253572405081744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14255181432812972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1425322939230259
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1425958672295446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14253571500323411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1426014461331821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.142615276463758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14251978095503454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14255925495709693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14264528336603077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.1426081133964048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14260845546935166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14272526308557407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14267597888976755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14262263884956247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14254575028639882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14250104116827417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.1424911724906778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14247589144441816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.142460568583573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.142542048259664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14253940340901208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1425295372258176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14255824460558694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14259055692478606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1425114062485318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1426033763538884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14261438735939946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14270474608685518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14274986530089315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14273446826364405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14268520626241976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1427406021219524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14273358384768167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14272344175805438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14276989013826183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14278934012960504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14291257806377863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14297799162174527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14297795503001826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14298555750341316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1429949648697756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14302302699070424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1429483306485337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1429858489512162
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1428983119107032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14294523204263954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1429344481168492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14295985786578594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14305310961230636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14306310229763694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14306027083906508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.143023271094724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14308958049816423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14304961772127586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1430449650554873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14303027239426896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14306004793572247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14307085514068604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14315381530960303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14321479093820894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14318862132191953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14319016057813522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14318148854338092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14319063693725417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14317120966805874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14310752702694313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14314295347861963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1431500148118996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14323113206094198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14322201003438062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14313860268266668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14305033020063299
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14304052339978965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14308735996914598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.1429899546453993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14295968534558584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14295144439597118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14290775520106155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14292582425326464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.1429644914705889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14292211656240708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1429856773641593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14299462649752112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14294307133261586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14292751266610149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14297328140830326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14294826571063285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14298855596503546
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14298479321246357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14309916621143068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14311119783282555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14316936317951448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1431760964037358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14325544501253223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14327739772452916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14333224323786556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14331639671787313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14333975863727658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14334789080684687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1433448317498643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14338530892712267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14334891414320147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1432914018965839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14325534919853167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14322450286990043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14322533724563463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1432277526714754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14317962692843544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14319761335321118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1431540196712038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14313184935393974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14304126544736556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14295964751924786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14295601893804574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.142943809147215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14290422884397633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14283223209947282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14285018994756368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14286230329965563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14282293569344978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1428325056192942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14292040011235352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1428823128182401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14290699631359444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1429521208210216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14294943961704898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14294471294640987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1428536801420628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14289496736225302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1428451250323047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1428451250323047 valid 0.18190616369247437
LOSS train 0.1428451250323047 valid 0.20720632374286652
LOSS train 0.1428451250323047 valid 0.21761828660964966
LOSS train 0.1428451250323047 valid 0.21404685825109482
LOSS train 0.1428451250323047 valid 0.21366944015026093
LOSS train 0.1428451250323047 valid 0.2194473296403885
LOSS train 0.1428451250323047 valid 0.21406615206173488
LOSS train 0.1428451250323047 valid 0.21360399946570396
LOSS train 0.1428451250323047 valid 0.21369672815004984
LOSS train 0.1428451250323047 valid 0.2145573765039444
LOSS train 0.1428451250323047 valid 0.21116419407454404
LOSS train 0.1428451250323047 valid 0.21395494788885117
LOSS train 0.1428451250323047 valid 0.2125664078272306
LOSS train 0.1428451250323047 valid 0.21221966189997538
LOSS train 0.1428451250323047 valid 0.2109793394804001
LOSS train 0.1428451250323047 valid 0.21148293744772673
LOSS train 0.1428451250323047 valid 0.21179583493401022
LOSS train 0.1428451250323047 valid 0.2122424625688129
LOSS train 0.1428451250323047 valid 0.21411729956928052
LOSS train 0.1428451250323047 valid 0.21353354752063752
LOSS train 0.1428451250323047 valid 0.21407991647720337
LOSS train 0.1428451250323047 valid 0.21246947754513135
LOSS train 0.1428451250323047 valid 0.21060662165932034
LOSS train 0.1428451250323047 valid 0.21151362794140974
LOSS train 0.1428451250323047 valid 0.21009515583515168
LOSS train 0.1428451250323047 valid 0.20964968032561815
LOSS train 0.1428451250323047 valid 0.209350257008164
LOSS train 0.1428451250323047 valid 0.2095124748136316
LOSS train 0.1428451250323047 valid 0.20802352705906177
LOSS train 0.1428451250323047 valid 0.20710565149784088
LOSS train 0.1428451250323047 valid 0.20622598259679734
LOSS train 0.1428451250323047 valid 0.2070529069751501
LOSS train 0.1428451250323047 valid 0.20583381029692563
LOSS train 0.1428451250323047 valid 0.20533182822606144
LOSS train 0.1428451250323047 valid 0.20579189274992263
LOSS train 0.1428451250323047 valid 0.2059016331202454
LOSS train 0.1428451250323047 valid 0.20487743414737083
LOSS train 0.1428451250323047 valid 0.20484307682827899
LOSS train 0.1428451250323047 valid 0.204586155521564
LOSS train 0.1428451250323047 valid 0.2042534802109003
LOSS train 0.1428451250323047 valid 0.20407634428361568
LOSS train 0.1428451250323047 valid 0.20524283143736066
LOSS train 0.1428451250323047 valid 0.20599728961323582
LOSS train 0.1428451250323047 valid 0.20508567345413295
LOSS train 0.1428451250323047 valid 0.20449977318445842
LOSS train 0.1428451250323047 valid 0.20298730938330942
LOSS train 0.1428451250323047 valid 0.20222970716496733
LOSS train 0.1428451250323047 valid 0.2037057907631
LOSS train 0.1428451250323047 valid 0.2035164966875193
LOSS train 0.1428451250323047 valid 0.20383699625730514
LOSS train 0.1428451250323047 valid 0.20364298130951675
LOSS train 0.1428451250323047 valid 0.2034225228887338
LOSS train 0.1428451250323047 valid 0.20442322177707023
LOSS train 0.1428451250323047 valid 0.20425115139396102
LOSS train 0.1428451250323047 valid 0.204449094154618
LOSS train 0.1428451250323047 valid 0.20407216171068804
LOSS train 0.1428451250323047 valid 0.2033908589367281
LOSS train 0.1428451250323047 valid 0.2041452624674501
LOSS train 0.1428451250323047 valid 0.2042796808784291
LOSS train 0.1428451250323047 valid 0.2039642445743084
LOSS train 0.1428451250323047 valid 0.20378009144399986
LOSS train 0.1428451250323047 valid 0.20344473253334722
LOSS train 0.1428451250323047 valid 0.2035314982372617
LOSS train 0.1428451250323047 valid 0.2034215102903545
LOSS train 0.1428451250323047 valid 0.20263127730442929
LOSS train 0.1428451250323047 valid 0.20252941684289413
LOSS train 0.1428451250323047 valid 0.20314343651728844
LOSS train 0.1428451250323047 valid 0.2024890813319122
LOSS train 0.1428451250323047 valid 0.20284910154515418
LOSS train 0.1428451250323047 valid 0.2033774203487805
LOSS train 0.1428451250323047 valid 0.20349036807745274
LOSS train 0.1428451250323047 valid 0.2039466711382071
LOSS train 0.1428451250323047 valid 0.2042174222942901
LOSS train 0.1428451250323047 valid 0.20420272829564842
LOSS train 0.1428451250323047 valid 0.20395453333854674
LOSS train 0.1428451250323047 valid 0.20407692362603388
LOSS train 0.1428451250323047 valid 0.2041269995562442
LOSS train 0.1428451250323047 valid 0.20397205230517265
LOSS train 0.1428451250323047 valid 0.2042546834372267
LOSS train 0.1428451250323047 valid 0.20392790343612432
LOSS train 0.1428451250323047 valid 0.20408165399675016
LOSS train 0.1428451250323047 valid 0.20432095447691476
LOSS train 0.1428451250323047 valid 0.2040120855153325
LOSS train 0.1428451250323047 valid 0.2038908904152257
LOSS train 0.1428451250323047 valid 0.2041033362641054
LOSS train 0.1428451250323047 valid 0.20420657479485801
LOSS train 0.1428451250323047 valid 0.20379147276111031
LOSS train 0.1428451250323047 valid 0.2036693375557661
LOSS train 0.1428451250323047 valid 0.20381403890218627
LOSS train 0.1428451250323047 valid 0.20362847199042639
LOSS train 0.1428451250323047 valid 0.20368841814470814
LOSS train 0.1428451250323047 valid 0.20373833859744278
LOSS train 0.1428451250323047 valid 0.2039548235554849
LOSS train 0.1428451250323047 valid 0.20431656691622227
LOSS train 0.1428451250323047 valid 0.20472320537818106
LOSS train 0.1428451250323047 valid 0.20555021023998657
LOSS train 0.1428451250323047 valid 0.2057162288230719
LOSS train 0.1428451250323047 valid 0.20620565253252887
LOSS train 0.1428451250323047 valid 0.2060625215973517
LOSS train 0.1428451250323047 valid 0.20637373104691506
LOSS train 0.1428451250323047 valid 0.2063216201149591
LOSS train 0.1428451250323047 valid 0.20678933344635309
LOSS train 0.1428451250323047 valid 0.20638056909575045
LOSS train 0.1428451250323047 valid 0.20622660062060907
LOSS train 0.1428451250323047 valid 0.2065106349331992
LOSS train 0.1428451250323047 valid 0.20651020601672945
LOSS train 0.1428451250323047 valid 0.2063197216419416
LOSS train 0.1428451250323047 valid 0.20638004842179794
LOSS train 0.1428451250323047 valid 0.20584484866452873
LOSS train 0.1428451250323047 valid 0.2055925562977791
LOSS train 0.1428451250323047 valid 0.20563254579230472
LOSS train 0.1428451250323047 valid 0.20575248995529755
LOSS train 0.1428451250323047 valid 0.20552216021888023
LOSS train 0.1428451250323047 valid 0.20572861087949654
LOSS train 0.1428451250323047 valid 0.20606493224268374
LOSS train 0.1428451250323047 valid 0.20602433666073042
LOSS train 0.1428451250323047 valid 0.20611897352923694
LOSS train 0.1428451250323047 valid 0.20616080877134355
LOSS train 0.1428451250323047 valid 0.20592922335412323
LOSS train 0.1428451250323047 valid 0.2058896007637183
LOSS train 0.1428451250323047 valid 0.20595675485193238
LOSS train 0.1428451250323047 valid 0.20597522547010516
LOSS train 0.1428451250323047 valid 0.20592359883513878
LOSS train 0.1428451250323047 valid 0.20600123955838143
LOSS train 0.1428451250323047 valid 0.2061463314294815
LOSS train 0.1428451250323047 valid 0.20600194424863846
LOSS train 0.1428451250323047 valid 0.20584215176856424
LOSS train 0.1428451250323047 valid 0.20574655977543443
LOSS train 0.1428451250323047 valid 0.2056578763926676
LOSS train 0.1428451250323047 valid 0.20538036743035684
LOSS train 0.1428451250323047 valid 0.20539588521000082
LOSS train 0.1428451250323047 valid 0.20555428144606677
LOSS train 0.1428451250323047 valid 0.2055277812077587
LOSS train 0.1428451250323047 valid 0.20558632921371886
LOSS train 0.1428451250323047 valid 0.20564486803831877
LOSS train 0.1428451250323047 valid 0.20585708221530213
LOSS train 0.1428451250323047 valid 0.2056882482593077
LOSS train 0.1428451250323047 valid 0.2053297522707262
LOSS train 0.1428451250323047 valid 0.20529849066151132
LOSS train 0.1428451250323047 valid 0.20516211294702122
LOSS train 0.1428451250323047 valid 0.20543209975915597
LOSS train 0.1428451250323047 valid 0.20565159209597278
LOSS train 0.1428451250323047 valid 0.20578165362764905
LOSS train 0.1428451250323047 valid 0.20571114278088012
LOSS train 0.1428451250323047 valid 0.2055095954187985
LOSS train 0.1428451250323047 valid 0.20555774887947187
LOSS train 0.1428451250323047 valid 0.20554297057544293
LOSS train 0.1428451250323047 valid 0.20530950278043747
LOSS train 0.1428451250323047 valid 0.2055273504065187
LOSS train 0.1428451250323047 valid 0.20545603454113007
LOSS train 0.1428451250323047 valid 0.20535376253506996
LOSS train 0.1428451250323047 valid 0.20539721777956738
LOSS train 0.1428451250323047 valid 0.2051436158956266
LOSS train 0.1428451250323047 valid 0.2051892475067795
LOSS train 0.1428451250323047 valid 0.205020423954533
LOSS train 0.1428451250323047 valid 0.20523241869150063
LOSS train 0.1428451250323047 valid 0.20506322905895816
LOSS train 0.1428451250323047 valid 0.2049843419013144
LOSS train 0.1428451250323047 valid 0.20526971185357315
LOSS train 0.1428451250323047 valid 0.20525567969307304
LOSS train 0.1428451250323047 valid 0.20542102330219672
LOSS train 0.1428451250323047 valid 0.20544358066938542
LOSS train 0.1428451250323047 valid 0.2051822942641615
LOSS train 0.1428451250323047 valid 0.2052252384402403
LOSS train 0.1428451250323047 valid 0.2053945535963232
LOSS train 0.1428451250323047 valid 0.20528185367584229
LOSS train 0.1428451250323047 valid 0.20537346869171735
LOSS train 0.1428451250323047 valid 0.20502385701097192
LOSS train 0.1428451250323047 valid 0.20489659288225795
LOSS train 0.1428451250323047 valid 0.20475471545668208
LOSS train 0.1428451250323047 valid 0.20457816699094938
LOSS train 0.1428451250323047 valid 0.20464588918311652
LOSS train 0.1428451250323047 valid 0.2047154938726756
LOSS train 0.1428451250323047 valid 0.20475481193641137
LOSS train 0.1428451250323047 valid 0.20449593586581094
LOSS train 0.1428451250323047 valid 0.20448461124165493
LOSS train 0.1428451250323047 valid 0.20436113138320083
LOSS train 0.1428451250323047 valid 0.204379491926579
LOSS train 0.1428451250323047 valid 0.2043725784930437
LOSS train 0.1428451250323047 valid 0.20437468878097004
LOSS train 0.1428451250323047 valid 0.20425118892890973
LOSS train 0.1428451250323047 valid 0.20413257738391122
LOSS train 0.1428451250323047 valid 0.20424898156051427
LOSS train 0.1428451250323047 valid 0.20429873385507127
LOSS train 0.1428451250323047 valid 0.20436780581603178
LOSS train 0.1428451250323047 valid 0.20456543356500645
LOSS train 0.1428451250323047 valid 0.20449197324202023
LOSS train 0.1428451250323047 valid 0.20445089969546237
LOSS train 0.1428451250323047 valid 0.204113020152642
LOSS train 0.1428451250323047 valid 0.20409056654101923
LOSS train 0.1428451250323047 valid 0.20420724270543503
LOSS train 0.1428451250323047 valid 0.20423507092831036
LOSS train 0.1428451250323047 valid 0.2039970277659016
LOSS train 0.1428451250323047 valid 0.20406246123854646
LOSS train 0.1428451250323047 valid 0.2039659681992653
LOSS train 0.1428451250323047 valid 0.20420043299696883
LOSS train 0.1428451250323047 valid 0.20433277277474476
LOSS train 0.1428451250323047 valid 0.20442534749857103
LOSS train 0.1428451250323047 valid 0.20451873855375166
LOSS train 0.1428451250323047 valid 0.2047338081896305
LOSS train 0.1428451250323047 valid 0.204758551435091
LOSS train 0.1428451250323047 valid 0.20495718341357638
LOSS train 0.1428451250323047 valid 0.20508006934461923
LOSS train 0.1428451250323047 valid 0.20486930874632855
LOSS train 0.1428451250323047 valid 0.20470853593291305
LOSS train 0.1428451250323047 valid 0.20466505186361017
LOSS train 0.1428451250323047 valid 0.20450045175598439
LOSS train 0.1428451250323047 valid 0.20428332414191502
LOSS train 0.1428451250323047 valid 0.20430780170066504
LOSS train 0.1428451250323047 valid 0.20454357919238864
LOSS train 0.1428451250323047 valid 0.20480609801708238
LOSS train 0.1428451250323047 valid 0.20467324948535776
LOSS train 0.1428451250323047 valid 0.20463252347399932
LOSS train 0.1428451250323047 valid 0.2045888111134556
LOSS train 0.1428451250323047 valid 0.2043183477812035
LOSS train 0.1428451250323047 valid 0.20410283144425462
LOSS train 0.1428451250323047 valid 0.2041330869022053
LOSS train 0.1428451250323047 valid 0.2040776634981873
LOSS train 0.1428451250323047 valid 0.20416712720100194
LOSS train 0.1428451250323047 valid 0.2041068822145462
LOSS train 0.1428451250323047 valid 0.20409105300094207
LOSS train 0.1428451250323047 valid 0.2042004477467623
LOSS train 0.1428451250323047 valid 0.20408278456450576
LOSS train 0.1428451250323047 valid 0.2039562982640096
LOSS train 0.1428451250323047 valid 0.20366558843188814
LOSS train 0.1428451250323047 valid 0.20349660291608457
LOSS train 0.1428451250323047 valid 0.20349769235182438
LOSS train 0.1428451250323047 valid 0.2035515494644642
LOSS train 0.1428451250323047 valid 0.20351316841668957
LOSS train 0.1428451250323047 valid 0.2036643981285717
LOSS train 0.1428451250323047 valid 0.20372346069389607
LOSS train 0.1428451250323047 valid 0.2039021161738141
LOSS train 0.1428451250323047 valid 0.2040395072857198
LOSS train 0.1428451250323047 valid 0.20400501940494928
LOSS train 0.1428451250323047 valid 0.2039506805703995
LOSS train 0.1428451250323047 valid 0.20376877657185166
LOSS train 0.1428451250323047 valid 0.20381591226984178
LOSS train 0.1428451250323047 valid 0.2038580487881388
LOSS train 0.1428451250323047 valid 0.20385820839444962
LOSS train 0.1428451250323047 valid 0.20345904777447382
LOSS train 0.1428451250323047 valid 0.20338252279521024
LOSS train 0.1428451250323047 valid 0.2034524753689766
LOSS train 0.1428451250323047 valid 0.20348775901912172
LOSS train 0.1428451250323047 valid 0.2035427687720197
LOSS train 0.1428451250323047 valid 0.2037348106807592
LOSS train 0.1428451250323047 valid 0.20384601539954905
LOSS train 0.1428451250323047 valid 0.20386545191168304
LOSS train 0.1428451250323047 valid 0.20380823951094382
LOSS train 0.1428451250323047 valid 0.2036946816856124
LOSS train 0.1428451250323047 valid 0.203825628221035
LOSS train 0.1428451250323047 valid 0.20390543151661694
LOSS train 0.1428451250323047 valid 0.20394783981499218
LOSS train 0.1428451250323047 valid 0.20385213343522293
LOSS train 0.1428451250323047 valid 0.20392366598440906
LOSS train 0.1428451250323047 valid 0.20392379094572627
LOSS train 0.1428451250323047 valid 0.20395647536497563
LOSS train 0.1428451250323047 valid 0.2038721202760355
LOSS train 0.1428451250323047 valid 0.2039465172461761
LOSS train 0.1428451250323047 valid 0.20387871396587623
LOSS train 0.1428451250323047 valid 0.20375730074368992
LOSS train 0.1428451250323047 valid 0.20373824438601162
LOSS train 0.1428451250323047 valid 0.2040003432913591
LOSS train 0.1428451250323047 valid 0.20386593344093729
LOSS train 0.1428451250323047 valid 0.20388623575369516
LOSS train 0.1428451250323047 valid 0.20398403063135326
LOSS train 0.1428451250323047 valid 0.20407383272746452
LOSS train 0.1428451250323047 valid 0.20406053458037002
LOSS train 0.1428451250323047 valid 0.20408950117764188
LOSS train 0.1428451250323047 valid 0.2041407880619113
LOSS train 0.1428451250323047 valid 0.2040168598294258
LOSS train 0.1428451250323047 valid 0.20393377668963147
LOSS train 0.1428451250323047 valid 0.20375374923734105
LOSS train 0.1428451250323047 valid 0.20362524115122282
LOSS train 0.1428451250323047 valid 0.20358512142713922
LOSS train 0.1428451250323047 valid 0.20369475353847852
LOSS train 0.1428451250323047 valid 0.20383087329674457
LOSS train 0.1428451250323047 valid 0.20375243867562565
LOSS train 0.1428451250323047 valid 0.20375240170698372
LOSS train 0.1428451250323047 valid 0.2038225693407879
LOSS train 0.1428451250323047 valid 0.20364793954151017
LOSS train 0.1428451250323047 valid 0.2035151588004679
LOSS train 0.1428451250323047 valid 0.2034983847897949
LOSS train 0.1428451250323047 valid 0.2035209513911089
LOSS train 0.1428451250323047 valid 0.20332874069121523
LOSS train 0.1428451250323047 valid 0.20344088380796868
LOSS train 0.1428451250323047 valid 0.2034363053686969
LOSS train 0.1428451250323047 valid 0.20357682293715793
LOSS train 0.1428451250323047 valid 0.20353815590755808
LOSS train 0.1428451250323047 valid 0.2035195023130793
LOSS train 0.1428451250323047 valid 0.20341504894453905
LOSS train 0.1428451250323047 valid 0.20338543926932148
LOSS train 0.1428451250323047 valid 0.20336519625701316
LOSS train 0.1428451250323047 valid 0.2032998343071433
LOSS train 0.1428451250323047 valid 0.20328588140051382
LOSS train 0.1428451250323047 valid 0.20327242708812326
LOSS train 0.1428451250323047 valid 0.2033125515020377
LOSS train 0.1428451250323047 valid 0.20332350047549816
LOSS train 0.1428451250323047 valid 0.2033520912964072
LOSS train 0.1428451250323047 valid 0.20334868476941034
LOSS train 0.1428451250323047 valid 0.20337873523434002
LOSS train 0.1428451250323047 valid 0.2033521168651771
LOSS train 0.1428451250323047 valid 0.20334355735423548
LOSS train 0.1428451250323047 valid 0.2033838843650157
LOSS train 0.1428451250323047 valid 0.20335086278225245
LOSS train 0.1428451250323047 valid 0.20323844170961225
LOSS train 0.1428451250323047 valid 0.2033783429685761
LOSS train 0.1428451250323047 valid 0.2032807048058277
LOSS train 0.1428451250323047 valid 0.2033677427219106
LOSS train 0.1428451250323047 valid 0.2034007122312163
LOSS train 0.1428451250323047 valid 0.2034349666487786
LOSS train 0.1428451250323047 valid 0.20348029308188767
LOSS train 0.1428451250323047 valid 0.20345879284044108
LOSS train 0.1428451250323047 valid 0.2034663470408406
LOSS train 0.1428451250323047 valid 0.20346143642428574
LOSS train 0.1428451250323047 valid 0.20348633045241946
LOSS train 0.1428451250323047 valid 0.20349075390568264
LOSS train 0.1428451250323047 valid 0.203467305103311
LOSS train 0.1428451250323047 valid 0.20352884349208208
LOSS train 0.1428451250323047 valid 0.20353349001616894
LOSS train 0.1428451250323047 valid 0.2034179737791419
LOSS train 0.1428451250323047 valid 0.2034245913738031
LOSS train 0.1428451250323047 valid 0.20335360945011519
LOSS train 0.1428451250323047 valid 0.20330177427445403
LOSS train 0.1428451250323047 valid 0.20323668282341073
LOSS train 0.1428451250323047 valid 0.2030654517045388
LOSS train 0.1428451250323047 valid 0.20314901308413663
LOSS train 0.1428451250323047 valid 0.20303961589066624
LOSS train 0.1428451250323047 valid 0.2031168911333491
LOSS train 0.1428451250323047 valid 0.20315709311549063
LOSS train 0.1428451250323047 valid 0.20324239202521063
LOSS train 0.1428451250323047 valid 0.2031600260122305
LOSS train 0.1428451250323047 valid 0.20314060007951346
LOSS train 0.1428451250323047 valid 0.20315437238137643
LOSS train 0.1428451250323047 valid 0.20309084033716224
LOSS train 0.1428451250323047 valid 0.2029706746784609
LOSS train 0.1428451250323047 valid 0.20289438945196925
LOSS train 0.1428451250323047 valid 0.20304208932007808
LOSS train 0.1428451250323047 valid 0.20298513329240697
LOSS train 0.1428451250323047 valid 0.20289162579363426
LOSS train 0.1428451250323047 valid 0.20294604446081554
LOSS train 0.1428451250323047 valid 0.2029857741248223
LOSS train 0.1428451250323047 valid 0.20299568415036676
LOSS train 0.1428451250323047 valid 0.20285904324958345
LOSS train 0.1428451250323047 valid 0.20296447702445264
LOSS train 0.1428451250323047 valid 0.20303395139998284
LOSS train 0.1428451250323047 valid 0.20294863571321345
LOSS train 0.1428451250323047 valid 0.20288697013937432
LOSS train 0.1428451250323047 valid 0.20287242437573685
LOSS train 0.1428451250323047 valid 0.20285410961312345
LOSS train 0.1428451250323047 valid 0.2029032644203731
LOSS train 0.1428451250323047 valid 0.20294476561566704
LOSS train 0.1428451250323047 valid 0.20298873281783678
LOSS train 0.1428451250323047 valid 0.20306881464405707
LOSS train 0.1428451250323047 valid 0.20295216993423504
LOSS train 0.1428451250323047 valid 0.20283993071233722
LOSS train 0.1428451250323047 valid 0.20282974809910473
LOSS train 0.1428451250323047 valid 0.20286703589750604
LOSS train 0.1428451250323047 valid 0.20296026770462536
LOSS train 0.1428451250323047 valid 0.20296060001285626
LOSS train 0.1428451250323047 valid 0.20322383832600383
LOSS train 0.1428451250323047 valid 0.20319417135537166
LOSS train 0.1428451250323047 valid 0.20332352380726218
LOSS train 0.1428451250323047 valid 0.203344810435923
LOSS train 0.1428451250323047 valid 0.2032501329923724
LOSS train 0.1428451250323047 valid 0.20327443725442232
LOSS train 0.1428451250323047 valid 0.20327657648448735
LOSS train 0.1428451250323047 valid 0.20332228966889654
LOSS train 0.1428451250323047 valid 0.2031975028955418
LOSS train 0.1428451250323047 valid 0.20331420872592668
EPOCH 9:
  batch 1 loss: 0.15620970726013184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1315455660223961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1328865091005961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.13187017291784286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13530717194080352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13568734377622604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1350584626197815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1320046754553914
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13273587243424523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1310894712805748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13217362761497498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13426672046383223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1342914505646779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1321332012968404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13158787737290065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1323851770721376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13360734415404937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13486778115232786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1359635179764346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13617055527865887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13693321851037799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13637867603789677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13457323092481363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13443534262478352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1343718534708023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13632507163744706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13688280736958539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1373296646135194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13830965551836738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13785746718446415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13758539504581882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1367970595601946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13699332263433572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13800391883534543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13876742145844867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1401511188596487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1412103371442975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.14028929801363693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13965880335905614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14028967693448066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.14036253530804704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1404319213969367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14064092726208444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1409735351123593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14073591464095644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.14042862634296002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14054345767548743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1411647585531076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1411459585841821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1410079190135002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14078863344940484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1403430998325348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14050598358208277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14076438739343924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14105845337564296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.14146303039576327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14174649537655346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14146605082627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.14210989667197405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14286278560757637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14353390233438523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14318391428359092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14276494724409922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14278831286355853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14292382322824917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14289516007358377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14309613784747338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14333349597804687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14312579364016437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1428074796284948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.14291245647719208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14255334581765863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.14281959505113836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1431842536942379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14311435619990032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14316960463398382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1432529233105771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14346358046317711
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14357490026498143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14371117074042558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14373728927270865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14387124504257992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14374150461461171
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14408561622812635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1442138340543298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14378343366606292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14387009112999358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14370625059713016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14380306507764237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1439000290301111
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14391306779541813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.143669992685318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1437146936052589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14371031871501436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.1435209007639634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14348737398783365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14340906014147492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1432323980392242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14351337832031827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1435353623330593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14363188171150662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1436463525774432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1436098643298288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1438435545334449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14383261515980675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1438445507917764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14379198890980158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14368493965378515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14369735723241753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14391211000355808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14379343081701984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14357399554657085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1434469621265884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14347683860544572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14351917712584786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14362841022425685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14367620035623893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14353825454994784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14370369372748526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14356886520981788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14366447617692396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1437256896837813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14370933453726575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14368426463296335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14385518431663513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14387434803777271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14399816365692558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1439392619067803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14376877629479698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14370515587238164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14364569321388507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14329483326185832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14323220654089647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14301833938529243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14294746225630794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.1432012523907949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1432950725629382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14320555280732072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1433688860467012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14319337441452912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14325570494781995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14333798836024714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14332054685671014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.1431127063309153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14329211002793804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14328802662761245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14304758547520152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14316666015499346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.143352820469229
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14334691514571507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.143474335800733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14360158802255205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14371746476569208
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1436415779126155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.143591885412893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1435604200531275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14347989657881913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14354501127064984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14355846667814554
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14345440985634922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1434315845647954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14360159350398147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.1436456685965778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1435925374489005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14336955587972294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14318426702934575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14320187453559774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1430366715593707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.1429323106563303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14303069101536975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14307258112563026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14297744228916112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.1428550969314024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14290647508426643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14293637973921638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14299993564120747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14340290763957353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14334823936223984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.143410543656216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14338130917814043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14343978868005025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1435609153666339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14368106579520012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1436167621742124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14358164764739373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14362070092590906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1437128407751175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.1435961439571482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14362817419269097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14365347780679402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14362918628447968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1438432764261961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14381636509314719
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14383533927275963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14381115803351768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.143734732908862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.143475715089873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14345767601120352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1433096039459933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14347052704542876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14340199348493596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14336490096284613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14342147093541516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14337937493680739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14328297580160745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14334471943309007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1431888092831137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14306799021477884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14293071417289488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14305947071739605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14304364670368167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14323157680062754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14330997318029404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14328719003596038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14322931998690894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14323392776013524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14322553187059367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1431761577110225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14325733846861477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1430309787731279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14308229146095422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14306799916399493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14315870100206324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14311499245065665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1431794379485978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14308143037874088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14321824100836783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.143093254449859
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14302314788623668
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14308212104698886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14309192984264135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14311389891623422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1430421519241108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14298135344671387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14289482733670703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14293340483080533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1428870740759222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14299064107677517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14288696858673414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14284323826432227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14278792778486038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14280222085389224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14280650688051688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14279637552919935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1427880759141883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14262785486937538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14267672432458353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14277531065407298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14279608521236473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14284000501036645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1428447795875994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1428231592511847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14275449811823282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14271892868275718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14280201053502514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14280177085311152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14269410360763973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14263218638393305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1426944267508146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14269944678705473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14286632650314163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14297680013621125
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14283080390537647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14287461213428865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14289905555405708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1427930076711608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14281659322173407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14288440605264102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14277654528950226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1428771328870897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14281696072913624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1427329128662891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1426438326254869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14269553472960952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14257002153179862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14247972650480442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14257747029031656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14250931567210945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14243943652799052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14248263367584774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14239237847277278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14240431077514135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14234360599686316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1423912904107235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1425468956692177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14256310884977555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14266365782101395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1425788380826513
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14245216414383952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1424401862354114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14244696067780563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14245147451962512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14259348448632927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1425494487796511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14260930353302068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1427143913869922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1426464478526051
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14280193646582182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14281449312450095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14270979816714924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14265668812978308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14252748362571988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14257728013366755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1426190587103759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14264593698450775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14259726887727095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14253856268398926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14254294614029395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14261930445540685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.1425912934685907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14253320247893164
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14247776656292188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1425357257929473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14257482640967248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14271631200635243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14270295662498927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14273314119231437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14272197994327396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14273446483017882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1426818490261212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.1426690692702929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14280427083002856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14268265281668388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14262283007027926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14272421293533766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14270775433789734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14271251217032063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14271139714685155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14278045412164328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14272595052466247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14267657048932736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14275411835097404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14280531548701966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14280093894033374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14275488951312962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.142834921722256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14283352219209472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1427597373106776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1427069861320971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1427405461888103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1427593885148026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14272617995303277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14276087473067875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14274340987118872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14283060377490694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14278307559259365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14284457904031708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14284778328548217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.14275361219671873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14279374333364622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14289435712445495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14284919814037328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14283228961080735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14295049708357638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1429112897582457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1428550025971418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1427788214773691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1427275812326197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14273150103337917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14274116328193082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14272376601880937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1428028457895827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14279401651262907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1427810321060511
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14282482258261067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14285815026753587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14276725419210803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1428575859526577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1428807677857598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14294966568012496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1429897216211432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14296575407347373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1429128187830902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14296491089511046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.1429709282318751
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1429583956982861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14299768371512467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14302252118707334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14311970695656648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14319787072507958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14320039839100024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.143198525094237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14321366263743168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14321762917097658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14314451132501874
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14316020365514903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.1430669298313693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1431277819054643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1431055776487586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1431206538891181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14319524458607139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14321026213619173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14320404732803654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14317787589911882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1432566772910613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1432202369200461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.1432300541427934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1432159524317363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1432233364659742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14321817483752966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14329321649009152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14335523430832584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14333125675286607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14331811072655243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14330964187781017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14331226924370075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1433031238267697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1432420107371667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14327335958813106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14327110124797357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14336197252256155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14337199065581108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14330231234178706
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14320679010760382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14320378043206342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.1432591775396409
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14314993177886776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14311807640075114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14311365909465457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14308600441685745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14311669255449766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14318167197548948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14312810614599403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14320195789607065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14320288588018978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14314350120268518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14313606493855527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14319226238006186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14317369841403896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14322282195437785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14321718036506126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1433258912595058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14332453433346803
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14338496123110095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14338144812433198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14343974795705133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14346812694446445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14352523220796562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14352703048658805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14355114612051031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.143562713329619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1435444028163118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.1435829654134154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1435494117002498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14349088039291039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14345550443559485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14342153195700122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14343086004789388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1434381635390836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14338976151413388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14340528660761542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14335841662456503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14331443663727633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1432203403325333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14314244797269066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14314623443377122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14312775395566055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.143080084328438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14300304748653586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14303526360055674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14303654416134973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1429944165954084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14299040484055073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14307147137628035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1430299600446096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14306990500107855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1431094897667652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14310020808544424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14309248695177818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1429959606617055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1430477359175935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14300470425100145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14300470425100145 valid 0.18246638774871826
LOSS train 0.14300470425100145 valid 0.2078024297952652
LOSS train 0.14300470425100145 valid 0.2186567187309265
LOSS train 0.14300470425100145 valid 0.2153955213725567
LOSS train 0.14300470425100145 valid 0.21484998166561126
LOSS train 0.14300470425100145 valid 0.22094426800807318
LOSS train 0.14300470425100145 valid 0.2153527034180505
LOSS train 0.14300470425100145 valid 0.21513858251273632
LOSS train 0.14300470425100145 valid 0.21507525775167677
LOSS train 0.14300470425100145 valid 0.21604593098163605
LOSS train 0.14300470425100145 valid 0.2125487435947765
LOSS train 0.14300470425100145 valid 0.21524788190921149
LOSS train 0.14300470425100145 valid 0.21384376745957595
LOSS train 0.14300470425100145 valid 0.21408225702387945
LOSS train 0.14300470425100145 valid 0.21283480823040007
LOSS train 0.14300470425100145 valid 0.21301907021552324
LOSS train 0.14300470425100145 valid 0.21340200567946715
LOSS train 0.14300470425100145 valid 0.21370394776264826
LOSS train 0.14300470425100145 valid 0.21564552109492452
LOSS train 0.14300470425100145 valid 0.2150515466928482
LOSS train 0.14300470425100145 valid 0.21553078151884533
LOSS train 0.14300470425100145 valid 0.21395737474614923
LOSS train 0.14300470425100145 valid 0.21199027740437051
LOSS train 0.14300470425100145 valid 0.21274981958170733
LOSS train 0.14300470425100145 valid 0.21125388085842134
LOSS train 0.14300470425100145 valid 0.21075962426570746
LOSS train 0.14300470425100145 valid 0.21043229599793753
LOSS train 0.14300470425100145 valid 0.21052412582295282
LOSS train 0.14300470425100145 valid 0.20911090846719413
LOSS train 0.14300470425100145 valid 0.20822540720303853
LOSS train 0.14300470425100145 valid 0.20736236485742754
LOSS train 0.14300470425100145 valid 0.20820994628593326
LOSS train 0.14300470425100145 valid 0.20696786903973782
LOSS train 0.14300470425100145 valid 0.2064147197148379
LOSS train 0.14300470425100145 valid 0.2068297782114574
LOSS train 0.14300470425100145 valid 0.2069374844431877
LOSS train 0.14300470425100145 valid 0.20591489567949967
LOSS train 0.14300470425100145 valid 0.20592071115970612
LOSS train 0.14300470425100145 valid 0.20569800451779977
LOSS train 0.14300470425100145 valid 0.2052785713225603
LOSS train 0.14300470425100145 valid 0.20517454241834035
LOSS train 0.14300470425100145 valid 0.20631700647728785
LOSS train 0.14300470425100145 valid 0.20710267265175664
LOSS train 0.14300470425100145 valid 0.20615680414167317
LOSS train 0.14300470425100145 valid 0.20556553602218627
LOSS train 0.14300470425100145 valid 0.20395803257175113
LOSS train 0.14300470425100145 valid 0.20319290585974428
LOSS train 0.14300470425100145 valid 0.2047224873676896
LOSS train 0.14300470425100145 valid 0.20456428521749925
LOSS train 0.14300470425100145 valid 0.2049015587568283
LOSS train 0.14300470425100145 valid 0.20471557098276474
LOSS train 0.14300470425100145 valid 0.20444786319365868
LOSS train 0.14300470425100145 valid 0.20548989356688732
LOSS train 0.14300470425100145 valid 0.2053595099736143
LOSS train 0.14300470425100145 valid 0.20558152307163585
LOSS train 0.14300470425100145 valid 0.20523611323109694
LOSS train 0.14300470425100145 valid 0.2045321532508783
LOSS train 0.14300470425100145 valid 0.20530957651549372
LOSS train 0.14300470425100145 valid 0.20549801648673366
LOSS train 0.14300470425100145 valid 0.20514668772617975
LOSS train 0.14300470425100145 valid 0.20495043181982198
LOSS train 0.14300470425100145 valid 0.20457911251052732
LOSS train 0.14300470425100145 valid 0.20466767795502194
LOSS train 0.14300470425100145 valid 0.20453423634171486
LOSS train 0.14300470425100145 valid 0.20372225046157838
LOSS train 0.14300470425100145 valid 0.20361554306564908
LOSS train 0.14300470425100145 valid 0.20423202372308988
LOSS train 0.14300470425100145 valid 0.2035837697193903
LOSS train 0.14300470425100145 valid 0.20395355466483295
LOSS train 0.14300470425100145 valid 0.20445711804287775
LOSS train 0.14300470425100145 valid 0.20457620603937499
LOSS train 0.14300470425100145 valid 0.20505425499545205
LOSS train 0.14300470425100145 valid 0.20533166968659178
LOSS train 0.14300470425100145 valid 0.20527836599865476
LOSS train 0.14300470425100145 valid 0.205028152068456
LOSS train 0.14300470425100145 valid 0.20510934097202202
LOSS train 0.14300470425100145 valid 0.20516281255653926
LOSS train 0.14300470425100145 valid 0.20505149872639242
LOSS train 0.14300470425100145 valid 0.20533366086362284
LOSS train 0.14300470425100145 valid 0.20501080565154553
LOSS train 0.14300470425100145 valid 0.2051535815368464
LOSS train 0.14300470425100145 valid 0.20543048203718373
LOSS train 0.14300470425100145 valid 0.2050963864628091
LOSS train 0.14300470425100145 valid 0.2049588140632425
LOSS train 0.14300470425100145 valid 0.20516645908355713
LOSS train 0.14300470425100145 valid 0.2052657203965409
LOSS train 0.14300470425100145 valid 0.20486176579162993
LOSS train 0.14300470425100145 valid 0.20476196909492667
LOSS train 0.14300470425100145 valid 0.2048958417739761
LOSS train 0.14300470425100145 valid 0.20471360501315858
LOSS train 0.14300470425100145 valid 0.20481414732697245
LOSS train 0.14300470425100145 valid 0.20486928052876308
LOSS train 0.14300470425100145 valid 0.20510235765287954
LOSS train 0.14300470425100145 valid 0.20547110032528004
LOSS train 0.14300470425100145 valid 0.20590694327103465
LOSS train 0.14300470425100145 valid 0.2067885516832272
LOSS train 0.14300470425100145 valid 0.2069563226601512
LOSS train 0.14300470425100145 valid 0.20747089933375923
LOSS train 0.14300470425100145 valid 0.20730450779500634
LOSS train 0.14300470425100145 valid 0.2076362493634224
LOSS train 0.14300470425100145 valid 0.20758934437048318
LOSS train 0.14300470425100145 valid 0.20806939125645393
LOSS train 0.14300470425100145 valid 0.20764741590879496
LOSS train 0.14300470425100145 valid 0.20751522323833063
LOSS train 0.14300470425100145 valid 0.20774925521441867
LOSS train 0.14300470425100145 valid 0.20773709242073995
LOSS train 0.14300470425100145 valid 0.2075294974808381
LOSS train 0.14300470425100145 valid 0.20758587232342474
LOSS train 0.14300470425100145 valid 0.20704748187590083
LOSS train 0.14300470425100145 valid 0.20679127804257652
LOSS train 0.14300470425100145 valid 0.20679506847450324
LOSS train 0.14300470425100145 valid 0.20695144536771945
LOSS train 0.14300470425100145 valid 0.20671348089137964
LOSS train 0.14300470425100145 valid 0.206929426967052
LOSS train 0.14300470425100145 valid 0.20728337505589362
LOSS train 0.14300470425100145 valid 0.2072399290728158
LOSS train 0.14300470425100145 valid 0.20730226658857787
LOSS train 0.14300470425100145 valid 0.2073574470261396
LOSS train 0.14300470425100145 valid 0.20712590618293827
LOSS train 0.14300470425100145 valid 0.2070862464606762
LOSS train 0.14300470425100145 valid 0.2071409344919457
LOSS train 0.14300470425100145 valid 0.20714346490433957
LOSS train 0.14300470425100145 valid 0.20707447984354282
LOSS train 0.14300470425100145 valid 0.20717069614798791
LOSS train 0.14300470425100145 valid 0.20731700241565704
LOSS train 0.14300470425100145 valid 0.2071652975347307
LOSS train 0.14300470425100145 valid 0.206999909455382
LOSS train 0.14300470425100145 valid 0.20691702945623547
LOSS train 0.14300470425100145 valid 0.20681717044623324
LOSS train 0.14300470425100145 valid 0.20653239488601685
LOSS train 0.14300470425100145 valid 0.20657298696860102
LOSS train 0.14300470425100145 valid 0.20674838690143643
LOSS train 0.14300470425100145 valid 0.20668215321418934
LOSS train 0.14300470425100145 valid 0.20673855560929028
LOSS train 0.14300470425100145 valid 0.20679912721669233
LOSS train 0.14300470425100145 valid 0.20702850194100073
LOSS train 0.14300470425100145 valid 0.2068654027951025
LOSS train 0.14300470425100145 valid 0.20648673554693442
LOSS train 0.14300470425100145 valid 0.20645636309393875
LOSS train 0.14300470425100145 valid 0.20631359251482145
LOSS train 0.14300470425100145 valid 0.20659975601848982
LOSS train 0.14300470425100145 valid 0.20681818679604733
LOSS train 0.14300470425100145 valid 0.20694586624215533
LOSS train 0.14300470425100145 valid 0.20686867740005255
LOSS train 0.14300470425100145 valid 0.2066641720204518
LOSS train 0.14300470425100145 valid 0.20671700818897926
LOSS train 0.14300470425100145 valid 0.2066918795003372
LOSS train 0.14300470425100145 valid 0.20645937796782804
LOSS train 0.14300470425100145 valid 0.20668739290445443
LOSS train 0.14300470425100145 valid 0.20662256211042404
LOSS train 0.14300470425100145 valid 0.206527661606176
LOSS train 0.14300470425100145 valid 0.2065834012862883
LOSS train 0.14300470425100145 valid 0.20632636225691028
LOSS train 0.14300470425100145 valid 0.20637197666741036
LOSS train 0.14300470425100145 valid 0.20618884601900656
LOSS train 0.14300470425100145 valid 0.20639201224996492
LOSS train 0.14300470425100145 valid 0.20620615780353546
LOSS train 0.14300470425100145 valid 0.20611554714320582
LOSS train 0.14300470425100145 valid 0.20644119953584372
LOSS train 0.14300470425100145 valid 0.20641309171915054
LOSS train 0.14300470425100145 valid 0.20657810420723435
LOSS train 0.14300470425100145 valid 0.2065855688702913
LOSS train 0.14300470425100145 valid 0.20631599270858647
LOSS train 0.14300470425100145 valid 0.20633961996290742
LOSS train 0.14300470425100145 valid 0.20651434271624594
LOSS train 0.14300470425100145 valid 0.20641662754926338
LOSS train 0.14300470425100145 valid 0.2065035457204202
LOSS train 0.14300470425100145 valid 0.20614130910308587
LOSS train 0.14300470425100145 valid 0.20600230621515647
LOSS train 0.14300470425100145 valid 0.20584353217307258
LOSS train 0.14300470425100145 valid 0.20565889435901977
LOSS train 0.14300470425100145 valid 0.2057442395666311
LOSS train 0.14300470425100145 valid 0.20582138062212507
LOSS train 0.14300470425100145 valid 0.20584114478237328
LOSS train 0.14300470425100145 valid 0.20557431876659393
LOSS train 0.14300470425100145 valid 0.20555960751053962
LOSS train 0.14300470425100145 valid 0.2054263211913028
LOSS train 0.14300470425100145 valid 0.2054386572556549
LOSS train 0.14300470425100145 valid 0.20544010159356635
LOSS train 0.14300470425100145 valid 0.20543507941895062
LOSS train 0.14300470425100145 valid 0.2053165440730627
LOSS train 0.14300470425100145 valid 0.20519265712617518
LOSS train 0.14300470425100145 valid 0.20531009292341973
LOSS train 0.14300470425100145 valid 0.20536542724332082
LOSS train 0.14300470425100145 valid 0.20543837321771158
LOSS train 0.14300470425100145 valid 0.20564127328895754
LOSS train 0.14300470425100145 valid 0.20557748331105646
LOSS train 0.14300470425100145 valid 0.2055386002076433
LOSS train 0.14300470425100145 valid 0.20518862570404375
LOSS train 0.14300470425100145 valid 0.2051594700468214
LOSS train 0.14300470425100145 valid 0.20528278269692865
LOSS train 0.14300470425100145 valid 0.20530976774170995
LOSS train 0.14300470425100145 valid 0.20505953004940805
LOSS train 0.14300470425100145 valid 0.20514557541338438
LOSS train 0.14300470425100145 valid 0.20505598554244409
LOSS train 0.14300470425100145 valid 0.20528906903096608
LOSS train 0.14300470425100145 valid 0.2054378895590148
LOSS train 0.14300470425100145 valid 0.20552645593580574
LOSS train 0.14300470425100145 valid 0.20561631742733807
LOSS train 0.14300470425100145 valid 0.20582234539091587
LOSS train 0.14300470425100145 valid 0.2058611825034393
LOSS train 0.14300470425100145 valid 0.2060570558994123
LOSS train 0.14300470425100145 valid 0.2061967406366846
LOSS train 0.14300470425100145 valid 0.2059850482379689
LOSS train 0.14300470425100145 valid 0.20580905072572755
LOSS train 0.14300470425100145 valid 0.20577064293970182
LOSS train 0.14300470425100145 valid 0.2056001644347601
LOSS train 0.14300470425100145 valid 0.20538038001037562
LOSS train 0.14300470425100145 valid 0.20541461132930228
LOSS train 0.14300470425100145 valid 0.20566222170988718
LOSS train 0.14300470425100145 valid 0.2059207993661058
LOSS train 0.14300470425100145 valid 0.2057863878051065
LOSS train 0.14300470425100145 valid 0.20576477778349683
LOSS train 0.14300470425100145 valid 0.20571756892115156
LOSS train 0.14300470425100145 valid 0.20545448723227477
LOSS train 0.14300470425100145 valid 0.2052312509191257
LOSS train 0.14300470425100145 valid 0.20526883768321183
LOSS train 0.14300470425100145 valid 0.20520731239417278
LOSS train 0.14300470425100145 valid 0.20531259688068199
LOSS train 0.14300470425100145 valid 0.20523811666802927
LOSS train 0.14300470425100145 valid 0.2052246865508783
LOSS train 0.14300470425100145 valid 0.20533524943633122
LOSS train 0.14300470425100145 valid 0.20522598594828037
LOSS train 0.14300470425100145 valid 0.20510248354236996
LOSS train 0.14300470425100145 valid 0.20480500095420415
LOSS train 0.14300470425100145 valid 0.2046193107303265
LOSS train 0.14300470425100145 valid 0.20463675588763233
LOSS train 0.14300470425100145 valid 0.2046971174708584
LOSS train 0.14300470425100145 valid 0.20465174096119976
LOSS train 0.14300470425100145 valid 0.2047944383776706
LOSS train 0.14300470425100145 valid 0.20484132548689327
LOSS train 0.14300470425100145 valid 0.2050133610850778
LOSS train 0.14300470425100145 valid 0.2051671304672061
LOSS train 0.14300470425100145 valid 0.20513431154764616
LOSS train 0.14300470425100145 valid 0.205092897503934
LOSS train 0.14300470425100145 valid 0.20490759806865352
LOSS train 0.14300470425100145 valid 0.20495907283281978
LOSS train 0.14300470425100145 valid 0.2049999114971201
LOSS train 0.14300470425100145 valid 0.2049895400282728
LOSS train 0.14300470425100145 valid 0.2045830007021626
LOSS train 0.14300470425100145 valid 0.20450110807705718
LOSS train 0.14300470425100145 valid 0.20457225102038423
LOSS train 0.14300470425100145 valid 0.20460709218135095
LOSS train 0.14300470425100145 valid 0.20465916401294412
LOSS train 0.14300470425100145 valid 0.20485585423148409
LOSS train 0.14300470425100145 valid 0.20497268771495275
LOSS train 0.14300470425100145 valid 0.20498361031294834
LOSS train 0.14300470425100145 valid 0.2049251159713153
LOSS train 0.14300470425100145 valid 0.204810607744508
LOSS train 0.14300470425100145 valid 0.2049296929240227
LOSS train 0.14300470425100145 valid 0.20500672166328507
LOSS train 0.14300470425100145 valid 0.20505840561929203
LOSS train 0.14300470425100145 valid 0.20496591430878922
LOSS train 0.14300470425100145 valid 0.20504007998883256
LOSS train 0.14300470425100145 valid 0.2050441509952732
LOSS train 0.14300470425100145 valid 0.2050838433788158
LOSS train 0.14300470425100145 valid 0.20499688120203724
LOSS train 0.14300470425100145 valid 0.2050716054531955
LOSS train 0.14300470425100145 valid 0.2049996462222692
LOSS train 0.14300470425100145 valid 0.20489455031660886
LOSS train 0.14300470425100145 valid 0.20487813017834192
LOSS train 0.14300470425100145 valid 0.20514836577513745
LOSS train 0.14300470425100145 valid 0.20501198356142517
LOSS train 0.14300470425100145 valid 0.2050328819702069
LOSS train 0.14300470425100145 valid 0.20512959372322515
LOSS train 0.14300470425100145 valid 0.2052163508601655
LOSS train 0.14300470425100145 valid 0.20520951836073442
LOSS train 0.14300470425100145 valid 0.20524776899325314
LOSS train 0.14300470425100145 valid 0.20529604257261
LOSS train 0.14300470425100145 valid 0.2051576131471881
LOSS train 0.14300470425100145 valid 0.20507108257925377
LOSS train 0.14300470425100145 valid 0.20489270612597466
LOSS train 0.14300470425100145 valid 0.20476068228810698
LOSS train 0.14300470425100145 valid 0.20472779880910025
LOSS train 0.14300470425100145 valid 0.20483573653481224
LOSS train 0.14300470425100145 valid 0.20496913628733676
LOSS train 0.14300470425100145 valid 0.20487762143035226
LOSS train 0.14300470425100145 valid 0.2048649433062231
LOSS train 0.14300470425100145 valid 0.20494869794683218
LOSS train 0.14300470425100145 valid 0.2047729176070009
LOSS train 0.14300470425100145 valid 0.2046435583528675
LOSS train 0.14300470425100145 valid 0.2046222613318592
LOSS train 0.14300470425100145 valid 0.2046280384274338
LOSS train 0.14300470425100145 valid 0.204442190347423
LOSS train 0.14300470425100145 valid 0.20454911626221842
LOSS train 0.14300470425100145 valid 0.20454277593147505
LOSS train 0.14300470425100145 valid 0.20468693644535252
LOSS train 0.14300470425100145 valid 0.2046482686160339
LOSS train 0.14300470425100145 valid 0.20462068693654348
LOSS train 0.14300470425100145 valid 0.20451129376888275
LOSS train 0.14300470425100145 valid 0.20447400837010124
LOSS train 0.14300470425100145 valid 0.20445359916719671
LOSS train 0.14300470425100145 valid 0.20439732105251873
LOSS train 0.14300470425100145 valid 0.204380401188419
LOSS train 0.14300470425100145 valid 0.20436575225854325
LOSS train 0.14300470425100145 valid 0.20440348288094676
LOSS train 0.14300470425100145 valid 0.20442398895920327
LOSS train 0.14300470425100145 valid 0.20443942137812607
LOSS train 0.14300470425100145 valid 0.20443787687398918
LOSS train 0.14300470425100145 valid 0.20445823594927787
LOSS train 0.14300470425100145 valid 0.2044341203778289
LOSS train 0.14300470425100145 valid 0.2044328007575692
LOSS train 0.14300470425100145 valid 0.20447358774273308
LOSS train 0.14300470425100145 valid 0.2044360668055321
LOSS train 0.14300470425100145 valid 0.2043163370402133
LOSS train 0.14300470425100145 valid 0.2044496408182811
LOSS train 0.14300470425100145 valid 0.2043556828743472
LOSS train 0.14300470425100145 valid 0.2044467388519219
LOSS train 0.14300470425100145 valid 0.2044802739203555
LOSS train 0.14300470425100145 valid 0.20451056587119257
LOSS train 0.14300470425100145 valid 0.2045518698416339
LOSS train 0.14300470425100145 valid 0.20452379416196775
LOSS train 0.14300470425100145 valid 0.2045268171701949
LOSS train 0.14300470425100145 valid 0.20452036129631054
LOSS train 0.14300470425100145 valid 0.20454779286233205
LOSS train 0.14300470425100145 valid 0.20454469304296036
LOSS train 0.14300470425100145 valid 0.20451653201121262
LOSS train 0.14300470425100145 valid 0.20457645899282312
LOSS train 0.14300470425100145 valid 0.20458940797278127
LOSS train 0.14300470425100145 valid 0.20446841968223453
LOSS train 0.14300470425100145 valid 0.2044737004323912
LOSS train 0.14300470425100145 valid 0.20439843787169604
LOSS train 0.14300470425100145 valid 0.20434320083712645
LOSS train 0.14300470425100145 valid 0.20428500066936753
LOSS train 0.14300470425100145 valid 0.20410548810775464
LOSS train 0.14300470425100145 valid 0.2041822086813991
LOSS train 0.14300470425100145 valid 0.20407013800166068
LOSS train 0.14300470425100145 valid 0.2041383939908772
LOSS train 0.14300470425100145 valid 0.20418346344881144
LOSS train 0.14300470425100145 valid 0.20428016005140362
LOSS train 0.14300470425100145 valid 0.2041959475804672
LOSS train 0.14300470425100145 valid 0.20418109735810613
LOSS train 0.14300470425100145 valid 0.20418787074160646
LOSS train 0.14300470425100145 valid 0.20412167554308555
LOSS train 0.14300470425100145 valid 0.20399016155235802
LOSS train 0.14300470425100145 valid 0.20391821306908414
LOSS train 0.14300470425100145 valid 0.20405657651339512
LOSS train 0.14300470425100145 valid 0.20399965870486209
LOSS train 0.14300470425100145 valid 0.20390530396527598
LOSS train 0.14300470425100145 valid 0.2039566107970827
LOSS train 0.14300470425100145 valid 0.2039960718050031
LOSS train 0.14300470425100145 valid 0.20400774243630862
LOSS train 0.14300470425100145 valid 0.2038717284233855
LOSS train 0.14300470425100145 valid 0.20397355212548443
LOSS train 0.14300470425100145 valid 0.20403292740600695
LOSS train 0.14300470425100145 valid 0.203941870717644
LOSS train 0.14300470425100145 valid 0.20388064415379284
LOSS train 0.14300470425100145 valid 0.20385635434382263
LOSS train 0.14300470425100145 valid 0.20384108392591122
LOSS train 0.14300470425100145 valid 0.2038839824284826
LOSS train 0.14300470425100145 valid 0.20392676105356625
LOSS train 0.14300470425100145 valid 0.20395658089017327
LOSS train 0.14300470425100145 valid 0.20404231902897865
LOSS train 0.14300470425100145 valid 0.203923634258704
LOSS train 0.14300470425100145 valid 0.20381062257457788
LOSS train 0.14300470425100145 valid 0.20379580862903862
LOSS train 0.14300470425100145 valid 0.20382707931247412
LOSS train 0.14300470425100145 valid 0.20392343838954105
LOSS train 0.14300470425100145 valid 0.2039176006918166
LOSS train 0.14300470425100145 valid 0.20418722534345257
LOSS train 0.14300470425100145 valid 0.20415089491992117
LOSS train 0.14300470425100145 valid 0.20429200823135799
LOSS train 0.14300470425100145 valid 0.2043047455388324
LOSS train 0.14300470425100145 valid 0.2042078070424415
LOSS train 0.14300470425100145 valid 0.20422880539338883
LOSS train 0.14300470425100145 valid 0.20422931353413992
LOSS train 0.14300470425100145 valid 0.20427951042106432
LOSS train 0.14300470425100145 valid 0.20415618003386518
LOSS train 0.14300470425100145 valid 0.20426765326561966
EPOCH 10:
  batch 1 loss: 0.15104533731937408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12823211401700974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1290769229332606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1275833360850811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13157869279384612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13163852194945017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13171383099896566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.12881207279860973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13029509782791138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12865097299218178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13017572936686603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13298530069490275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1331982331780287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13090690597891808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1308133100469907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1319390027783811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13331383598201416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13428213737077183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1353584861284808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13573830388486385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13642462484893345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13585290211168202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13422663218301276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1341321657722195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1340722408890724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1360545871922603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1365424226279612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13713495193847589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13811306218648778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13765519286195438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13714952478485723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13639407558366656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13644030464418005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13753785894197576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13802684332643236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1393123000032372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1402628723834012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1394386885589675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1384028966228167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13895290549844502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13876851793469452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.138649333268404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13875508117814397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13887793249027294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13864801741308636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1384428935530393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13829306354548068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13888921914622188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1391512111437564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13894205436110496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13874359589581395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13823119450647098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13823780150346035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13846029410207714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1386416723782366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13897484660680806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13924232347492585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1388552811895979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1395187626703311
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14027161933481694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.1407994311608252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14029610409371315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13995927513118775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13995398895349354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14017240599944042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14006416443170924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14035857046273217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1406453638611471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1402961380671764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1399678668805531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13998133510770933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13966063989533317
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13993929775610361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1403405734816113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.1403670382499695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.1403509817625347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.14053567798880787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1407183152742875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14094449692889105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14112156592309474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14111711691927026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14123983953784153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14112345102321672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14141679909967242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14155370926155764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.1411632755987866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14128001727934542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.1412028046324849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14125380196263282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14130295457111464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14127114177732678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14097070297145325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14103957842434606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1410271129868132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14080072500203786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14079653456186256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14076033080976033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1405753537404294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14086479385091802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1408384494483471
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14087214431550243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14081402692724676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.140805678315533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14104101219429419
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.1410008375133787
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14099229028764762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.1409271860791144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14084838109987755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14085025442849605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14109979786656118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1409579052312954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14073467773518392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14060315239218485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14065260165616086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14076167422792185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14091872025666566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1410145580004423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14088455234038627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14102108764047383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14090843436618647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14102011130861014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14110568226849446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14108351866404215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14108815789222717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1412719464302063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14128295923509296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14142803202463886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14142521156463772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14128033643545107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14119437772494095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14119491470224074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14084910590088728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14085601124548375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14066544853484453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14062562099209538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14087442692150087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14097000266948756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14093747486670813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14106895876445358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.14089410422103746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14098900905314912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.1410561057043747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1411149147507194
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14091767510399222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14114567252068683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14122936644986883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.1410301399676978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14137634534288096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.141563376224281
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14176312694946924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14206005001304955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14232086007924458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1425567208941466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14243458124337258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14241395640757776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14248053299692962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14249180447144114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14254784433147574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14254429748973008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14251339752227069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.142555272357064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14273167456741687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14282262517265015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1427438709794021
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1425317546634963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14241345119045443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1424457873412949
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14231568064895414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14223951405498403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14238906638587223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14243003251085504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14234539411615493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14219871886892815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14225577717197352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14232488146850042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14234885497188027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1427087220600096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1427105526241024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14277275697479036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14284544206327862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14282967018488363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14293003941957766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14302180593456726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14293890980922658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14289878196007497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14295018159894532
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1430436913023658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14291210278393107
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1429721155258083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14302607402205467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1429607576802763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14320104122937968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14315422261471575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14315663151366195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.1431421786164626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.143057840933301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14279747951000474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14279555051465226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14265503827950463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14278610803186895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14273183692747088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14269784963367008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14278243672965196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14274263666833148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14262630114468133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14273473276819998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.1425762988159046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14241587546152565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14226364918302692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14238412976264953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.1423776647632156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14256689563955902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.1426586801177459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14263780317573904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1425787813441698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14257718850341108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14256410616608808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14253271138722742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14261167009943695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14240449121729895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1424639145106212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14243570926624374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14254458666248707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1424922672033842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14256654020812776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.1424724305361773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14256475393204962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1424439439321296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14236620654185267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14240551817676295
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.1424065799295128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14244243643920998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14236419280199533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14233721270520464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14224155212336398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1422559341447333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14220222161163257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14232468501735135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14221476610734374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14214897640049456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14210051780419725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14209712419874412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14208216695383252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14205600657179707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.1420579851890097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14190210171831333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14199139382916423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14211747458865565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1421149294658837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1421670008301735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14216656484214432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14215496135136438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14209063139121994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14202883716408662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.1421081093012118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14212067530024797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14202419506088768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14196623933985253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1420153334851891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1420139647160585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14218186530002688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14229606856940358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14216666136064457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14218372233550658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14219512588010644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.1420718349964547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1421008012100552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14216841648874887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14207448973753195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14216328103233267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14210312310400044
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14202787316239932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1419445670076779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14197563714463346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.1418571262467991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1417873300396014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14189584795318355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14182454382451318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14173795456420563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14179172481277158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14169771659098487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.1417287194718283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14166891956603148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14169346087310516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14184908780612443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14186372537608746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1419620621650892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.1418562404707902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14172230294831484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14169566415507218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14169818219245503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14170640860110112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1418598585669905
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14182867013475522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14188885623115605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1419781993671849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1419311759347466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14206625091149502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1420829403759245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14199413982530434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1419359096557991
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14180406900924564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14184226796473606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14188276944485934
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14190386321212425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14187627054410043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14182860739165098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1418328465434251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14193373240988616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14192598901929393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14186248786963068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.1417938312754417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14184608741309315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14188021853281435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14205014790807452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14204333246320108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14206558207420145
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14205770583460167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.142071391160959
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14202371733263136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14202229729694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14214748995644705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14204460926838333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1419849361948393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14208666015129823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14207401736839417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.1420851980281897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14210653025656939
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14216355918298015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14210287617011505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14206315324925944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1421449656138219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1421892017394573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14220608485316089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.1421593038003836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14226058116626172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1422543969193269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14219160644701245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14212199887343213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14216017302344827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1421819980805221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.142145562372361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1421680387446206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14215463694444921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14223956578019736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14218739708724049
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.1422427807658138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14225685861946522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1421545044142742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14219071286065238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14229819467264704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14226606890389865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1422465558430291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1423585434670502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14231015401826777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14225597572879176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14217543858690423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1421194813895825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14212088351213167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14211408787717422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14210138899219993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1421711613418977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.1421578791064336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1421338543909919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14215016416082643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14216427323841008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14208144944272835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14216332295504602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14217845243490163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1422573325601784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14230669605121457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14230018798061597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14225727419591142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1423153992802064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14230452112356823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14230049995982902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14233761377612855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14236465758747524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14246671235970268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14253852320344826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1425299389859823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14254601693746308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14257208695610882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14257494682290903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14250005866800036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14252550548685647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14242611998735472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14246982789223955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1424520341320332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14245524987196312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14252672750321801
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.1425315576548479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14253984808770148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14251372380758906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14259304607970805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.1425488870779071
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14255308068969688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14253800444716785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14255722867964504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14255184326320886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14261580148985856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14269161595040886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14266827658151573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1426462403470927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1426426629961273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14263787685797133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14262247279615894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.142568573538287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.1425942763649747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14258601323860448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14268289278023435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1426696495957745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14257840925859191
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14248902894160598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1424981105040355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14255091519309923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14244927833977936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14239967443298496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14241381619027804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14237644707872754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14240129405274243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14247583816825496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14241842386570383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.1424625592552266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1424748738373027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1424204390933256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1424000335800564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14243538729916108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1423989994264705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1424271590141363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14241004636697038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14250652416160814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14251916643928986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1425595679728117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14256359003056054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14261994770634065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1426497294441786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14269686952981775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14269412599147588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14272210503166372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.1427337942885704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14272334028827657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14275175995670647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14272881920138994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1426660347688064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14263521925015835
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1425999925873957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1425959242541077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14259072452916335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14254443544480536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1425539763797149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14251064898165983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14247791163179258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14241062085008832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1423687628173566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14239835606789902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.1424220375700383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14239570239810964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1423156030784505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14240137460115163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14242485849239303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1423906876959584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14239128605014048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1424735801069644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1424385002383622
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.142473765628916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14251511684385562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.1425499333243849
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.142575192378401
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14250467457669846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14255850570723255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1425463236173836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1425463236173836 valid 0.1943356990814209
LOSS train 0.1425463236173836 valid 0.21602042019367218
LOSS train 0.1425463236173836 valid 0.22479575872421265
LOSS train 0.1425463236173836 valid 0.22403714433312416
LOSS train 0.1425463236173836 valid 0.22422331869602202
LOSS train 0.1425463236173836 valid 0.23052153239647546
LOSS train 0.1425463236173836 valid 0.22479262948036194
LOSS train 0.1425463236173836 valid 0.22514010593295097
LOSS train 0.1425463236173836 valid 0.2245560901032554
LOSS train 0.1425463236173836 valid 0.22570901960134507
LOSS train 0.1425463236173836 valid 0.22169909287582745
LOSS train 0.1425463236173836 valid 0.22464473421374956
LOSS train 0.1425463236173836 valid 0.22324155844174898
LOSS train 0.1425463236173836 valid 0.2234262994357518
LOSS train 0.1425463236173836 valid 0.22197166879971822
LOSS train 0.1425463236173836 valid 0.22224067896604538
LOSS train 0.1425463236173836 valid 0.22313054344233343
LOSS train 0.1425463236173836 valid 0.2241160488790936
LOSS train 0.1425463236173836 valid 0.22592984532055102
LOSS train 0.1425463236173836 valid 0.22471954375505448
LOSS train 0.1425463236173836 valid 0.22474980851014456
LOSS train 0.1425463236173836 valid 0.22393057563088156
LOSS train 0.1425463236173836 valid 0.22155322652796042
LOSS train 0.1425463236173836 valid 0.22227977712949118
LOSS train 0.1425463236173836 valid 0.22109328627586364
LOSS train 0.1425463236173836 valid 0.22039973735809326
LOSS train 0.1425463236173836 valid 0.2201490021414227
LOSS train 0.1425463236173836 valid 0.22008661340389932
LOSS train 0.1425463236173836 valid 0.21874006914681401
LOSS train 0.1425463236173836 valid 0.21772125214338303
LOSS train 0.1425463236173836 valid 0.21731860839551495
LOSS train 0.1425463236173836 valid 0.2180929589085281
LOSS train 0.1425463236173836 valid 0.2170268649404699
LOSS train 0.1425463236173836 valid 0.2165514112395399
LOSS train 0.1425463236173836 valid 0.2174500346183777
LOSS train 0.1425463236173836 valid 0.21768664113349384
LOSS train 0.1425463236173836 valid 0.21683836063823184
LOSS train 0.1425463236173836 valid 0.21661987861520365
LOSS train 0.1425463236173836 valid 0.21637090467489684
LOSS train 0.1425463236173836 valid 0.21587448865175246
LOSS train 0.1425463236173836 valid 0.2157735493851871
LOSS train 0.1425463236173836 valid 0.21719820620048613
LOSS train 0.1425463236173836 valid 0.21793983946012896
LOSS train 0.1425463236173836 valid 0.2169369100169702
LOSS train 0.1425463236173836 valid 0.2164577586783303
LOSS train 0.1425463236173836 valid 0.21492062832998193
LOSS train 0.1425463236173836 valid 0.21436566431471643
LOSS train 0.1425463236173836 valid 0.21591697447001934
LOSS train 0.1425463236173836 valid 0.21550085410779837
LOSS train 0.1425463236173836 valid 0.2158203399181366
LOSS train 0.1425463236173836 valid 0.21574219243199216
LOSS train 0.1425463236173836 valid 0.21559964693509615
LOSS train 0.1425463236173836 valid 0.21656209905192536
LOSS train 0.1425463236173836 valid 0.21650761917785363
LOSS train 0.1425463236173836 valid 0.2167283990166404
LOSS train 0.1425463236173836 valid 0.21625267235296114
LOSS train 0.1425463236173836 valid 0.215505619582377
LOSS train 0.1425463236173836 valid 0.21634334351482062
LOSS train 0.1425463236173836 valid 0.21637812433606488
LOSS train 0.1425463236173836 valid 0.2160187525053819
LOSS train 0.1425463236173836 valid 0.21591396727522866
LOSS train 0.1425463236173836 valid 0.21548561391330534
LOSS train 0.1425463236173836 valid 0.21561729151105125
LOSS train 0.1425463236173836 valid 0.2154277046211064
LOSS train 0.1425463236173836 valid 0.2145664646075322
LOSS train 0.1425463236173836 valid 0.2143766148523851
LOSS train 0.1425463236173836 valid 0.21488673562434182
LOSS train 0.1425463236173836 valid 0.21429483723991058
LOSS train 0.1425463236173836 valid 0.21469879258370053
LOSS train 0.1425463236173836 valid 0.21519465403897423
LOSS train 0.1425463236173836 valid 0.21530212944662067
LOSS train 0.1425463236173836 valid 0.21573458943102095
LOSS train 0.1425463236173836 valid 0.21605166547918972
LOSS train 0.1425463236173836 valid 0.21602115196150704
LOSS train 0.1425463236173836 valid 0.21578661243120828
LOSS train 0.1425463236173836 valid 0.21580488372005915
LOSS train 0.1425463236173836 valid 0.21609390691503302
LOSS train 0.1425463236173836 valid 0.21599919329851103
LOSS train 0.1425463236173836 valid 0.2162801959092104
LOSS train 0.1425463236173836 valid 0.21584171075373887
LOSS train 0.1425463236173836 valid 0.21600647748988352
LOSS train 0.1425463236173836 valid 0.2161220379355477
LOSS train 0.1425463236173836 valid 0.21587598700839353
LOSS train 0.1425463236173836 valid 0.21586425087991215
LOSS train 0.1425463236173836 valid 0.21607524489655214
LOSS train 0.1425463236173836 valid 0.21609312521163807
LOSS train 0.1425463236173836 valid 0.21576171942140865
LOSS train 0.1425463236173836 valid 0.21566861105913465
LOSS train 0.1425463236173836 valid 0.21577196861251016
LOSS train 0.1425463236173836 valid 0.215621104505327
LOSS train 0.1425463236173836 valid 0.21571363443201716
LOSS train 0.1425463236173836 valid 0.21572314332360806
LOSS train 0.1425463236173836 valid 0.21595825206848882
LOSS train 0.1425463236173836 valid 0.2161717788970217
LOSS train 0.1425463236173836 valid 0.21646104549106798
LOSS train 0.1425463236173836 valid 0.21733024902641773
LOSS train 0.1425463236173836 valid 0.21744114805742637
LOSS train 0.1425463236173836 valid 0.21804587695063377
LOSS train 0.1425463236173836 valid 0.2179415397571795
LOSS train 0.1425463236173836 valid 0.21827023953199387
LOSS train 0.1425463236173836 valid 0.21831368898401166
LOSS train 0.1425463236173836 valid 0.21876163283983865
LOSS train 0.1425463236173836 valid 0.21834433845524648
LOSS train 0.1425463236173836 valid 0.21812665233245263
LOSS train 0.1425463236173836 valid 0.21853455078034173
LOSS train 0.1425463236173836 valid 0.21847664874117328
LOSS train 0.1425463236173836 valid 0.21828081312580644
LOSS train 0.1425463236173836 valid 0.21829484513512365
LOSS train 0.1425463236173836 valid 0.2177654511611396
LOSS train 0.1425463236173836 valid 0.2175149834968827
LOSS train 0.1425463236173836 valid 0.21746309061308164
LOSS train 0.1425463236173836 valid 0.21761071096573556
LOSS train 0.1425463236173836 valid 0.2174582171492872
LOSS train 0.1425463236173836 valid 0.2175932445547037
LOSS train 0.1425463236173836 valid 0.2180736181528672
LOSS train 0.1425463236173836 valid 0.21797625617734318
LOSS train 0.1425463236173836 valid 0.21795720460577908
LOSS train 0.1425463236173836 valid 0.21804704630779007
LOSS train 0.1425463236173836 valid 0.21779161917061365
LOSS train 0.1425463236173836 valid 0.217720831806461
LOSS train 0.1425463236173836 valid 0.2178049567809775
LOSS train 0.1425463236173836 valid 0.21778662246270258
LOSS train 0.1425463236173836 valid 0.21768420450086515
LOSS train 0.1425463236173836 valid 0.21770439933865302
LOSS train 0.1425463236173836 valid 0.21780562472343445
LOSS train 0.1425463236173836 valid 0.2175715255831915
LOSS train 0.1425463236173836 valid 0.21742488314786296
LOSS train 0.1425463236173836 valid 0.21740715124178678
LOSS train 0.1425463236173836 valid 0.21724481846010962
LOSS train 0.1425463236173836 valid 0.21696116236539986
LOSS train 0.1425463236173836 valid 0.21699919932671174
LOSS train 0.1425463236173836 valid 0.21720246304616783
LOSS train 0.1425463236173836 valid 0.2171311829995392
LOSS train 0.1425463236173836 valid 0.21724030041872566
LOSS train 0.1425463236173836 valid 0.21730216101363853
LOSS train 0.1425463236173836 valid 0.21748335769071298
LOSS train 0.1425463236173836 valid 0.21729320070169267
LOSS train 0.1425463236173836 valid 0.2169911821467289
LOSS train 0.1425463236173836 valid 0.21695441961717263
LOSS train 0.1425463236173836 valid 0.2168262447629656
LOSS train 0.1425463236173836 valid 0.2170517658088224
LOSS train 0.1425463236173836 valid 0.2172786604980348
LOSS train 0.1425463236173836 valid 0.21738894763109567
LOSS train 0.1425463236173836 valid 0.21737101456771293
LOSS train 0.1425463236173836 valid 0.21715884301169167
LOSS train 0.1425463236173836 valid 0.21719647147884108
LOSS train 0.1425463236173836 valid 0.2172203661024976
LOSS train 0.1425463236173836 valid 0.2170057046050961
LOSS train 0.1425463236173836 valid 0.2171806510062826
LOSS train 0.1425463236173836 valid 0.21717576583226522
LOSS train 0.1425463236173836 valid 0.21705129742622375
LOSS train 0.1425463236173836 valid 0.21704956889152527
LOSS train 0.1425463236173836 valid 0.21679475075668758
LOSS train 0.1425463236173836 valid 0.2168344763385785
LOSS train 0.1425463236173836 valid 0.21675309413863766
LOSS train 0.1425463236173836 valid 0.21690059816225982
LOSS train 0.1425463236173836 valid 0.21669944067289876
LOSS train 0.1425463236173836 valid 0.2165805771946907
LOSS train 0.1425463236173836 valid 0.2169486401403475
LOSS train 0.1425463236173836 valid 0.21697237780317663
LOSS train 0.1425463236173836 valid 0.2171431416125031
LOSS train 0.1425463236173836 valid 0.21709428203326683
LOSS train 0.1425463236173836 valid 0.21682712161467851
LOSS train 0.1425463236173836 valid 0.21687521694636927
LOSS train 0.1425463236173836 valid 0.21707394475286657
LOSS train 0.1425463236173836 valid 0.2169749181134155
LOSS train 0.1425463236173836 valid 0.2169847088659595
LOSS train 0.1425463236173836 valid 0.21668744850016775
LOSS train 0.1425463236173836 valid 0.2165406199072945
LOSS train 0.1425463236173836 valid 0.21636883406078114
LOSS train 0.1425463236173836 valid 0.2162143361498738
LOSS train 0.1425463236173836 valid 0.21631343602094538
LOSS train 0.1425463236173836 valid 0.21636159407954686
LOSS train 0.1425463236173836 valid 0.2163848824713422
LOSS train 0.1425463236173836 valid 0.21607704724584306
LOSS train 0.1425463236173836 valid 0.2160120146687735
LOSS train 0.1425463236173836 valid 0.21590742477252658
LOSS train 0.1425463236173836 valid 0.21590616101964136
LOSS train 0.1425463236173836 valid 0.2159948960029879
LOSS train 0.1425463236173836 valid 0.21598420391480128
LOSS train 0.1425463236173836 valid 0.21589671445814945
LOSS train 0.1425463236173836 valid 0.21578048701797212
LOSS train 0.1425463236173836 valid 0.21586549298359398
LOSS train 0.1425463236173836 valid 0.21591482279093369
LOSS train 0.1425463236173836 valid 0.21603824963440765
LOSS train 0.1425463236173836 valid 0.2162398059842407
LOSS train 0.1425463236173836 valid 0.21619015374285652
LOSS train 0.1425463236173836 valid 0.2161749059215505
LOSS train 0.1425463236173836 valid 0.21589282777889696
LOSS train 0.1425463236173836 valid 0.21586460149601885
LOSS train 0.1425463236173836 valid 0.21597897218472045
LOSS train 0.1425463236173836 valid 0.21601739285203317
LOSS train 0.1425463236173836 valid 0.21581244955099926
LOSS train 0.1425463236173836 valid 0.21588898765057632
LOSS train 0.1425463236173836 valid 0.21578730497604762
LOSS train 0.1425463236173836 valid 0.21600636885482438
LOSS train 0.1425463236173836 valid 0.21611044143662234
LOSS train 0.1425463236173836 valid 0.2162187100961955
LOSS train 0.1425463236173836 valid 0.2163067451074495
LOSS train 0.1425463236173836 valid 0.2165186533331871
LOSS train 0.1425463236173836 valid 0.21657591175973712
LOSS train 0.1425463236173836 valid 0.216747414726432
LOSS train 0.1425463236173836 valid 0.21684742751967143
LOSS train 0.1425463236173836 valid 0.21661749591722207
LOSS train 0.1425463236173836 valid 0.21643613824030247
LOSS train 0.1425463236173836 valid 0.21641781492140685
LOSS train 0.1425463236173836 valid 0.21623241289514275
LOSS train 0.1425463236173836 valid 0.21606489543158275
LOSS train 0.1425463236173836 valid 0.21610080889252384
LOSS train 0.1425463236173836 valid 0.21633839940740948
LOSS train 0.1425463236173836 valid 0.21656121242950313
LOSS train 0.1425463236173836 valid 0.2164747621512638
LOSS train 0.1425463236173836 valid 0.21642548692338343
LOSS train 0.1425463236173836 valid 0.21635836119128166
LOSS train 0.1425463236173836 valid 0.21608798226644826
LOSS train 0.1425463236173836 valid 0.21589121022434146
LOSS train 0.1425463236173836 valid 0.21592019855426753
LOSS train 0.1425463236173836 valid 0.21588009575364786
LOSS train 0.1425463236173836 valid 0.2159514104666775
LOSS train 0.1425463236173836 valid 0.2158870511434295
LOSS train 0.1425463236173836 valid 0.21587671494591829
LOSS train 0.1425463236173836 valid 0.21599626541137695
LOSS train 0.1425463236173836 valid 0.2159039411576874
LOSS train 0.1425463236173836 valid 0.21575045678764582
LOSS train 0.1425463236173836 valid 0.21547129842970106
LOSS train 0.1425463236173836 valid 0.215271134885539
LOSS train 0.1425463236173836 valid 0.21530485868716556
LOSS train 0.1425463236173836 valid 0.21534997584265575
LOSS train 0.1425463236173836 valid 0.21528893598562765
LOSS train 0.1425463236173836 valid 0.21542894198842671
LOSS train 0.1425463236173836 valid 0.21547112630043194
LOSS train 0.1425463236173836 valid 0.2156413048505783
LOSS train 0.1425463236173836 valid 0.21576970150542363
LOSS train 0.1425463236173836 valid 0.21574023525174865
LOSS train 0.1425463236173836 valid 0.21566038937010665
LOSS train 0.1425463236173836 valid 0.21547651802331713
LOSS train 0.1425463236173836 valid 0.21553185997129995
LOSS train 0.1425463236173836 valid 0.21553607848512024
LOSS train 0.1425463236173836 valid 0.21554746824827153
LOSS train 0.1425463236173836 valid 0.21515859396507342
LOSS train 0.1425463236173836 valid 0.21502860675708882
LOSS train 0.1425463236173836 valid 0.2151399664272947
LOSS train 0.1425463236173836 valid 0.21521604275752487
LOSS train 0.1425463236173836 valid 0.21528313473844138
LOSS train 0.1425463236173836 valid 0.21545476165353036
LOSS train 0.1425463236173836 valid 0.21553196376416742
LOSS train 0.1425463236173836 valid 0.21552551468374276
LOSS train 0.1425463236173836 valid 0.21547277181619598
LOSS train 0.1425463236173836 valid 0.21538215522545887
LOSS train 0.1425463236173836 valid 0.21551218682527543
LOSS train 0.1425463236173836 valid 0.2155979834348557
LOSS train 0.1425463236173836 valid 0.2156523476753916
LOSS train 0.1425463236173836 valid 0.2155731041794238
LOSS train 0.1425463236173836 valid 0.21562897273170667
LOSS train 0.1425463236173836 valid 0.21563069206826826
LOSS train 0.1425463236173836 valid 0.21565695491153747
LOSS train 0.1425463236173836 valid 0.21556503775054842
LOSS train 0.1425463236173836 valid 0.21564053836487984
LOSS train 0.1425463236173836 valid 0.21559460397852895
LOSS train 0.1425463236173836 valid 0.2154598166163151
LOSS train 0.1425463236173836 valid 0.21541931934055242
LOSS train 0.1425463236173836 valid 0.21571797342928312
LOSS train 0.1425463236173836 valid 0.2155871258715713
LOSS train 0.1425463236173836 valid 0.2156234344636852
LOSS train 0.1425463236173836 valid 0.21572864325541372
LOSS train 0.1425463236173836 valid 0.2157824048422333
LOSS train 0.1425463236173836 valid 0.21574428577101631
LOSS train 0.1425463236173836 valid 0.21579211872460238
LOSS train 0.1425463236173836 valid 0.2158504702344703
LOSS train 0.1425463236173836 valid 0.21570036328501171
LOSS train 0.1425463236173836 valid 0.21558745518821631
LOSS train 0.1425463236173836 valid 0.21540910476709113
LOSS train 0.1425463236173836 valid 0.21528225793288305
LOSS train 0.1425463236173836 valid 0.21523575169326614
LOSS train 0.1425463236173836 valid 0.2153113515810533
LOSS train 0.1425463236173836 valid 0.21541501700446225
LOSS train 0.1425463236173836 valid 0.21535590850489233
LOSS train 0.1425463236173836 valid 0.2153595779761136
LOSS train 0.1425463236173836 valid 0.21541567034618828
LOSS train 0.1425463236173836 valid 0.21528592152254922
LOSS train 0.1425463236173836 valid 0.215144493507745
LOSS train 0.1425463236173836 valid 0.2151208613992583
LOSS train 0.1425463236173836 valid 0.21511233374213162
LOSS train 0.1425463236173836 valid 0.21493092609543196
LOSS train 0.1425463236173836 valid 0.2150543585158231
LOSS train 0.1425463236173836 valid 0.21504715767565308
LOSS train 0.1425463236173836 valid 0.2151994839674089
LOSS train 0.1425463236173836 valid 0.21514130104333162
LOSS train 0.1425463236173836 valid 0.21510261616904844
LOSS train 0.1425463236173836 valid 0.21501923141808346
LOSS train 0.1425463236173836 valid 0.2149696202110179
LOSS train 0.1425463236173836 valid 0.21492904674721092
LOSS train 0.1425463236173836 valid 0.21487390689882402
LOSS train 0.1425463236173836 valid 0.21486741258781783
LOSS train 0.1425463236173836 valid 0.21487497566109998
LOSS train 0.1425463236173836 valid 0.2148964756445305
LOSS train 0.1425463236173836 valid 0.21489770108401174
LOSS train 0.1425463236173836 valid 0.21490813286712507
LOSS train 0.1425463236173836 valid 0.2149242392451468
LOSS train 0.1425463236173836 valid 0.21494057963291804
LOSS train 0.1425463236173836 valid 0.21489918172557487
LOSS train 0.1425463236173836 valid 0.21492020627915465
LOSS train 0.1425463236173836 valid 0.21497687003990212
LOSS train 0.1425463236173836 valid 0.21495665457883947
LOSS train 0.1425463236173836 valid 0.21483952544751714
LOSS train 0.1425463236173836 valid 0.21497216718453988
LOSS train 0.1425463236173836 valid 0.21486336502461945
LOSS train 0.1425463236173836 valid 0.2149546483120361
LOSS train 0.1425463236173836 valid 0.21501017229263836
LOSS train 0.1425463236173836 valid 0.21503549222984622
LOSS train 0.1425463236173836 valid 0.21509142564998945
LOSS train 0.1425463236173836 valid 0.21504995288948217
LOSS train 0.1425463236173836 valid 0.21502284291453255
LOSS train 0.1425463236173836 valid 0.214993291172632
LOSS train 0.1425463236173836 valid 0.21501471182656667
LOSS train 0.1425463236173836 valid 0.2150040963971162
LOSS train 0.1425463236173836 valid 0.21497007160908793
LOSS train 0.1425463236173836 valid 0.21503282268092316
LOSS train 0.1425463236173836 valid 0.21503648582297058
LOSS train 0.1425463236173836 valid 0.21493109730072318
LOSS train 0.1425463236173836 valid 0.21492101349563242
LOSS train 0.1425463236173836 valid 0.2148423877006732
LOSS train 0.1425463236173836 valid 0.21476873238758407
LOSS train 0.1425463236173836 valid 0.2147689183001165
LOSS train 0.1425463236173836 valid 0.2145788464637903
LOSS train 0.1425463236173836 valid 0.21464874691392746
LOSS train 0.1425463236173836 valid 0.21451814837229727
LOSS train 0.1425463236173836 valid 0.21459248434843087
LOSS train 0.1425463236173836 valid 0.21466055775123527
LOSS train 0.1425463236173836 valid 0.21476514705202795
LOSS train 0.1425463236173836 valid 0.2146722677521115
LOSS train 0.1425463236173836 valid 0.21464627105967107
LOSS train 0.1425463236173836 valid 0.21467826727990275
LOSS train 0.1425463236173836 valid 0.21459425686897632
LOSS train 0.1425463236173836 valid 0.21444775893617032
LOSS train 0.1425463236173836 valid 0.2143876516659345
LOSS train 0.1425463236173836 valid 0.2145570433015993
LOSS train 0.1425463236173836 valid 0.21448091302926725
LOSS train 0.1425463236173836 valid 0.21439812171951508
LOSS train 0.1425463236173836 valid 0.21445297791677362
LOSS train 0.1425463236173836 valid 0.21448789038790986
LOSS train 0.1425463236173836 valid 0.2144981067605883
LOSS train 0.1425463236173836 valid 0.21435218189270086
LOSS train 0.1425463236173836 valid 0.214417532432911
LOSS train 0.1425463236173836 valid 0.21450874887514806
LOSS train 0.1425463236173836 valid 0.2144217619489383
LOSS train 0.1425463236173836 valid 0.21439536152037145
LOSS train 0.1425463236173836 valid 0.21435943507086272
LOSS train 0.1425463236173836 valid 0.21433601385372075
LOSS train 0.1425463236173836 valid 0.21436537363699504
LOSS train 0.1425463236173836 valid 0.21442294634475328
LOSS train 0.1425463236173836 valid 0.21444891160354018
LOSS train 0.1425463236173836 valid 0.21450645440698682
LOSS train 0.1425463236173836 valid 0.21439690260732241
LOSS train 0.1425463236173836 valid 0.21426804926193935
LOSS train 0.1425463236173836 valid 0.21423982946055659
LOSS train 0.1425463236173836 valid 0.21425288585888572
LOSS train 0.1425463236173836 valid 0.21434276883995068
LOSS train 0.1425463236173836 valid 0.21433624753878974
LOSS train 0.1425463236173836 valid 0.21456315173870988
LOSS train 0.1425463236173836 valid 0.21450421507173628
LOSS train 0.1425463236173836 valid 0.21461028136436452
LOSS train 0.1425463236173836 valid 0.2146418276076146
LOSS train 0.1425463236173836 valid 0.21452190387216244
LOSS train 0.1425463236173836 valid 0.2145410672850805
LOSS train 0.1425463236173836 valid 0.21454079705672185
LOSS train 0.1425463236173836 valid 0.21456749002355321
LOSS train 0.1425463236173836 valid 0.21445516958508803
LOSS train 0.1425463236173836 valid 0.21456819263900198
EPOCH 11:
  batch 1 loss: 0.15621599555015564
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1341998316347599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13652568807204565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1360065396875143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.14018226116895677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13950798784693083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13891154208353587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.13658726029098034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1379997283220291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.13618870228528976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13812035186724228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.14000067487359047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.14054352503556472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13833808792488916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13787358303864797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1394212618470192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.140252848758417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.1413584061794811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.1429408710253866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.1429735891520977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.1438780426979065
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.14294611391696063
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1409658455978269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.14092324332644543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.14086299568414687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1428257774275083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.14352647987780748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.14404587633907795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1450762252869277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.14493328457077345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.14455820019206694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.1437864548061043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.14379167353565042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.1447409941431354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.145375157892704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1465776034941276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.14756679273134954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1467094723331301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.14572209119796753
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.14625025168061256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1461099698776152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.1460399152267547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.14601632467536038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.14619039744138718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.14590338203642103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.1456359057970669
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.14564624492158282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.14622754541536173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.14622351253519253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.14608097553253174
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.14582338987612256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.14520783077638882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.14523261602757112
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.14530241420423542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.14557760777798565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.1457062649673649
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.14591745836170097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.14566389689671583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.1461313984404176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14683095080157121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14750474459323726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14711411381440778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1465756520628929
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14654464682098478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14657433984371332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.14646553711006136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1467897766339245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.14690500180072644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.14662208559288495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1463864800121103
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.1464722480782321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.14603979947666326
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1463694258095467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.14686276904634527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.14677943646907807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.14676781351629056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1468645783213826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14706896188167426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14708540805532963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14716743528842927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14709522050839882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.1472031181541885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14708153275122127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14746026996345746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14749548733234405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14707292798300123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14712258034396444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14693402507427064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14704894592587867
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.14711502715945243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14704609220172024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.1467880234932122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14688053238455967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14696585822929728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14669172591284702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14667712695275745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14651338356671875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.14628604312940519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14664123034236407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.14669243186712266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.14672935156538935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14667590137790232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1466786446212565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14695917399456868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14692464442480177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14695710492021632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14691255805648376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14674349142997353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.1467533785542217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14695418585430492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.1467659685525808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.14654821357024567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.1464133889010522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1464207299184381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14652786941631982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14658247923542714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.1466840039461087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1465150447467626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14668002787257442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14657239317893983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14670582201855242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14672898830937559
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1466897180894526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14666205644607544
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1468088927268982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.1468597324121566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14707650607965123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1470183664932847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14685773907243743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14674951239274098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14663896260370735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14630277522585608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14625689387321472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1460394563514795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1459428236440376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14617983942084453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14619864914974157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14607054923755536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14619314166710531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1459778986339058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1460409291775514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14609005590769605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14612411884786367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14587260084226727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.146084975528306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14611657224085234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14589227685312026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.14605746025571953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1462836452578538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14621285657087962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14630797179724206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14639216132069888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1464837550922157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1463846948433232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14628845549398853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1462402768815175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.1461992683304343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14625182945894288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14627992713226462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14618808198720218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1461247626854026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14627987909832119
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14635973949373865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14626332317910543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1460431226275184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1458885314234768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1459201272733197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14577567426576502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14565814370234337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14571481662638047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14573234284830372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14563831912223682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14550191099416313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.14558131586032352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14561269227947507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14565753272141924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14598101735283425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.145930729764566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14610433482757493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14620524466865592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1462157232409024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1462788267859391
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1463273561310247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14625598858717992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14620283475598772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14624853836753035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1463861881810076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14628379748381198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1463190615571365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14630849043789662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.1462458715778995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14647750606915602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14642580004090472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14642760146063627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14640032010964857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.1463207518491818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14606566859532127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14603343474292996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14588651218306478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14598134361207485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14592510306123477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14588866700040232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14596199358038128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14595781018336615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.14585345492857257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14593463720193187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14578779790424495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1456448477692902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14548764666586972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14560781518618265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14562530391871648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14582756351468698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14590777344826802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1458643227119312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14579256835371948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14578039123228304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14576494844827784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.1457478790244925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14585487000201935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14562402794306928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.1456717827201429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.1456623578125292
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14573242839409098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14568612458450453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.1457670990626017
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14568521847239638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1457910090028452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1456578843164862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14554802019262938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14556137848159542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.145566458761434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14556150427409287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.145463588155902
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.1454450015583609
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14533319010379467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14534884558643324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1452697592822811
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14537090369883707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1452412368986896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1451978027820587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14514270938036353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14515266943076424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14513048731986386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14508704077757772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14505897681323848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.1448806183246093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14489317006669064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14500973511847756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14502104542341576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14507836639881133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14507493471719354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1450417142302271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14494802266830512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14487223770088098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14490730356352002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1449170554697048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.1447942441307617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14471858127634654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14476170698648255
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14477602048562124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14496552201979918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14503430614944632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1448801738907629
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14491402831944553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14494439318494975
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14481404748626223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14483408585246582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1448955360188413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14479240057747603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14487698147142375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14480567487073562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.1447185519260957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1446239253703928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.1446826155331448
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14454764555801045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.14447065558878408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14455259069531404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.14445649039187877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1443720783063588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.14442868599934236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14435389447042526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14436456270463077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14431858104867565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14434105488406102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.14449307066306732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.1444884883044483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1445689214335086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14449455741689438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14436186659294842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14433712219369824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.1443082780362814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.1443298667771359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14446273985169447
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14442069742347108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1444814686552953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14457417808070377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.1445038836913478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14463520835110005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14465248709338963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1445689401527246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14450316168244098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1443482122772577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14438520981730407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14440429068513608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14441640508956596
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14436625966838762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14428286346628147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1442763389801824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14436525415062518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14433696318057276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14426640351677247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14419625028490257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.14426289853482202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14427503447528858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14442642300374925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14443463009275212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14446942106312383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14444928735775767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14446954487539757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.144427328533493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14441068045725333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14455077568103808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1444386422357323
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.14436921665881886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1444540767486279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14444803592800362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14446907725173763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14446407751884402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14452212469193712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14445857656272976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14441682085439878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14447289370898023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14450664672855143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14451757646695582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14447725657651672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14454072321365988
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1445447007299177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1444887967460607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14441788007551942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14442075482186148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14444008775470543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14438641385028236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14439610615589876
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.1443733073511096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14444415102834288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14440037008654866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14445634903413074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14445331028994474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1443442191853906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.1443711957548346
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14446441415283415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14444819344630974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14443386181471368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14452541343825684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14446963995275364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14445056079813604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14437722880132392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14431171431388268
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14428937567973868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1442873116582632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14427513082271798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14437382112550473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14435701568921408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1443365935932149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.1443596410424742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1443610786170256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1442825309221686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1443658449849033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14437918774884567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1444635188458739
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14450957292736058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.1444832564642032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1444281861385775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14449425428149534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14447793273131052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14447521561003746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14449821475804325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14453262922467378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14464222303638358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.1446998806768342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.144692470512678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1446767071388779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1446913921957539
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.1447111899809291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.1446323042179083
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14464797635461382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14454103224653297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1446061119744458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1445731513757632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14458596236430682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14466027080860283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14467225124945446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14465895833556586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14461240705651074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1446905257203911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14463287877916087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14461943484673873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14460272525423137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.144608221556011
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14459941294044257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14466799836503597
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14473242068023823
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1447061983586541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14468180152154206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14468776885374093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14466968806359568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14464018920422772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14458373561501503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14460299555713216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1445892012337359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14467570947034517
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14465982184826748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14455571321468377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1444494895781008
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14443924094898156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14447939695002368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14436740331726966
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14431396518882952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14432137514640153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.1442742762288877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1442936538694307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14436814593265973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14432404817348393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14436126568899402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14436543073724298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14429827064205783
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14428265834231566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.1443403834111501
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.1442992179925903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14432939466002376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14430226508329197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.1444095268121196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.1444136897665524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14447291324146883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.1444751519752645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14453852407360843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14456433727825807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14461667697927724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1446213115490379
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.1446494862267917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14464098929750677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.1446178670618599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14465358565744374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1446035714098462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14454591506987474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14450659627938484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14446930205595307
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.144472679638836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14447447443526146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14442238537801635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14442188894074667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14437416293122599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14434080304491598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14424121328978287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1441431359930353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.1441387926930921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14412433495928476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14405708625001679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14397012994245248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14401311516437842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14402389279091954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.1439789676982345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14395632961720678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14402923824522515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14397389683672177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14398621256131472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14403963130470007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14403868657656205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.1440426542687772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14394798675116072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.1439876786083173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.1439436712475904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.1439436712475904 valid 0.1765635758638382
LOSS train 0.1439436712475904 valid 0.2014542520046234
LOSS train 0.1439436712475904 valid 0.2136121243238449
LOSS train 0.1439436712475904 valid 0.2109345979988575
LOSS train 0.1439436712475904 valid 0.21061261594295502
LOSS train 0.1439436712475904 valid 0.21686911086241403
LOSS train 0.1439436712475904 valid 0.21140943893364497
LOSS train 0.1439436712475904 valid 0.21138706430792809
LOSS train 0.1439436712475904 valid 0.2113659315639072
LOSS train 0.1439436712475904 valid 0.2122833475470543
LOSS train 0.1439436712475904 valid 0.20863120799714868
LOSS train 0.1439436712475904 valid 0.21082162981232008
LOSS train 0.1439436712475904 valid 0.2095142832169166
LOSS train 0.1439436712475904 valid 0.2101114849959101
LOSS train 0.1439436712475904 valid 0.20886788467566172
LOSS train 0.1439436712475904 valid 0.2090125223621726
LOSS train 0.1439436712475904 valid 0.20948629607172572
LOSS train 0.1439436712475904 valid 0.2095734534992112
LOSS train 0.1439436712475904 valid 0.21163844983828695
LOSS train 0.1439436712475904 valid 0.21095979139208793
LOSS train 0.1439436712475904 valid 0.21139253959769294
LOSS train 0.1439436712475904 valid 0.20992805131457068
LOSS train 0.1439436712475904 valid 0.20815700163011966
LOSS train 0.1439436712475904 valid 0.2088808293143908
LOSS train 0.1439436712475904 valid 0.2075526821613312
LOSS train 0.1439436712475904 valid 0.2070821036513035
LOSS train 0.1439436712475904 valid 0.2066401348069862
LOSS train 0.1439436712475904 valid 0.20675926868404662
LOSS train 0.1439436712475904 valid 0.2055201206741662
LOSS train 0.1439436712475904 valid 0.2046436334649722
LOSS train 0.1439436712475904 valid 0.20387122611845693
LOSS train 0.1439436712475904 valid 0.20465791504830122
LOSS train 0.1439436712475904 valid 0.20336494165839572
LOSS train 0.1439436712475904 valid 0.20278172107303843
LOSS train 0.1439436712475904 valid 0.2032833265406745
LOSS train 0.1439436712475904 valid 0.20349053583211368
LOSS train 0.1439436712475904 valid 0.202470502740628
LOSS train 0.1439436712475904 valid 0.2024180104857997
LOSS train 0.1439436712475904 valid 0.20200299796385643
LOSS train 0.1439436712475904 valid 0.20162183679640294
LOSS train 0.1439436712475904 valid 0.20156649445615163
LOSS train 0.1439436712475904 valid 0.20278130578143255
LOSS train 0.1439436712475904 valid 0.20361967523430669
LOSS train 0.1439436712475904 valid 0.2026728821749037
LOSS train 0.1439436712475904 valid 0.2020959327618281
LOSS train 0.1439436712475904 valid 0.20047114724698273
LOSS train 0.1439436712475904 valid 0.1997552881849573
LOSS train 0.1439436712475904 valid 0.20132596977055073
LOSS train 0.1439436712475904 valid 0.20118125330428688
LOSS train 0.1439436712475904 valid 0.20151211231946944
LOSS train 0.1439436712475904 valid 0.20133494483489617
LOSS train 0.1439436712475904 valid 0.20111352807054153
LOSS train 0.1439436712475904 valid 0.2020228198676739
LOSS train 0.1439436712475904 valid 0.20192157780682599
LOSS train 0.1439436712475904 valid 0.20215998589992523
LOSS train 0.1439436712475904 valid 0.20181284871484553
LOSS train 0.1439436712475904 valid 0.20113591248529
LOSS train 0.1439436712475904 valid 0.20189717000928417
LOSS train 0.1439436712475904 valid 0.20203988506632337
LOSS train 0.1439436712475904 valid 0.20167941749095916
LOSS train 0.1439436712475904 valid 0.20143355673453847
LOSS train 0.1439436712475904 valid 0.2010509285234636
LOSS train 0.1439436712475904 valid 0.20112448410382347
LOSS train 0.1439436712475904 valid 0.20096366596408188
LOSS train 0.1439436712475904 valid 0.2002383271088967
LOSS train 0.1439436712475904 valid 0.2001345060539968
LOSS train 0.1439436712475904 valid 0.20070174002825325
LOSS train 0.1439436712475904 valid 0.2000188634676092
LOSS train 0.1439436712475904 valid 0.20041544001171555
LOSS train 0.1439436712475904 valid 0.20087749361991883
LOSS train 0.1439436712475904 valid 0.20098920964019398
LOSS train 0.1439436712475904 valid 0.2014688052650955
LOSS train 0.1439436712475904 valid 0.20175173123405404
LOSS train 0.1439436712475904 valid 0.20176711779188466
LOSS train 0.1439436712475904 valid 0.20154422422250112
LOSS train 0.1439436712475904 valid 0.20170176323307187
LOSS train 0.1439436712475904 valid 0.2017533221802154
LOSS train 0.1439436712475904 valid 0.20166676262250313
LOSS train 0.1439436712475904 valid 0.20190220203580736
LOSS train 0.1439436712475904 valid 0.20158167071640493
LOSS train 0.1439436712475904 valid 0.20169526669714186
LOSS train 0.1439436712475904 valid 0.20196789975573376
LOSS train 0.1439436712475904 valid 0.20167711628488746
LOSS train 0.1439436712475904 valid 0.20156239540804
LOSS train 0.1439436712475904 valid 0.20176971081425163
LOSS train 0.1439436712475904 valid 0.20184960320245388
LOSS train 0.1439436712475904 valid 0.20144483960908036
LOSS train 0.1439436712475904 valid 0.20137870057739995
LOSS train 0.1439436712475904 valid 0.20150656964671745
LOSS train 0.1439436712475904 valid 0.20129521307018067
LOSS train 0.1439436712475904 valid 0.20137135553490984
LOSS train 0.1439436712475904 valid 0.20139186456799507
LOSS train 0.1439436712475904 valid 0.20170113780806143
LOSS train 0.1439436712475904 valid 0.20209114316930163
LOSS train 0.1439436712475904 valid 0.20252225242163005
LOSS train 0.1439436712475904 valid 0.2034014112626513
LOSS train 0.1439436712475904 valid 0.20357155830589765
LOSS train 0.1439436712475904 valid 0.20412164865707866
LOSS train 0.1439436712475904 valid 0.20397784628651358
LOSS train 0.1439436712475904 valid 0.2043224672973156
LOSS train 0.1439436712475904 valid 0.2042847026397686
LOSS train 0.1439436712475904 valid 0.204744806330578
LOSS train 0.1439436712475904 valid 0.2043196124937928
LOSS train 0.1439436712475904 valid 0.20414228106920534
LOSS train 0.1439436712475904 valid 0.2044109487817401
LOSS train 0.1439436712475904 valid 0.20436398319478305
LOSS train 0.1439436712475904 valid 0.20421121986669916
LOSS train 0.1439436712475904 valid 0.20428162964957733
LOSS train 0.1439436712475904 valid 0.2037427339258544
LOSS train 0.1439436712475904 valid 0.2034594927321781
LOSS train 0.1439436712475904 valid 0.20343999537798735
LOSS train 0.1439436712475904 valid 0.20359931686627014
LOSS train 0.1439436712475904 valid 0.2033712677987276
LOSS train 0.1439436712475904 valid 0.20353836736135317
LOSS train 0.1439436712475904 valid 0.2039389158072679
LOSS train 0.1439436712475904 valid 0.203898256837294
LOSS train 0.1439436712475904 valid 0.20393261479006874
LOSS train 0.1439436712475904 valid 0.20397014006719752
LOSS train 0.1439436712475904 valid 0.203734130543821
LOSS train 0.1439436712475904 valid 0.2037287015467882
LOSS train 0.1439436712475904 valid 0.20373346958278624
LOSS train 0.1439436712475904 valid 0.203742226249859
LOSS train 0.1439436712475904 valid 0.20370638200907204
LOSS train 0.1439436712475904 valid 0.20381637061795882
LOSS train 0.1439436712475904 valid 0.20402679109573366
LOSS train 0.1439436712475904 valid 0.20388061908029376
LOSS train 0.1439436712475904 valid 0.20370301698136517
LOSS train 0.1439436712475904 valid 0.20362184941768646
LOSS train 0.1439436712475904 valid 0.20351234794587128
LOSS train 0.1439436712475904 valid 0.20324570318827262
LOSS train 0.1439436712475904 valid 0.20335348086957714
LOSS train 0.1439436712475904 valid 0.20351957179831737
LOSS train 0.1439436712475904 valid 0.20346166888125858
LOSS train 0.1439436712475904 valid 0.2034996235326155
LOSS train 0.1439436712475904 valid 0.2035433527496126
LOSS train 0.1439436712475904 valid 0.2037517383475514
LOSS train 0.1439436712475904 valid 0.20358133500944958
LOSS train 0.1439436712475904 valid 0.20319863646358682
LOSS train 0.1439436712475904 valid 0.20319612002630028
LOSS train 0.1439436712475904 valid 0.20302075860755783
LOSS train 0.1439436712475904 valid 0.20328524930680053
LOSS train 0.1439436712475904 valid 0.20351341461211864
LOSS train 0.1439436712475904 valid 0.2036381408676401
LOSS train 0.1439436712475904 valid 0.20357100675917333
LOSS train 0.1439436712475904 valid 0.20336968395216712
LOSS train 0.1439436712475904 valid 0.2034220786331451
LOSS train 0.1439436712475904 valid 0.203385630128335
LOSS train 0.1439436712475904 valid 0.20314854937227997
LOSS train 0.1439436712475904 valid 0.20334437009472175
LOSS train 0.1439436712475904 valid 0.20328035324811936
LOSS train 0.1439436712475904 valid 0.20318110208242934
LOSS train 0.1439436712475904 valid 0.20326757185945385
LOSS train 0.1439436712475904 valid 0.2030295907866721
LOSS train 0.1439436712475904 valid 0.20307043891448479
LOSS train 0.1439436712475904 valid 0.2028698125193196
LOSS train 0.1439436712475904 valid 0.2030689429778319
LOSS train 0.1439436712475904 valid 0.20288064924015362
LOSS train 0.1439436712475904 valid 0.20277042121072358
LOSS train 0.1439436712475904 valid 0.2031245417190048
LOSS train 0.1439436712475904 valid 0.20308226747438313
LOSS train 0.1439436712475904 valid 0.20327235545430863
LOSS train 0.1439436712475904 valid 0.20327942422878595
LOSS train 0.1439436712475904 valid 0.20303317017716133
LOSS train 0.1439436712475904 valid 0.20306705083788895
LOSS train 0.1439436712475904 valid 0.20321642268787732
LOSS train 0.1439436712475904 valid 0.20311880713126745
LOSS train 0.1439436712475904 valid 0.20321440946556138
LOSS train 0.1439436712475904 valid 0.2028600217863208
LOSS train 0.1439436712475904 valid 0.20269761203661474
LOSS train 0.1439436712475904 valid 0.2025239852421424
LOSS train 0.1439436712475904 valid 0.2023634117597725
LOSS train 0.1439436712475904 valid 0.2024648926805618
LOSS train 0.1439436712475904 valid 0.2025651235800947
LOSS train 0.1439436712475904 valid 0.20258679677700175
LOSS train 0.1439436712475904 valid 0.2023023351601192
LOSS train 0.1439436712475904 valid 0.20228829040107402
LOSS train 0.1439436712475904 valid 0.2021456508986694
LOSS train 0.1439436712475904 valid 0.20215979637054915
LOSS train 0.1439436712475904 valid 0.20214304436185507
LOSS train 0.1439436712475904 valid 0.20213381639785236
LOSS train 0.1439436712475904 valid 0.20203308258926012
LOSS train 0.1439436712475904 valid 0.2019341773875467
LOSS train 0.1439436712475904 valid 0.20206497756184125
LOSS train 0.1439436712475904 valid 0.20209936408892923
LOSS train 0.1439436712475904 valid 0.2021972492739961
LOSS train 0.1439436712475904 valid 0.20240888888797454
LOSS train 0.1439436712475904 valid 0.20236416400116394
LOSS train 0.1439436712475904 valid 0.20232403991704292
LOSS train 0.1439436712475904 valid 0.20200789282246243
LOSS train 0.1439436712475904 valid 0.2019773094277633
LOSS train 0.1439436712475904 valid 0.20208668412338376
LOSS train 0.1439436712475904 valid 0.20210430212318897
LOSS train 0.1439436712475904 valid 0.2018492265840886
LOSS train 0.1439436712475904 valid 0.20193092839926788
LOSS train 0.1439436712475904 valid 0.2018575818874897
LOSS train 0.1439436712475904 valid 0.2020762628134416
LOSS train 0.1439436712475904 valid 0.20221642242167806
LOSS train 0.1439436712475904 valid 0.20229711887812374
LOSS train 0.1439436712475904 valid 0.2023736523773203
LOSS train 0.1439436712475904 valid 0.2025885696709156
LOSS train 0.1439436712475904 valid 0.2026347933865305
LOSS train 0.1439436712475904 valid 0.20280811434039975
LOSS train 0.1439436712475904 valid 0.20294374924868785
LOSS train 0.1439436712475904 valid 0.20274591774624937
LOSS train 0.1439436712475904 valid 0.20257435913492994
LOSS train 0.1439436712475904 valid 0.20251408783556188
LOSS train 0.1439436712475904 valid 0.2023579191038574
LOSS train 0.1439436712475904 valid 0.20215667963314515
LOSS train 0.1439436712475904 valid 0.20218013925700667
LOSS train 0.1439436712475904 valid 0.2024313646413031
LOSS train 0.1439436712475904 valid 0.2026879540975625
LOSS train 0.1439436712475904 valid 0.20254693032716806
LOSS train 0.1439436712475904 valid 0.2025393420401873
LOSS train 0.1439436712475904 valid 0.20249128627164342
LOSS train 0.1439436712475904 valid 0.2022337953711665
LOSS train 0.1439436712475904 valid 0.2019951985804019
LOSS train 0.1439436712475904 valid 0.20207502510965145
LOSS train 0.1439436712475904 valid 0.20201059109574065
LOSS train 0.1439436712475904 valid 0.20211506774436394
LOSS train 0.1439436712475904 valid 0.20205197686498816
LOSS train 0.1439436712475904 valid 0.2020404923420686
LOSS train 0.1439436712475904 valid 0.2021666549079053
LOSS train 0.1439436712475904 valid 0.2020501761024843
LOSS train 0.1439436712475904 valid 0.20192236233768718
LOSS train 0.1439436712475904 valid 0.2016201052400801
LOSS train 0.1439436712475904 valid 0.20143007592553586
LOSS train 0.1439436712475904 valid 0.20142342813214542
LOSS train 0.1439436712475904 valid 0.20147291492474706
LOSS train 0.1439436712475904 valid 0.20142325394538813
LOSS train 0.1439436712475904 valid 0.2015673965215683
LOSS train 0.1439436712475904 valid 0.2016097658769393
LOSS train 0.1439436712475904 valid 0.20179546823532418
LOSS train 0.1439436712475904 valid 0.20193656281340275
LOSS train 0.1439436712475904 valid 0.20190483497248757
LOSS train 0.1439436712475904 valid 0.2018488221980156
LOSS train 0.1439436712475904 valid 0.2016262244748867
LOSS train 0.1439436712475904 valid 0.20167273194981022
LOSS train 0.1439436712475904 valid 0.20172182069856578
LOSS train 0.1439436712475904 valid 0.20171856113308145
LOSS train 0.1439436712475904 valid 0.20133287111918133
LOSS train 0.1439436712475904 valid 0.20126937471979386
LOSS train 0.1439436712475904 valid 0.2013458076341093
LOSS train 0.1439436712475904 valid 0.20138340041715913
LOSS train 0.1439436712475904 valid 0.2014390384686775
LOSS train 0.1439436712475904 valid 0.20163671873053726
LOSS train 0.1439436712475904 valid 0.2017540472552059
LOSS train 0.1439436712475904 valid 0.20176384414014545
LOSS train 0.1439436712475904 valid 0.20169725960060472
LOSS train 0.1439436712475904 valid 0.20158502549292093
LOSS train 0.1439436712475904 valid 0.20170435720682145
LOSS train 0.1439436712475904 valid 0.20178621544306022
LOSS train 0.1439436712475904 valid 0.2018394252610585
LOSS train 0.1439436712475904 valid 0.2017559612928172
LOSS train 0.1439436712475904 valid 0.2018165404754361
LOSS train 0.1439436712475904 valid 0.20181564110166886
LOSS train 0.1439436712475904 valid 0.20185246015898883
LOSS train 0.1439436712475904 valid 0.20175695390320939
LOSS train 0.1439436712475904 valid 0.2018405215222706
LOSS train 0.1439436712475904 valid 0.2017914407156609
LOSS train 0.1439436712475904 valid 0.2016903777535145
LOSS train 0.1439436712475904 valid 0.20168014687139868
LOSS train 0.1439436712475904 valid 0.20195208637768985
LOSS train 0.1439436712475904 valid 0.20182613439206387
LOSS train 0.1439436712475904 valid 0.20183322911686968
LOSS train 0.1439436712475904 valid 0.2019244357662381
LOSS train 0.1439436712475904 valid 0.20201794993608518
LOSS train 0.1439436712475904 valid 0.20198560804686744
LOSS train 0.1439436712475904 valid 0.20202035862785667
LOSS train 0.1439436712475904 valid 0.20208003690473209
LOSS train 0.1439436712475904 valid 0.20192113088236915
LOSS train 0.1439436712475904 valid 0.20183309428806234
LOSS train 0.1439436712475904 valid 0.20166209383922465
LOSS train 0.1439436712475904 valid 0.20151656161952805
LOSS train 0.1439436712475904 valid 0.20147327224921135
LOSS train 0.1439436712475904 valid 0.20158610658212142
LOSS train 0.1439436712475904 valid 0.20171087224414383
LOSS train 0.1439436712475904 valid 0.2016088096458559
LOSS train 0.1439436712475904 valid 0.2015946606616322
LOSS train 0.1439436712475904 valid 0.20166878311437517
LOSS train 0.1439436712475904 valid 0.20149384598646844
LOSS train 0.1439436712475904 valid 0.20135407430845648
LOSS train 0.1439436712475904 valid 0.20131505576642691
LOSS train 0.1439436712475904 valid 0.20129945247628242
LOSS train 0.1439436712475904 valid 0.20112397406302707
LOSS train 0.1439436712475904 valid 0.20123704860084934
LOSS train 0.1439436712475904 valid 0.20123494020500382
LOSS train 0.1439436712475904 valid 0.20135880435593037
LOSS train 0.1439436712475904 valid 0.20131118352421457
LOSS train 0.1439436712475904 valid 0.2012846699326096
LOSS train 0.1439436712475904 valid 0.2011809440008525
LOSS train 0.1439436712475904 valid 0.20114188049871898
LOSS train 0.1439436712475904 valid 0.20113408647171438
LOSS train 0.1439436712475904 valid 0.20108485893177905
LOSS train 0.1439436712475904 valid 0.20107077330756348
LOSS train 0.1439436712475904 valid 0.20106081462512582
LOSS train 0.1439436712475904 valid 0.20109399654776663
LOSS train 0.1439436712475904 valid 0.20110876060495472
LOSS train 0.1439436712475904 valid 0.20113722369974893
LOSS train 0.1439436712475904 valid 0.20114502769250137
LOSS train 0.1439436712475904 valid 0.20115651085972786
LOSS train 0.1439436712475904 valid 0.2011335282725749
LOSS train 0.1439436712475904 valid 0.20111962713745257
LOSS train 0.1439436712475904 valid 0.2011650267607308
LOSS train 0.1439436712475904 valid 0.20112460753635356
LOSS train 0.1439436712475904 valid 0.20100766972440187
LOSS train 0.1439436712475904 valid 0.20114866838930479
LOSS train 0.1439436712475904 valid 0.20105929352367352
LOSS train 0.1439436712475904 valid 0.20115252366507208
LOSS train 0.1439436712475904 valid 0.2011869313454551
LOSS train 0.1439436712475904 valid 0.20121254334526678
LOSS train 0.1439436712475904 valid 0.20124595111589338
LOSS train 0.1439436712475904 valid 0.20121162604445067
LOSS train 0.1439436712475904 valid 0.20120445550821078
LOSS train 0.1439436712475904 valid 0.20118778459965045
LOSS train 0.1439436712475904 valid 0.201220288730803
LOSS train 0.1439436712475904 valid 0.20121143371621264
LOSS train 0.1439436712475904 valid 0.20118589182193347
LOSS train 0.1439436712475904 valid 0.20124950967494798
LOSS train 0.1439436712475904 valid 0.20125917725989065
LOSS train 0.1439436712475904 valid 0.20112761328928172
LOSS train 0.1439436712475904 valid 0.20112852896114006
LOSS train 0.1439436712475904 valid 0.2010559900577024
LOSS train 0.1439436712475904 valid 0.2010103451670508
LOSS train 0.1439436712475904 valid 0.2010037303890711
LOSS train 0.1439436712475904 valid 0.20083093450619624
LOSS train 0.1439436712475904 valid 0.20090408444953112
LOSS train 0.1439436712475904 valid 0.20079224080485306
LOSS train 0.1439436712475904 valid 0.20085839554667473
LOSS train 0.1439436712475904 valid 0.2009065997727374
LOSS train 0.1439436712475904 valid 0.2009984884298209
LOSS train 0.1439436712475904 valid 0.20089778263583283
LOSS train 0.1439436712475904 valid 0.2008811065321227
LOSS train 0.1439436712475904 valid 0.20086979673610436
LOSS train 0.1439436712475904 valid 0.20080084786443653
LOSS train 0.1439436712475904 valid 0.20066561627743848
LOSS train 0.1439436712475904 valid 0.20058029242569492
LOSS train 0.1439436712475904 valid 0.200700770853535
LOSS train 0.1439436712475904 valid 0.20064784339546451
LOSS train 0.1439436712475904 valid 0.20054477602682985
LOSS train 0.1439436712475904 valid 0.20059371117283317
LOSS train 0.1439436712475904 valid 0.2006352246681616
LOSS train 0.1439436712475904 valid 0.20066005065601472
LOSS train 0.1439436712475904 valid 0.20052262836573076
LOSS train 0.1439436712475904 valid 0.20062684423701707
LOSS train 0.1439436712475904 valid 0.20068390356457752
LOSS train 0.1439436712475904 valid 0.2005990529594394
LOSS train 0.1439436712475904 valid 0.20056084386725248
LOSS train 0.1439436712475904 valid 0.2005379069810626
LOSS train 0.1439436712475904 valid 0.2005082151524317
LOSS train 0.1439436712475904 valid 0.20054468823330743
LOSS train 0.1439436712475904 valid 0.2005913877419257
LOSS train 0.1439436712475904 valid 0.20061507300389084
LOSS train 0.1439436712475904 valid 0.2007054396965686
LOSS train 0.1439436712475904 valid 0.2005786793885258
LOSS train 0.1439436712475904 valid 0.20046730247181907
LOSS train 0.1439436712475904 valid 0.20044086086616086
LOSS train 0.1439436712475904 valid 0.20046469531640285
LOSS train 0.1439436712475904 valid 0.20057190430230934
LOSS train 0.1439436712475904 valid 0.20056826336968245
LOSS train 0.1439436712475904 valid 0.20084635039998425
LOSS train 0.1439436712475904 valid 0.20080116556291766
LOSS train 0.1439436712475904 valid 0.20095552018334195
LOSS train 0.1439436712475904 valid 0.20097398569104427
LOSS train 0.1439436712475904 valid 0.2008763285619872
LOSS train 0.1439436712475904 valid 0.20088623198744368
LOSS train 0.1439436712475904 valid 0.20087979766514782
LOSS train 0.1439436712475904 valid 0.200915113497495
LOSS train 0.1439436712475904 valid 0.20078761827038683
LOSS train 0.1439436712475904 valid 0.20089714596588115
EPOCH 12:
  batch 1 loss: 0.15193019807338715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12738947942852974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.12717682868242264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12731782905757427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.1330747738480568
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.132184357692798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1325240678020886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1300202189013362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13134867697954178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12949124053120614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1307153207334605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13289293212195238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13327804265113977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13117624553186552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13094832847515742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13213972141966224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13346315613564322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13488491997122765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13576609327604897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13584199063479901
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13667201321749461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13583246550776742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13374859191801236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.1333606687063972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13336401790380478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13575231541807836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.13628154303188678
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1364879541631256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13741349634425393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13714221293727558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.13669241988851177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13599317357875407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1360110186717727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13699117237154176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13752385654619761
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13872571434411737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13968770226111282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1388654602985633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.13793695068512207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13844531681388617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13838835078768613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13842199405743963
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13843154370091681
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13846600479023022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13825288861989976
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13814050202136455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13798839852530906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.138696924628069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.1389399263627675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1388223062455654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1385771835259363
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13812608658694303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13812328214352987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13830151533087096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13852336501533336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13894405628421477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.1392808165728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13875517184878217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13945779772633213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14027057377000648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.140960515156144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.14054324610098715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.14009847397369052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.14000668178778142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.14008288784668996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13997289549672243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.14018807226597374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.1403455910656382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.1400414722553198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13967178804533822
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13973015714699114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13929119343972868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1395334667743069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1399640538603873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13988351911306382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.139906822164592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13997622269701648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14021144186457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14038847584890413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.1405965684913099
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.14055203867179375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14068940599880567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.14048193359231376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14083212188311986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.141030511785956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14067922419933385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14071858777054425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14069139305502176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14087651293264347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1409321914944384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14094754848833924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14063909706538139
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.14073204922099267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14074310145162522
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.14046371688968257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.14041952804351845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.14030760081158472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.1401974980016144
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.14055922190950373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1405640560388565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.1406437843152792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1406490180422278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.14065306322667206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1409181607170747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14084685019084386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14080383080356526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14082779728363606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14070575518740547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14075192412652007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14093515317548405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14076935224704915
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1405100301573319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14042339223412287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.14046711873328477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14052423595086388
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1405969682823995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14065644138643885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14054275070459155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14068737056325464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.140582235964636
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14076178668698003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.1407946632167355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1407590258291097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14072736647100217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14088606125116349
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14086794646249878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14105599925039322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14099651534343138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1408437076349591
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14079102065700752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.140743977866555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14042237796115153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14038418502287758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.14018090186056806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.14013271260040777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14038269150563898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.14051207785841321
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14042788997724437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.1405987676849468
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1404256169285093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.1405733421127847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14064561565157394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.1406179043379697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14039110873515406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14057081480478417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14060691585891869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14041410128072818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.1405283969298408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.1407552483997889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14073938434322675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14090708278070221
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14098775234857672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14109037353906756
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14101835461212442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14093981716902027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14093617813136333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14086798056485547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14094174884354013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14097477753394805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14088950301520525
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.1408396134187716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.1410005037891276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14106841399077258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1409703156933552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.1407721853617466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.140611750982612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14064673115750273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14052349430996747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14039400466800442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.1404807357227101
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14053162714542702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.1404448560504026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.14033933586812433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1404188334428031
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.1404421190704618
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14050697653808378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.14087693654211228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.1408248519462146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14093769653882393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1410103498233689
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14106266398126907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.1411623334982893
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14121202831385565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14113515129555826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.14111199604498373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14119265244532658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14128886497913198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14117660266446305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1412524418934943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14126797659616722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14123464172891298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14147812543281665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1414498332791378
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14148104217709953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14148129305014243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14140414670870013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14117785917623393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.1411531451675627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14100156851749324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.1411486718058586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14108760810610074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14106921305750855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14115106846605027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.14111564008920802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1410085462942356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14108827970560314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14091680879178253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.14080618916509244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14067622801761306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1408285959490708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14086469841935623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.141066640720896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14116082032661484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.1411380578772487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14107575253691784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14104909046242634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14105937684873282
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14104671779712405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14113031936698853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.1409466588023034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14098607701548624
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14099872760004825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1410755977836425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.1410221234961812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14109215527772903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14102633458982528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14115197611931662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14101724610909036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.1409080493137826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14092905041964157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14096868109135402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.140957519540499
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14088465554775598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14086015261391288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.1407477115380003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14078290100698754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14071551550038253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14086075333737524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1407628513280318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1407222433015704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14067675692659196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14070399462684127
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14068627431068892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14066821721489312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14065753659423516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14050768358193763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14053189115003054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14067736105813133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14070161740224524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.140769176363945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14077579256785344
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14075175684595864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14069205351733408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1406140997712537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14069898575544357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14072110204142518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14063909064239102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14058436424464218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14064522001273844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14065971999214247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14083911935260013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1409386329295981
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14079748453862767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1408191141249104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14085088712426852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14072504395941146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1407625683293807
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14084606787273243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14073895651944066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.140833998416309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14075547311028871
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14068550476804376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14062551313485855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14068250069870566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14056698893958872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.1404985285539558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.14058633223982925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1405241231588151
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.14046217234117583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.140514047230993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.14045075536622695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.14048998433013335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.14043132339475853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.14046911475524096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.140637994283124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14064934677802599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14074857086462425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14064877381962207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.14051137745999134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.14049480172066853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.14048181607346355
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14048820381907567
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14063376651078768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.14061536660202506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.140684053150274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.14077861586938034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14072070258233685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14086898946322052
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14086969999166635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14079398848116398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.1407367926944926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14059958491005645
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.14064345744871856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14069826653423279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1407297511325508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14068050568107687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14061988977539422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.14063103893747578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1406934920541677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14068158711156536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14062335500284023
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14055874781348768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1406308995744291
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14066199692571238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14081116611995395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14081880629439897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14085186134189462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.140845739803974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14086469830390427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14082079818472265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14081151026803013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1409673549651359
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1408710663809496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1407956706566943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14088818534062458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14085897554192076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14087973473542328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14087952189601777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14094926065858737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.1408889547441945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14086364106109134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1409301832318306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1409706705593848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14098621690701582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14096740929048454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14103137134086519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14105428671270875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14098348944673877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.14091558378618374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1409491427023621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.1409642649404814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14093248574444425
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.1409591104259644
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14095424615972957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.141034640458183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.1409949135099877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14107210648592336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14108149499643122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1409744061849862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.141026331782341
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1411369586007887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14111667075617748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14110433574101067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.1412191803088296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14117930716192217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.141159989776906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.1411012494806148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14105671887707444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14104713671801813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.1410478133087357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.1410496297429143
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14114172137162304
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14113085766222852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.14112622997017352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14116190518826655
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1411737036876014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14109962328009773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1411937355549763
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1412185841015361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14131062846328762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14135880878752455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.141355113056238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14130934112113538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.1413797159843585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14137180950244269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.1413839697005584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14142939711202046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1414671695776402
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1415745324030086
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14164406274886507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1416324166449036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14163684452937536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14165477635577203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14168390055419877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.141610095814451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14163325389200543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14152956326571545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14159048339063973
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14156495542253503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14158273520760048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14165614623471598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14166832625941964
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14165395860377766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.1416245244140855
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1417134556400625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14166511015759575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14165432961371444
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14164526135328426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.1416643105428619
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.1416495318338275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.1417275260288519
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14178782927604458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14175981275792748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.1417536400258541
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14178168129773788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.1417978934833569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14178003436663814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14172131681413042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14173986054137167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14175164485123098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.1418451448571653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.1418369063402264
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14174636953102185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1416479517944193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.1416534730468888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14170099812774703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14159723726822604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14154836346515628
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14155722393483136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14151229228646983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14151872236131205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14159282574085827
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1415412719966954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14157499430947146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.1415611969548113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.14150233409354385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14149950999565927
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14156445805157455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14154863208184987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14158721926600434
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14158302697670708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14167998068862492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14168695489473762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14172606692061446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14171956715912654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14179393297078413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14183446175037315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14189195068187366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14190280179499493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14193024929951537
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14193355675592445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14192884503041997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14197937626198237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14194389382326925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14190062465292685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14187170267773316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14184443529286908
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14185428100505046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14185755791138435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14181211872233285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1418040762190808
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14176308577962682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1417223239445002
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14164202253330122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14156087735197045
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14156627115842543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14155254997231506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.141497384870677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14141660555385557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14145205727090007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1414611965353215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14141051932459786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14139066642999135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14146007619926643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14141499762253096
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14144204281942016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14150472872389971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14150696416568553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14150565541756432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14141672252023474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14145130879243722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14140139738003077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14140139738003077 valid 0.18126586079597473
LOSS train 0.14140139738003077 valid 0.2057422548532486
LOSS train 0.14140139738003077 valid 0.21817234655221304
LOSS train 0.14140139738003077 valid 0.21536776050925255
LOSS train 0.14140139738003077 valid 0.21499218046665192
LOSS train 0.14140139738003077 valid 0.22113454590241113
LOSS train 0.14140139738003077 valid 0.2156448428119932
LOSS train 0.14140139738003077 valid 0.21578721329569817
LOSS train 0.14140139738003077 valid 0.21585629218154484
LOSS train 0.14140139738003077 valid 0.21683899611234664
LOSS train 0.14140139738003077 valid 0.21319884061813354
LOSS train 0.14140139738003077 valid 0.21537589033444723
LOSS train 0.14140139738003077 valid 0.21411118828333342
LOSS train 0.14140139738003077 valid 0.2145165918128831
LOSS train 0.14140139738003077 valid 0.2132892151673635
LOSS train 0.14140139738003077 valid 0.2134887408465147
LOSS train 0.14140139738003077 valid 0.213914453106768
LOSS train 0.14140139738003077 valid 0.21389182077513802
LOSS train 0.14140139738003077 valid 0.21592362460337186
LOSS train 0.14140139738003077 valid 0.21537800431251525
LOSS train 0.14140139738003077 valid 0.21593673598198665
LOSS train 0.14140139738003077 valid 0.21427117762240497
LOSS train 0.14140139738003077 valid 0.21254556593687637
LOSS train 0.14140139738003077 valid 0.2132452130317688
LOSS train 0.14140139738003077 valid 0.2119092106819153
LOSS train 0.14140139738003077 valid 0.21149215618005165
LOSS train 0.14140139738003077 valid 0.21109411230793707
LOSS train 0.14140139738003077 valid 0.21119691697614534
LOSS train 0.14140139738003077 valid 0.20992871348200173
LOSS train 0.14140139738003077 valid 0.20905783623456956
LOSS train 0.14140139738003077 valid 0.20826289057731628
LOSS train 0.14140139738003077 valid 0.2091133459471166
LOSS train 0.14140139738003077 valid 0.20781438233274402
LOSS train 0.14140139738003077 valid 0.20717741373707266
LOSS train 0.14140139738003077 valid 0.20761922725609372
LOSS train 0.14140139738003077 valid 0.20778169731299082
LOSS train 0.14140139738003077 valid 0.20669772737735026
LOSS train 0.14140139738003077 valid 0.20664267398809133
LOSS train 0.14140139738003077 valid 0.20628560506380522
LOSS train 0.14140139738003077 valid 0.2058789163827896
LOSS train 0.14140139738003077 valid 0.20582011487425828
LOSS train 0.14140139738003077 valid 0.20705643225283848
LOSS train 0.14140139738003077 valid 0.20789680612641712
LOSS train 0.14140139738003077 valid 0.20694244314323773
LOSS train 0.14140139738003077 valid 0.2062803930706448
LOSS train 0.14140139738003077 valid 0.20464278336452402
LOSS train 0.14140139738003077 valid 0.2039142907299894
LOSS train 0.14140139738003077 valid 0.20550232597937187
LOSS train 0.14140139738003077 valid 0.20535127149552715
LOSS train 0.14140139738003077 valid 0.20570657759904862
LOSS train 0.14140139738003077 valid 0.20549615896215626
LOSS train 0.14140139738003077 valid 0.20525718107819557
LOSS train 0.14140139738003077 valid 0.20620387652010289
LOSS train 0.14140139738003077 valid 0.2060570545770504
LOSS train 0.14140139738003077 valid 0.20631267265840011
LOSS train 0.14140139738003077 valid 0.20598698194537843
LOSS train 0.14140139738003077 valid 0.2052945499880272
LOSS train 0.14140139738003077 valid 0.20610545678385372
LOSS train 0.14140139738003077 valid 0.20626260895850296
LOSS train 0.14140139738003077 valid 0.20590704306960106
LOSS train 0.14140139738003077 valid 0.20563901276862034
LOSS train 0.14140139738003077 valid 0.2052882051756305
LOSS train 0.14140139738003077 valid 0.20531786363276225
LOSS train 0.14140139738003077 valid 0.20518069411627948
LOSS train 0.14140139738003077 valid 0.20446626452299266
LOSS train 0.14140139738003077 valid 0.20438306959289493
LOSS train 0.14140139738003077 valid 0.2049687004356242
LOSS train 0.14140139738003077 valid 0.20428302927928813
LOSS train 0.14140139738003077 valid 0.20468066438384677
LOSS train 0.14140139738003077 valid 0.20512860545090267
LOSS train 0.14140139738003077 valid 0.20525037696663762
LOSS train 0.14140139738003077 valid 0.20573155250814226
LOSS train 0.14140139738003077 valid 0.20600211497855514
LOSS train 0.14140139738003077 valid 0.20603405462728963
LOSS train 0.14140139738003077 valid 0.205790207584699
LOSS train 0.14140139738003077 valid 0.20597595466594948
LOSS train 0.14140139738003077 valid 0.20598744513926567
LOSS train 0.14140139738003077 valid 0.2059464156627655
LOSS train 0.14140139738003077 valid 0.20617780655245238
LOSS train 0.14140139738003077 valid 0.20589299872517586
LOSS train 0.14140139738003077 valid 0.20603073011209935
LOSS train 0.14140139738003077 valid 0.20632311556397415
LOSS train 0.14140139738003077 valid 0.2060054623218904
LOSS train 0.14140139738003077 valid 0.20587200937526567
LOSS train 0.14140139738003077 valid 0.20608387379085316
LOSS train 0.14140139738003077 valid 0.20616110708824423
LOSS train 0.14140139738003077 valid 0.20574418463926206
LOSS train 0.14140139738003077 valid 0.20567241971465675
LOSS train 0.14140139738003077 valid 0.20581054787957268
LOSS train 0.14140139738003077 valid 0.2055879082944658
LOSS train 0.14140139738003077 valid 0.20565931482629463
LOSS train 0.14140139738003077 valid 0.2056869673340217
LOSS train 0.14140139738003077 valid 0.2059695355353817
LOSS train 0.14140139738003077 valid 0.20638217856275273
LOSS train 0.14140139738003077 valid 0.20681336365248026
LOSS train 0.14140139738003077 valid 0.20770655386149883
LOSS train 0.14140139738003077 valid 0.2078784157320396
LOSS train 0.14140139738003077 valid 0.20843702311418494
LOSS train 0.14140139738003077 valid 0.20828533849932931
LOSS train 0.14140139738003077 valid 0.20863505557179451
LOSS train 0.14140139738003077 valid 0.20858939729704715
LOSS train 0.14140139738003077 valid 0.20906772990437114
LOSS train 0.14140139738003077 valid 0.20864147512079442
LOSS train 0.14140139738003077 valid 0.2084619324081219
LOSS train 0.14140139738003077 valid 0.20872748139358702
LOSS train 0.14140139738003077 valid 0.20867973551997598
LOSS train 0.14140139738003077 valid 0.2085077128956251
LOSS train 0.14140139738003077 valid 0.20854734822555823
LOSS train 0.14140139738003077 valid 0.20798840818055178
LOSS train 0.14140139738003077 valid 0.20769030993635004
LOSS train 0.14140139738003077 valid 0.20766775229492704
LOSS train 0.14140139738003077 valid 0.2078493470326066
LOSS train 0.14140139738003077 valid 0.2076178897271114
LOSS train 0.14140139738003077 valid 0.20779158240347578
LOSS train 0.14140139738003077 valid 0.2081911132387493
LOSS train 0.14140139738003077 valid 0.20815483442154423
LOSS train 0.14140139738003077 valid 0.20820927225116992
LOSS train 0.14140139738003077 valid 0.20823792406057906
LOSS train 0.14140139738003077 valid 0.20801533295326874
LOSS train 0.14140139738003077 valid 0.2080106562624375
LOSS train 0.14140139738003077 valid 0.2080181803831384
LOSS train 0.14140139738003077 valid 0.20803147004764588
LOSS train 0.14140139738003077 valid 0.2080086493879799
LOSS train 0.14140139738003077 valid 0.20811893814994442
LOSS train 0.14140139738003077 valid 0.20834467267990112
LOSS train 0.14140139738003077 valid 0.20820401561638666
LOSS train 0.14140139738003077 valid 0.2080215004485423
LOSS train 0.14140139738003077 valid 0.20794023224152625
LOSS train 0.14140139738003077 valid 0.20783586730790693
LOSS train 0.14140139738003077 valid 0.2075861958357004
LOSS train 0.14140139738003077 valid 0.20767387224517705
LOSS train 0.14140139738003077 valid 0.20785878779310168
LOSS train 0.14140139738003077 valid 0.20780836112965317
LOSS train 0.14140139738003077 valid 0.20784593562581646
LOSS train 0.14140139738003077 valid 0.20789780793366608
LOSS train 0.14140139738003077 valid 0.20809650289661744
LOSS train 0.14140139738003077 valid 0.20792772724245587
LOSS train 0.14140139738003077 valid 0.20752487493597943
LOSS train 0.14140139738003077 valid 0.2075170297845662
LOSS train 0.14140139738003077 valid 0.20734402558633258
LOSS train 0.14140139738003077 valid 0.20760677416696616
LOSS train 0.14140139738003077 valid 0.20781921658297658
LOSS train 0.14140139738003077 valid 0.20794630071499964
LOSS train 0.14140139738003077 valid 0.20787503715190622
LOSS train 0.14140139738003077 valid 0.20767939974521768
LOSS train 0.14140139738003077 valid 0.20771803023063973
LOSS train 0.14140139738003077 valid 0.2076787626256748
LOSS train 0.14140139738003077 valid 0.20743228454847593
LOSS train 0.14140139738003077 valid 0.2076345135101536
LOSS train 0.14140139738003077 valid 0.2075543196996053
LOSS train 0.14140139738003077 valid 0.20745548892100127
LOSS train 0.14140139738003077 valid 0.20753203371637746
LOSS train 0.14140139738003077 valid 0.20728932515858045
LOSS train 0.14140139738003077 valid 0.20732600803112056
LOSS train 0.14140139738003077 valid 0.20712841797259546
LOSS train 0.14140139738003077 valid 0.20733955760414785
LOSS train 0.14140139738003077 valid 0.20715641965911646
LOSS train 0.14140139738003077 valid 0.20704595546556426
LOSS train 0.14140139738003077 valid 0.2073936605790876
LOSS train 0.14140139738003077 valid 0.20733386762440204
LOSS train 0.14140139738003077 valid 0.20753368355842852
LOSS train 0.14140139738003077 valid 0.20755091916631768
LOSS train 0.14140139738003077 valid 0.2073200550730243
LOSS train 0.14140139738003077 valid 0.20732335864407261
LOSS train 0.14140139738003077 valid 0.20746499128413923
LOSS train 0.14140139738003077 valid 0.20737676959798998
LOSS train 0.14140139738003077 valid 0.2074885974208752
LOSS train 0.14140139738003077 valid 0.20712955393606708
LOSS train 0.14140139738003077 valid 0.20696906207581245
LOSS train 0.14140139738003077 valid 0.20679940116756101
LOSS train 0.14140139738003077 valid 0.20662651346092337
LOSS train 0.14140139738003077 valid 0.2067204000818175
LOSS train 0.14140139738003077 valid 0.20682177343809535
LOSS train 0.14140139738003077 valid 0.20684904853502908
LOSS train 0.14140139738003077 valid 0.20656160703727178
LOSS train 0.14140139738003077 valid 0.20654629746621306
LOSS train 0.14140139738003077 valid 0.20640013539521707
LOSS train 0.14140139738003077 valid 0.20641509786750492
LOSS train 0.14140139738003077 valid 0.20640397329903182
LOSS train 0.14140139738003077 valid 0.20638748763336076
LOSS train 0.14140139738003077 valid 0.2062755125003625
LOSS train 0.14140139738003077 valid 0.20616485047471392
LOSS train 0.14140139738003077 valid 0.20630584250056677
LOSS train 0.14140139738003077 valid 0.20634205040076506
LOSS train 0.14140139738003077 valid 0.20641990384540043
LOSS train 0.14140139738003077 valid 0.20662536432025252
LOSS train 0.14140139738003077 valid 0.20657534913264494
LOSS train 0.14140139738003077 valid 0.20653427051419906
LOSS train 0.14140139738003077 valid 0.20621119975728333
LOSS train 0.14140139738003077 valid 0.2061707602519738
LOSS train 0.14140139738003077 valid 0.20628422124223558
LOSS train 0.14140139738003077 valid 0.20629153214395046
LOSS train 0.14140139738003077 valid 0.20603224797261194
LOSS train 0.14140139738003077 valid 0.2061142058102126
LOSS train 0.14140139738003077 valid 0.20603289405504863
LOSS train 0.14140139738003077 valid 0.20625424445891866
LOSS train 0.14140139738003077 valid 0.2063995942397771
LOSS train 0.14140139738003077 valid 0.20647492515619356
LOSS train 0.14140139738003077 valid 0.20654713119094695
LOSS train 0.14140139738003077 valid 0.20675391234457494
LOSS train 0.14140139738003077 valid 0.20678122311978792
LOSS train 0.14140139738003077 valid 0.20695967714090158
LOSS train 0.14140139738003077 valid 0.20709496699824123
LOSS train 0.14140139738003077 valid 0.20689074110751057
LOSS train 0.14140139738003077 valid 0.20671944443772478
LOSS train 0.14140139738003077 valid 0.20666115868438795
LOSS train 0.14140139738003077 valid 0.206510616529391
LOSS train 0.14140139738003077 valid 0.20631348950645098
LOSS train 0.14140139738003077 valid 0.20633724502969586
LOSS train 0.14140139738003077 valid 0.20658809216249557
LOSS train 0.14140139738003077 valid 0.2068650776176091
LOSS train 0.14140139738003077 valid 0.2067150094739671
LOSS train 0.14140139738003077 valid 0.20668629938168145
LOSS train 0.14140139738003077 valid 0.20664367984945528
LOSS train 0.14140139738003077 valid 0.20637945262498633
LOSS train 0.14140139738003077 valid 0.20613834958661487
LOSS train 0.14140139738003077 valid 0.20621219366651528
LOSS train 0.14140139738003077 valid 0.20614500931643565
LOSS train 0.14140139738003077 valid 0.20626074402299646
LOSS train 0.14140139738003077 valid 0.20619343688542194
LOSS train 0.14140139738003077 valid 0.20616656757587762
LOSS train 0.14140139738003077 valid 0.20630228224101368
LOSS train 0.14140139738003077 valid 0.20617452384110524
LOSS train 0.14140139738003077 valid 0.2060460285283625
LOSS train 0.14140139738003077 valid 0.20574058989683788
LOSS train 0.14140139738003077 valid 0.20555087976751074
LOSS train 0.14140139738003077 valid 0.20554611526659405
LOSS train 0.14140139738003077 valid 0.20559174011935266
LOSS train 0.14140139738003077 valid 0.20554709219776388
LOSS train 0.14140139738003077 valid 0.20567490199337835
LOSS train 0.14140139738003077 valid 0.20572825995358554
LOSS train 0.14140139738003077 valid 0.20592232061357335
LOSS train 0.14140139738003077 valid 0.2060596476680731
LOSS train 0.14140139738003077 valid 0.20603000869353613
LOSS train 0.14140139738003077 valid 0.2059858869999013
LOSS train 0.14140139738003077 valid 0.20576416126499741
LOSS train 0.14140139738003077 valid 0.20581354251139275
LOSS train 0.14140139738003077 valid 0.2058712893298694
LOSS train 0.14140139738003077 valid 0.20586810064615066
LOSS train 0.14140139738003077 valid 0.20547381493573388
LOSS train 0.14140139738003077 valid 0.20540453793473262
LOSS train 0.14140139738003077 valid 0.20548363088453112
LOSS train 0.14140139738003077 valid 0.2055250318881906
LOSS train 0.14140139738003077 valid 0.20557795223764708
LOSS train 0.14140139738003077 valid 0.20576948027829733
LOSS train 0.14140139738003077 valid 0.20588708659861146
LOSS train 0.14140139738003077 valid 0.20589725398703626
LOSS train 0.14140139738003077 valid 0.20583077817554435
LOSS train 0.14140139738003077 valid 0.20571646061407994
LOSS train 0.14140139738003077 valid 0.20584399047493934
LOSS train 0.14140139738003077 valid 0.2059363034438802
LOSS train 0.14140139738003077 valid 0.20598654416463677
LOSS train 0.14140139738003077 valid 0.2058917262514118
LOSS train 0.14140139738003077 valid 0.2059628549581907
LOSS train 0.14140139738003077 valid 0.20596902805800532
LOSS train 0.14140139738003077 valid 0.20600624816142954
LOSS train 0.14140139738003077 valid 0.20591067209665878
LOSS train 0.14140139738003077 valid 0.20600064373178076
LOSS train 0.14140139738003077 valid 0.20594564393795595
LOSS train 0.14140139738003077 valid 0.20585948718855016
LOSS train 0.14140139738003077 valid 0.2058481249487263
LOSS train 0.14140139738003077 valid 0.20611864166291616
LOSS train 0.14140139738003077 valid 0.20598887306542452
LOSS train 0.14140139738003077 valid 0.2059873336023002
LOSS train 0.14140139738003077 valid 0.20607176791384535
LOSS train 0.14140139738003077 valid 0.20616226466862778
LOSS train 0.14140139738003077 valid 0.2061267936888259
LOSS train 0.14140139738003077 valid 0.20615777715262193
LOSS train 0.14140139738003077 valid 0.20622296922158131
LOSS train 0.14140139738003077 valid 0.20606625695471412
LOSS train 0.14140139738003077 valid 0.2059776699136104
LOSS train 0.14140139738003077 valid 0.20579999934553223
LOSS train 0.14140139738003077 valid 0.205652168518676
LOSS train 0.14140139738003077 valid 0.2056095430178799
LOSS train 0.14140139738003077 valid 0.20572246798060156
LOSS train 0.14140139738003077 valid 0.20585087214367112
LOSS train 0.14140139738003077 valid 0.20575304240633865
LOSS train 0.14140139738003077 valid 0.2057411739562484
LOSS train 0.14140139738003077 valid 0.20581912965009716
LOSS train 0.14140139738003077 valid 0.205627455056778
LOSS train 0.14140139738003077 valid 0.20549126014484628
LOSS train 0.14140139738003077 valid 0.20544827837787621
LOSS train 0.14140139738003077 valid 0.2054348075400393
LOSS train 0.14140139738003077 valid 0.20525922072710287
LOSS train 0.14140139738003077 valid 0.20537202271975968
LOSS train 0.14140139738003077 valid 0.20536677057405453
LOSS train 0.14140139738003077 valid 0.2054935295770808
LOSS train 0.14140139738003077 valid 0.20544928149320185
LOSS train 0.14140139738003077 valid 0.20542321725390775
LOSS train 0.14140139738003077 valid 0.20531535916801158
LOSS train 0.14140139738003077 valid 0.20528328134534285
LOSS train 0.14140139738003077 valid 0.20527771638374623
LOSS train 0.14140139738003077 valid 0.20522394829744364
LOSS train 0.14140139738003077 valid 0.20521048965806865
LOSS train 0.14140139738003077 valid 0.20520641796164593
LOSS train 0.14140139738003077 valid 0.20524288454671968
LOSS train 0.14140139738003077 valid 0.20526719833363588
LOSS train 0.14140139738003077 valid 0.20529206789679974
LOSS train 0.14140139738003077 valid 0.20530410900861523
LOSS train 0.14140139738003077 valid 0.20531294621527196
LOSS train 0.14140139738003077 valid 0.20528469422825943
LOSS train 0.14140139738003077 valid 0.20526979061842754
LOSS train 0.14140139738003077 valid 0.20530682794823504
LOSS train 0.14140139738003077 valid 0.2052662335581293
LOSS train 0.14140139738003077 valid 0.20514900466946304
LOSS train 0.14140139738003077 valid 0.2052920341150823
LOSS train 0.14140139738003077 valid 0.20520787059969545
LOSS train 0.14140139738003077 valid 0.20529791042692475
LOSS train 0.14140139738003077 valid 0.20532844485684892
LOSS train 0.14140139738003077 valid 0.20535724102970093
LOSS train 0.14140139738003077 valid 0.20538885893833217
LOSS train 0.14140139738003077 valid 0.20535779302605453
LOSS train 0.14140139738003077 valid 0.2053511738539123
LOSS train 0.14140139738003077 valid 0.20533941479720128
LOSS train 0.14140139738003077 valid 0.20537790431389732
LOSS train 0.14140139738003077 valid 0.20537419577093818
LOSS train 0.14140139738003077 valid 0.20534562347721227
LOSS train 0.14140139738003077 valid 0.2054093929722249
LOSS train 0.14140139738003077 valid 0.2054166580115366
LOSS train 0.14140139738003077 valid 0.20528469181153924
LOSS train 0.14140139738003077 valid 0.20529017838826433
LOSS train 0.14140139738003077 valid 0.20521664769845718
LOSS train 0.14140139738003077 valid 0.20517201899282703
LOSS train 0.14140139738003077 valid 0.20516167551187087
LOSS train 0.14140139738003077 valid 0.20498612607900912
LOSS train 0.14140139738003077 valid 0.20506057040365927
LOSS train 0.14140139738003077 valid 0.20495325784891025
LOSS train 0.14140139738003077 valid 0.20501833774785444
LOSS train 0.14140139738003077 valid 0.2050571141136091
LOSS train 0.14140139738003077 valid 0.20514342507178132
LOSS train 0.14140139738003077 valid 0.20504345101082072
LOSS train 0.14140139738003077 valid 0.20502984126558504
LOSS train 0.14140139738003077 valid 0.20502051986284084
LOSS train 0.14140139738003077 valid 0.20495576734939022
LOSS train 0.14140139738003077 valid 0.20482310864017972
LOSS train 0.14140139738003077 valid 0.20473881059193186
LOSS train 0.14140139738003077 valid 0.20486442985715073
LOSS train 0.14140139738003077 valid 0.20480878666572316
LOSS train 0.14140139738003077 valid 0.20470600737846706
LOSS train 0.14140139738003077 valid 0.20475742556154727
LOSS train 0.14140139738003077 valid 0.20479896946474255
LOSS train 0.14140139738003077 valid 0.20483111971390178
LOSS train 0.14140139738003077 valid 0.20469519037633874
LOSS train 0.14140139738003077 valid 0.20480615301274283
LOSS train 0.14140139738003077 valid 0.20486366008070933
LOSS train 0.14140139738003077 valid 0.2047738630776805
LOSS train 0.14140139738003077 valid 0.20473091632814847
LOSS train 0.14140139738003077 valid 0.20471250229440202
LOSS train 0.14140139738003077 valid 0.20468599921396605
LOSS train 0.14140139738003077 valid 0.20472729429602624
LOSS train 0.14140139738003077 valid 0.20477370796804753
LOSS train 0.14140139738003077 valid 0.20480285697108644
LOSS train 0.14140139738003077 valid 0.20489709596373878
LOSS train 0.14140139738003077 valid 0.20476929071006802
LOSS train 0.14140139738003077 valid 0.20465382797197557
LOSS train 0.14140139738003077 valid 0.20462913620756584
LOSS train 0.14140139738003077 valid 0.20466158710190563
LOSS train 0.14140139738003077 valid 0.20477036584439223
LOSS train 0.14140139738003077 valid 0.2047698757816158
LOSS train 0.14140139738003077 valid 0.2050529548070497
LOSS train 0.14140139738003077 valid 0.2050083946446963
LOSS train 0.14140139738003077 valid 0.20516438738993517
LOSS train 0.14140139738003077 valid 0.20518123050612852
LOSS train 0.14140139738003077 valid 0.20508478694482818
LOSS train 0.14140139738003077 valid 0.20510293704597918
LOSS train 0.14140139738003077 valid 0.20509919746081687
LOSS train 0.14140139738003077 valid 0.20513906581606461
LOSS train 0.14140139738003077 valid 0.20500621991232038
LOSS train 0.14140139738003077 valid 0.2051146509564989
EPOCH 13:
  batch 1 loss: 0.15484461188316345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.12864765524864197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.1284011205037435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.12818334624171257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13206004798412324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13228032737970352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13247224688529968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1301962397992611
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.1310476760069529
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1287645861506462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13020425493066962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13207388296723366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13275300654081199
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.1305617934891156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.13038089176019033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.1319695096462965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13313955682165482
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13465399460660088
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13573875082166573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13602972477674485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13664692356472924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13590407405387273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13401750701925028
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13356183345119158
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13367632627487183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13585755630181387
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1362934140143571
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1366799117199012
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.13746903733960514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13716032554705937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1367815064807092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13618060061708093
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13615292671954993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13716100682230556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13759706956999643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13870760715670055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13950304687023163
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.13869857631231608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1377410670885673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13836201168596746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.1382124834671253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13823032059839793
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.1383432482564172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1384713158688762
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13833357526196374
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.138129240144854
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.13789627336441201
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13851240960260233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13875098344014616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.1385750225186348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.13832433755491294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.13785176466290766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1380062887691102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13818188894678046
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13837079785086892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13872646593621799
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13897225940436647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.1386181198317429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13931494370355443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.14004097655415534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14067625926166286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1402190801116728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13978313548224314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13970652036368847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13977189430823692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13956161294922684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.1397136592153293
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13987829032189705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13954064703505972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13918066269585064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13922033186109972
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13877273671742943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13900699303166508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13947149315798604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13928845355908076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13936256352615983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.1395078536558461
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.1397063473287301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13977718324978142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13991831513121725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1399032739393505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14001611629273833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13981974995638952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14019374948527133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.1402980410877396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13990571549118952
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.14008155764862038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13999827604063533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14012054504638308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1402163315978315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14023547161083955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13993273489177227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.13998656191172137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.13997453459082765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.139757706538627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13977429821776846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13965746337912746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13951916285619445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1398815538997602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1399129306524992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13996175942149494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.139974715297713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.13997085848190252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14019496832042933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14011879314978917
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14016907851932184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14012043421792092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.1400174068769923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14007341663498396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.1402765036306598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14018402986966813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13992221354107773
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13982375829884436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13987794315867258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.13992318465657858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14003996506076435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14010210188790265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.1400031771321418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14016553891055725
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.14003856802980105
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14019087326428123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14020837733491523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14012079255852272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.14005069158250286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.14020352351665497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14020897400757623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14038614025266152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.1403340791584924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.1402295984963114
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14019156247377396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14011600090347173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.1397768338634209
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.1397590274761494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13955367606744837
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.13952320941068508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13980315510621844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1398990933586211
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13981434265556542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13995566036847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13983669131994247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13994539164482278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14005569085268907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14001037055379026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13975598466479117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13998440647947377
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14000558628611368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13976711369290643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13996390716449633
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14018561326017315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14013096163670222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14024548341106896
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14039387612750656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14049968513008815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14049989352752637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14045386775847404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.14045125886033744
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14042357996011237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1404708816280848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14049125570546156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14037517877295613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14031196556846548
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14044725260248891
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14052139368890984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1404261362807053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14019686471332204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1400482562112521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.1400681327203077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.1399173377347844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13983632403717944
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13995503178414176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.1399989545519589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13988133818777496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13977239771902217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.13985132533072056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13986945079905647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13989371480420232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.140292788755759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14024611600161938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14035933289947455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1404628901431958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1405156985419231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14063372311520053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14073517971514352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14063756103100983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1406179564224707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1406662771778722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14077587258369528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14065103652946492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14074551412668176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14077196215328416
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14073133125355106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.1409524685392777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14091529044771442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14091526878248786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14090439669596844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14084033044625302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14061216374641747
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14059952383089547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.14045368483168397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14057659152895213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14053574608481345
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14048365942469918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1405611386513475
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1405270452081573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1404444718869721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14056690822093232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14043255215105804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1402811792034369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14014707655427558
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.1402768020118986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14031866602423068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14054210048239185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14066142935148426
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14061382252757795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14057780525019004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14056080614251118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14058022736679024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14058824150113886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14066875770211765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14047122861851347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14051961548188152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14048644497587875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.14056612217105557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14052885970366852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14060622692108155
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14054139387027353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.1406465463570036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.14052881176273027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14044214075829786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14046321038318718
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14048590862647795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1405006126211635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.14042926253551066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14040614884251204
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14030557167022786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.1403572851065862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.1402830595960094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1404140520246089
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.14031702545023364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14027732191607356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14025626442862743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14027918040875562
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14026430657003153
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14024434675325137
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14022107060466493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14005592026240457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14009396294471224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14022414803865454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.14024752225861492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14029037800431252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.14029184455771845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1402593046012852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14018889233882248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.14012293807986215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14021174244436563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.14022215802106075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14013089657061759
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14006166411347168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14012819088448888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.1401588470603411
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.1403440390338843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.14043468103267764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.1402876011375692
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.14033374514882313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14036560724928693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14022237210905641
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14024633685412924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.1403119684989328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14019392200894515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14029459840169659
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.14022392655870572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14015361003796845
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.1400668345964872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14012809386001016
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.14001322922381487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13994989895086357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1400499043159106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13998764463894658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13992840193185327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.139968736389918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.139900535928397
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13993224105302324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13988979703124757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13993224291734294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.1401094267242833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.14010454651775894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.1402172307104184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.14012412830359405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1399979045595265
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.1399979206251687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13999360766505048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.14001846760001085
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.14016112987803925
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1401369545583417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.1402115598320961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1403071623849305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14025517866667675
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.1404233640872392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.14042106649548713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14033298442761102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14029389971316455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.14015381092464688
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1401929493391081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1402440706365987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.14027709159694735
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14024038564145955
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.14017199637544273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.1401777970665074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14024758707840465
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.14024495590598351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.14018668025923695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.14012841417048222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.1401759157546412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.14020211605509375
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14036642899588933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.1403767904148826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14041279905037926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14039895997482277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14041922879069577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.1403771150391549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14036860788165595
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14052308693250515
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.14044128215349866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1403746988946273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14044915286394266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.14044528692229394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14048329938989168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14049032693955957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14055526487551923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14050148958058067
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14047349883620472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.14054451427276593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.1405823535702608
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14059402943103613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14055460088733418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14062097928087627
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.14061651838727804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1405541684765082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1404841447738068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.14050789843587314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.140524162050566
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.14049895491167816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14053925298393294
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14053038828248202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14060153753861138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.14057282453155243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14064714741981682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14065363497912198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1405422084652592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.140591253893716
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.1407138221284263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14066826171157035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.14065163647993428
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14077189741498333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.14071762622661993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.1407038579286819
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14064039834657638
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.1406021005156653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14058972183293286
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14058646106471617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14058989040821873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.1406723949749496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14066685422146616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1406549889348693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14069137297672768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1407040622431398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.14061649005646926
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.14071635715663433
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.1407245364857883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14080847308442399
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14086401976665075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14085244499547508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1407973784503924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14085964968099313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14085484637816748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14086545411338833
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.140909974371881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.140952570393445
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14107298821525396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14113074860682612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.1411297149819339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.1411355970846733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1411544442293538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14117963055226332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14110175398263064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.1411307944766598
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14103797813564617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.1410943327721247
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1410613550701607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.1410758295502418
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14115557493761066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14117391342867394
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14117771823139288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14114414469390957
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14122436785999734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14119214861832483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14117800217281362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14116013405760328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14116813810098738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14115635000169277
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14124752280123512
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14132606227006486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.14130290555155306
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14130060975947004
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14130204419294992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14130200720919764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14129047095775604
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.1412356039326565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14125381566784492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.1412422362260702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14133538845971844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14132714625990506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14123951421811565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.1411421648654097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14114323209926308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14119053949028826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14108850594333036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14104739931282814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1410453215525941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14100606074290617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14102900883010333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14109367886072652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.14104210381037235
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14109031281451573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14109333751832737
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1410420104126695
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14102112341331932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14107244366031385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14103911617349635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14107564207772877
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14106771704284884
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14117507489950018
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14118465435353492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14122765787666844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14122794474335923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.141294943647237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14132788243705674
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14137383902521983
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14137879558807082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14140866112641312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14141988323456575
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14140560617881123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14143154790318038
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.1413941584393248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14134526639507058
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14131417350985545
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14129176050824607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.14130178913806699
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.1412960522059343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14124528720974922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14124904178132502
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14120627072664488
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.14116909269399727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.1410807930097181
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.1410071462884054
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14101425854064392
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14101027648415032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1409645808706117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.1408759787159288
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14089839473042798
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14090806079337498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14085080584277324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.14083413425712296
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14091247034355484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.14087336848499954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14088662214928943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.1409410195508912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14094534201117662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14095239579550492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.1408544084017581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14089930388135263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14084778876059642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14084778876059642 valid 0.1835072934627533
LOSS train 0.14084778876059642 valid 0.20678459107875824
LOSS train 0.14084778876059642 valid 0.21898008386294046
LOSS train 0.14084778876059642 valid 0.2159838080406189
LOSS train 0.14084778876059642 valid 0.21457253098487855
LOSS train 0.14084778876059642 valid 0.22091110547383627
LOSS train 0.14084778876059642 valid 0.21568007341453008
LOSS train 0.14084778876059642 valid 0.2160068415105343
LOSS train 0.14084778876059642 valid 0.21590369608667162
LOSS train 0.14084778876059642 valid 0.21692621111869811
LOSS train 0.14084778876059642 valid 0.21308251538059927
LOSS train 0.14084778876059642 valid 0.21517846236626306
LOSS train 0.14084778876059642 valid 0.21370907242481524
LOSS train 0.14084778876059642 valid 0.21425010583230428
LOSS train 0.14084778876059642 valid 0.21296244164307912
LOSS train 0.14084778876059642 valid 0.2131816428154707
LOSS train 0.14084778876059642 valid 0.21350918184308446
LOSS train 0.14084778876059642 valid 0.2135575231578615
LOSS train 0.14084778876059642 valid 0.21544479853228518
LOSS train 0.14084778876059642 valid 0.21503102406859398
LOSS train 0.14084778876059642 valid 0.21582100433962687
LOSS train 0.14084778876059642 valid 0.21401642533865842
LOSS train 0.14084778876059642 valid 0.21229153547597968
LOSS train 0.14084778876059642 valid 0.21299614322682223
LOSS train 0.14084778876059642 valid 0.21158589899539948
LOSS train 0.14084778876059642 valid 0.211324220093397
LOSS train 0.14084778876059642 valid 0.21100561872676568
LOSS train 0.14084778876059642 valid 0.2110071847481387
LOSS train 0.14084778876059642 valid 0.20975180175797692
LOSS train 0.14084778876059642 valid 0.20889073063929875
LOSS train 0.14084778876059642 valid 0.20808593496199576
LOSS train 0.14084778876059642 valid 0.20900467224419117
LOSS train 0.14084778876059642 valid 0.2076675273252256
LOSS train 0.14084778876059642 valid 0.20693344666677363
LOSS train 0.14084778876059642 valid 0.20734883078507016
LOSS train 0.14084778876059642 valid 0.2074386423660649
LOSS train 0.14084778876059642 valid 0.20636102194721634
LOSS train 0.14084778876059642 valid 0.206291837519721
LOSS train 0.14084778876059642 valid 0.2059525530307721
LOSS train 0.14084778876059642 valid 0.20553308241069318
LOSS train 0.14084778876059642 valid 0.2054631124909331
LOSS train 0.14084778876059642 valid 0.20664868361893154
LOSS train 0.14084778876059642 valid 0.20747908118159272
LOSS train 0.14084778876059642 valid 0.20655118504708464
LOSS train 0.14084778876059642 valid 0.20586416059070164
LOSS train 0.14084778876059642 valid 0.20418958301129547
LOSS train 0.14084778876059642 valid 0.2034608027402391
LOSS train 0.14084778876059642 valid 0.20497242702792087
LOSS train 0.14084778876059642 valid 0.2049065527259087
LOSS train 0.14084778876059642 valid 0.2052616658806801
LOSS train 0.14084778876059642 valid 0.20499846485315584
LOSS train 0.14084778876059642 valid 0.204810995895129
LOSS train 0.14084778876059642 valid 0.20578303876912818
LOSS train 0.14084778876059642 valid 0.20567070554803918
LOSS train 0.14084778876059642 valid 0.20590829659592022
LOSS train 0.14084778876059642 valid 0.20560591481626034
LOSS train 0.14084778876059642 valid 0.20492303711280488
LOSS train 0.14084778876059642 valid 0.20577244794574276
LOSS train 0.14084778876059642 valid 0.20597806345608274
LOSS train 0.14084778876059642 valid 0.2056051266690095
LOSS train 0.14084778876059642 valid 0.20536637061932048
LOSS train 0.14084778876059642 valid 0.20498802128338045
LOSS train 0.14084778876059642 valid 0.20499641578348857
LOSS train 0.14084778876059642 valid 0.20487771299667656
LOSS train 0.14084778876059642 valid 0.20417943528065316
LOSS train 0.14084778876059642 valid 0.20411694659428162
LOSS train 0.14084778876059642 valid 0.20472219163802133
LOSS train 0.14084778876059642 valid 0.20403824943829985
LOSS train 0.14084778876059642 valid 0.2044332580289979
LOSS train 0.14084778876059642 valid 0.20483171684401377
LOSS train 0.14084778876059642 valid 0.20491676557231958
LOSS train 0.14084778876059642 valid 0.20539716713958317
LOSS train 0.14084778876059642 valid 0.20564228867831297
LOSS train 0.14084778876059642 valid 0.20570768174287435
LOSS train 0.14084778876059642 valid 0.2054531208674113
LOSS train 0.14084778876059642 valid 0.20565475680326162
LOSS train 0.14084778876059642 valid 0.20566326224958742
LOSS train 0.14084778876059642 valid 0.20561932695981783
LOSS train 0.14084778876059642 valid 0.20586352182340018
LOSS train 0.14084778876059642 valid 0.2056537652388215
LOSS train 0.14084778876059642 valid 0.20578106501956045
LOSS train 0.14084778876059642 valid 0.20607781591938762
LOSS train 0.14084778876059642 valid 0.2057444160602179
LOSS train 0.14084778876059642 valid 0.20560151462753615
LOSS train 0.14084778876059642 valid 0.20584190207369188
LOSS train 0.14084778876059642 valid 0.20591590709464494
LOSS train 0.14084778876059642 valid 0.20553350859674915
LOSS train 0.14084778876059642 valid 0.2054514983160929
LOSS train 0.14084778876059642 valid 0.20556668083319504
LOSS train 0.14084778876059642 valid 0.20535648365815481
LOSS train 0.14084778876059642 valid 0.2054510982809486
LOSS train 0.14084778876059642 valid 0.205488886360241
LOSS train 0.14084778876059642 valid 0.2057957992117892
LOSS train 0.14084778876059642 valid 0.20622443835786047
LOSS train 0.14084778876059642 valid 0.20666393731769764
LOSS train 0.14084778876059642 valid 0.20757712765286365
LOSS train 0.14084778876059642 valid 0.20775905389761187
LOSS train 0.14084778876059642 valid 0.20831658843220496
LOSS train 0.14084778876059642 valid 0.20815963350763225
LOSS train 0.14084778876059642 valid 0.20847676888108255
LOSS train 0.14084778876059642 valid 0.20839717039967529
LOSS train 0.14084778876059642 valid 0.20888958140915515
LOSS train 0.14084778876059642 valid 0.20846124007863906
LOSS train 0.14084778876059642 valid 0.20827209777556932
LOSS train 0.14084778876059642 valid 0.2084875132356371
LOSS train 0.14084778876059642 valid 0.20842219425259895
LOSS train 0.14084778876059642 valid 0.20824481393689306
LOSS train 0.14084778876059642 valid 0.20823986938706152
LOSS train 0.14084778876059642 valid 0.20766336477677758
LOSS train 0.14084778876059642 valid 0.20735718309879303
LOSS train 0.14084778876059642 valid 0.20733158156141504
LOSS train 0.14084778876059642 valid 0.20752936934254
LOSS train 0.14084778876059642 valid 0.20727790813002966
LOSS train 0.14084778876059642 valid 0.20750475491870912
LOSS train 0.14084778876059642 valid 0.2079077810049057
LOSS train 0.14084778876059642 valid 0.2078686316722426
LOSS train 0.14084778876059642 valid 0.20789366642124632
LOSS train 0.14084778876059642 valid 0.20790960210359702
LOSS train 0.14084778876059642 valid 0.20769004799237772
LOSS train 0.14084778876059642 valid 0.20770489784578483
LOSS train 0.14084778876059642 valid 0.20771576710476364
LOSS train 0.14084778876059642 valid 0.20772998999865327
LOSS train 0.14084778876059642 valid 0.20770502005650746
LOSS train 0.14084778876059642 valid 0.2078221375423093
LOSS train 0.14084778876059642 valid 0.20806268572807313
LOSS train 0.14084778876059642 valid 0.20791539561653893
LOSS train 0.14084778876059642 valid 0.20773617618196594
LOSS train 0.14084778876059642 valid 0.20763903902843595
LOSS train 0.14084778876059642 valid 0.20754133904165076
LOSS train 0.14084778876059642 valid 0.20730373458220408
LOSS train 0.14084778876059642 valid 0.2073846037378748
LOSS train 0.14084778876059642 valid 0.20757115733894435
LOSS train 0.14084778876059642 valid 0.20753246254490731
LOSS train 0.14084778876059642 valid 0.20755634134385123
LOSS train 0.14084778876059642 valid 0.2076149887508816
LOSS train 0.14084778876059642 valid 0.20781264892395804
LOSS train 0.14084778876059642 valid 0.20763641791622134
LOSS train 0.14084778876059642 valid 0.20722231020530066
LOSS train 0.14084778876059642 valid 0.2072082261601798
LOSS train 0.14084778876059642 valid 0.2070290366453784
LOSS train 0.14084778876059642 valid 0.20729970794620242
LOSS train 0.14084778876059642 valid 0.20752046876390215
LOSS train 0.14084778876059642 valid 0.20762870455538476
LOSS train 0.14084778876059642 valid 0.20755670747409263
LOSS train 0.14084778876059642 valid 0.20736271256002886
LOSS train 0.14084778876059642 valid 0.20738307794887725
LOSS train 0.14084778876059642 valid 0.20734502800873347
LOSS train 0.14084778876059642 valid 0.20710180377638018
LOSS train 0.14084778876059642 valid 0.20732276031634952
LOSS train 0.14084778876059642 valid 0.2072257728377978
LOSS train 0.14084778876059642 valid 0.2071233813138987
LOSS train 0.14084778876059642 valid 0.20718018828253998
LOSS train 0.14084778876059642 valid 0.20693685815614812
LOSS train 0.14084778876059642 valid 0.20696851059600904
LOSS train 0.14084778876059642 valid 0.20676604442058072
LOSS train 0.14084778876059642 valid 0.20696452976419374
LOSS train 0.14084778876059642 valid 0.20677484087883288
LOSS train 0.14084778876059642 valid 0.20667179400407815
LOSS train 0.14084778876059642 valid 0.2069931279563304
LOSS train 0.14084778876059642 valid 0.20693510062992573
LOSS train 0.14084778876059642 valid 0.20712796464469863
LOSS train 0.14084778876059642 valid 0.2071476227339403
LOSS train 0.14084778876059642 valid 0.2069203513531597
LOSS train 0.14084778876059642 valid 0.2069035896622553
LOSS train 0.14084778876059642 valid 0.20704412803505406
LOSS train 0.14084778876059642 valid 0.2069551333425993
LOSS train 0.14084778876059642 valid 0.20706531003920617
LOSS train 0.14084778876059642 valid 0.20669783669568242
LOSS train 0.14084778876059642 valid 0.2065276220529037
LOSS train 0.14084778876059642 valid 0.20635763336630428
LOSS train 0.14084778876059642 valid 0.20617770390552387
LOSS train 0.14084778876059642 valid 0.2062742214563281
LOSS train 0.14084778876059642 valid 0.2063754395597932
LOSS train 0.14084778876059642 valid 0.20639221130431384
LOSS train 0.14084778876059642 valid 0.20609639278479985
LOSS train 0.14084778876059642 valid 0.20610654328695752
LOSS train 0.14084778876059642 valid 0.20595789309275353
LOSS train 0.14084778876059642 valid 0.20598101348019718
LOSS train 0.14084778876059642 valid 0.20598101715801814
LOSS train 0.14084778876059642 valid 0.20596478606263796
LOSS train 0.14084778876059642 valid 0.20583811402320862
LOSS train 0.14084778876059642 valid 0.20572456684741344
LOSS train 0.14084778876059642 valid 0.2058817840339056
LOSS train 0.14084778876059642 valid 0.2059201207821784
LOSS train 0.14084778876059642 valid 0.20598436151001903
LOSS train 0.14084778876059642 valid 0.20618825774359448
LOSS train 0.14084778876059642 valid 0.20613899229363325
LOSS train 0.14084778876059642 valid 0.2060890758132681
LOSS train 0.14084778876059642 valid 0.2057574136704995
LOSS train 0.14084778876059642 valid 0.20572156851228915
LOSS train 0.14084778876059642 valid 0.2058330110386404
LOSS train 0.14084778876059642 valid 0.20582671688559154
LOSS train 0.14084778876059642 valid 0.2055694633042874
LOSS train 0.14084778876059642 valid 0.20566109782948935
LOSS train 0.14084778876059642 valid 0.2055751227415525
LOSS train 0.14084778876059642 valid 0.20578979670393224
LOSS train 0.14084778876059642 valid 0.2059525705987427
LOSS train 0.14084778876059642 valid 0.20602273542170574
LOSS train 0.14084778876059642 valid 0.2060919328400837
LOSS train 0.14084778876059642 valid 0.20628567904233933
LOSS train 0.14084778876059642 valid 0.20632173261832243
LOSS train 0.14084778876059642 valid 0.20650470891211292
LOSS train 0.14084778876059642 valid 0.2066387934197346
LOSS train 0.14084778876059642 valid 0.2064343342447982
LOSS train 0.14084778876059642 valid 0.2062681835599062
LOSS train 0.14084778876059642 valid 0.2062117471133621
LOSS train 0.14084778876059642 valid 0.2060599606394192
LOSS train 0.14084778876059642 valid 0.2058660022627849
LOSS train 0.14084778876059642 valid 0.20589854685883774
LOSS train 0.14084778876059642 valid 0.2061495751142502
LOSS train 0.14084778876059642 valid 0.20642630946579701
LOSS train 0.14084778876059642 valid 0.20626615971889137
LOSS train 0.14084778876059642 valid 0.2062346530912068
LOSS train 0.14084778876059642 valid 0.20619151337403002
LOSS train 0.14084778876059642 valid 0.205921520535336
LOSS train 0.14084778876059642 valid 0.20568460619284046
LOSS train 0.14084778876059642 valid 0.20577519156965793
LOSS train 0.14084778876059642 valid 0.20570371443525368
LOSS train 0.14084778876059642 valid 0.20582039252808104
LOSS train 0.14084778876059642 valid 0.20575914511626417
LOSS train 0.14084778876059642 valid 0.20572363211019007
LOSS train 0.14084778876059642 valid 0.20586678258202099
LOSS train 0.14084778876059642 valid 0.2057263112789847
LOSS train 0.14084778876059642 valid 0.205604671168008
LOSS train 0.14084778876059642 valid 0.2052993408176634
LOSS train 0.14084778876059642 valid 0.20510536478420274
LOSS train 0.14084778876059642 valid 0.20510017891549853
LOSS train 0.14084778876059642 valid 0.20515327495441102
LOSS train 0.14084778876059642 valid 0.20509991622387583
LOSS train 0.14084778876059642 valid 0.20523565534664237
LOSS train 0.14084778876059642 valid 0.20528938727719442
LOSS train 0.14084778876059642 valid 0.20548441216092686
LOSS train 0.14084778876059642 valid 0.2056130162508191
LOSS train 0.14084778876059642 valid 0.20557677701242968
LOSS train 0.14084778876059642 valid 0.20554336765979198
LOSS train 0.14084778876059642 valid 0.20531702167907004
LOSS train 0.14084778876059642 valid 0.2053592076155707
LOSS train 0.14084778876059642 valid 0.2054192741878894
LOSS train 0.14084778876059642 valid 0.20541683176072573
LOSS train 0.14084778876059642 valid 0.20501521763702232
LOSS train 0.14084778876059642 valid 0.20493453332992015
LOSS train 0.14084778876059642 valid 0.20501375561657031
LOSS train 0.14084778876059642 valid 0.20506393106140716
LOSS train 0.14084778876059642 valid 0.20511318712693746
LOSS train 0.14084778876059642 valid 0.20529036278627358
LOSS train 0.14084778876059642 valid 0.2054073312660543
LOSS train 0.14084778876059642 valid 0.20541118929984598
LOSS train 0.14084778876059642 valid 0.20533801074470243
LOSS train 0.14084778876059642 valid 0.20523587862650552
LOSS train 0.14084778876059642 valid 0.2053577493429184
LOSS train 0.14084778876059642 valid 0.20545021886844558
LOSS train 0.14084778876059642 valid 0.2054930037804066
LOSS train 0.14084778876059642 valid 0.2053871425126381
LOSS train 0.14084778876059642 valid 0.20545967779760285
LOSS train 0.14084778876059642 valid 0.20546246767044068
LOSS train 0.14084778876059642 valid 0.20549829467199743
LOSS train 0.14084778876059642 valid 0.20539598746754317
LOSS train 0.14084778876059642 valid 0.20549687188725138
LOSS train 0.14084778876059642 valid 0.20543895316630256
LOSS train 0.14084778876059642 valid 0.2053713793364855
LOSS train 0.14084778876059642 valid 0.2053577826283444
LOSS train 0.14084778876059642 valid 0.2056297707079931
LOSS train 0.14084778876059642 valid 0.20550490490837242
LOSS train 0.14084778876059642 valid 0.20549643485609329
LOSS train 0.14084778876059642 valid 0.20557975279835036
LOSS train 0.14084778876059642 valid 0.20566459179372715
LOSS train 0.14084778876059642 valid 0.2056276456135489
LOSS train 0.14084778876059642 valid 0.20565646480935723
LOSS train 0.14084778876059642 valid 0.20573160633943338
LOSS train 0.14084778876059642 valid 0.20557650515326747
LOSS train 0.14084778876059642 valid 0.20549006061800293
LOSS train 0.14084778876059642 valid 0.20531163337256977
LOSS train 0.14084778876059642 valid 0.20517375699548057
LOSS train 0.14084778876059642 valid 0.205131391484807
LOSS train 0.14084778876059642 valid 0.20522748611190103
LOSS train 0.14084778876059642 valid 0.2053682196183481
LOSS train 0.14084778876059642 valid 0.20527238288511007
LOSS train 0.14084778876059642 valid 0.20525279930598445
LOSS train 0.14084778876059642 valid 0.20533605208320002
LOSS train 0.14084778876059642 valid 0.20513637390519893
LOSS train 0.14084778876059642 valid 0.20499816080853608
LOSS train 0.14084778876059642 valid 0.2049451416477244
LOSS train 0.14084778876059642 valid 0.20492184762398682
LOSS train 0.14084778876059642 valid 0.2047400869114298
LOSS train 0.14084778876059642 valid 0.2048497379348989
LOSS train 0.14084778876059642 valid 0.20484137175591677
LOSS train 0.14084778876059642 valid 0.20497627904190835
LOSS train 0.14084778876059642 valid 0.204930543847796
LOSS train 0.14084778876059642 valid 0.20490360327039212
LOSS train 0.14084778876059642 valid 0.20479137568638242
LOSS train 0.14084778876059642 valid 0.2047605815109928
LOSS train 0.14084778876059642 valid 0.2047503869215103
LOSS train 0.14084778876059642 valid 0.20470332356850035
LOSS train 0.14084778876059642 valid 0.20468961645145806
LOSS train 0.14084778876059642 valid 0.20468277638241397
LOSS train 0.14084778876059642 valid 0.20471691143875187
LOSS train 0.14084778876059642 valid 0.2047410290249269
LOSS train 0.14084778876059642 valid 0.2047559087908508
LOSS train 0.14084778876059642 valid 0.20477786600390405
LOSS train 0.14084778876059642 valid 0.20478563994169235
LOSS train 0.14084778876059642 valid 0.2047567702211019
LOSS train 0.14084778876059642 valid 0.2047404099181788
LOSS train 0.14084778876059642 valid 0.20477634919161844
LOSS train 0.14084778876059642 valid 0.2047342103660891
LOSS train 0.14084778876059642 valid 0.20461888298636577
LOSS train 0.14084778876059642 valid 0.20475715988017376
LOSS train 0.14084778876059642 valid 0.20466063081829866
LOSS train 0.14084778876059642 valid 0.20475752545254572
LOSS train 0.14084778876059642 valid 0.20478154665829681
LOSS train 0.14084778876059642 valid 0.20481409199776188
LOSS train 0.14084778876059642 valid 0.20484530997621284
LOSS train 0.14084778876059642 valid 0.20481512170189467
LOSS train 0.14084778876059642 valid 0.20480665269370277
LOSS train 0.14084778876059642 valid 0.20480252660004197
LOSS train 0.14084778876059642 valid 0.20485210612652793
LOSS train 0.14084778876059642 valid 0.20484781939583488
LOSS train 0.14084778876059642 valid 0.2048214930183128
LOSS train 0.14084778876059642 valid 0.20489282601471967
LOSS train 0.14084778876059642 valid 0.2048934231450939
LOSS train 0.14084778876059642 valid 0.2047603728249669
LOSS train 0.14084778876059642 valid 0.2047672318613789
LOSS train 0.14084778876059642 valid 0.2046920932505442
LOSS train 0.14084778876059642 valid 0.2046548460751495
LOSS train 0.14084778876059642 valid 0.2046379141141603
LOSS train 0.14084778876059642 valid 0.20446396593864147
LOSS train 0.14084778876059642 valid 0.2045494223847711
LOSS train 0.14084778876059642 valid 0.20443548909203357
LOSS train 0.14084778876059642 valid 0.2044927793577677
LOSS train 0.14084778876059642 valid 0.20453463395134655
LOSS train 0.14084778876059642 valid 0.2046165073911349
LOSS train 0.14084778876059642 valid 0.20451469233208913
LOSS train 0.14084778876059642 valid 0.2045041541737246
LOSS train 0.14084778876059642 valid 0.20449997838195022
LOSS train 0.14084778876059642 valid 0.20444340486369447
LOSS train 0.14084778876059642 valid 0.2043070467995174
LOSS train 0.14084778876059642 valid 0.20422275062827838
LOSS train 0.14084778876059642 valid 0.20435328635688352
LOSS train 0.14084778876059642 valid 0.20430148851412994
LOSS train 0.14084778876059642 valid 0.2041932243337322
LOSS train 0.14084778876059642 valid 0.20423980403472394
LOSS train 0.14084778876059642 valid 0.20428604634800965
LOSS train 0.14084778876059642 valid 0.20431873437605405
LOSS train 0.14084778876059642 valid 0.2041833964190052
LOSS train 0.14084778876059642 valid 0.20429025975943998
LOSS train 0.14084778876059642 valid 0.20435459000476894
LOSS train 0.14084778876059642 valid 0.20426415574516174
LOSS train 0.14084778876059642 valid 0.20422487081127827
LOSS train 0.14084778876059642 valid 0.20420652880579576
LOSS train 0.14084778876059642 valid 0.20419132944335228
LOSS train 0.14084778876059642 valid 0.20423369743994305
LOSS train 0.14084778876059642 valid 0.20428440883288695
LOSS train 0.14084778876059642 valid 0.20431298555128954
LOSS train 0.14084778876059642 valid 0.20440059072910557
LOSS train 0.14084778876059642 valid 0.20427098794508788
LOSS train 0.14084778876059642 valid 0.20415062719667462
LOSS train 0.14084778876059642 valid 0.20412254743696598
LOSS train 0.14084778876059642 valid 0.20415884865766146
LOSS train 0.14084778876059642 valid 0.20427396814416907
LOSS train 0.14084778876059642 valid 0.20428060426851502
LOSS train 0.14084778876059642 valid 0.20456448574033048
LOSS train 0.14084778876059642 valid 0.2045162244186507
LOSS train 0.14084778876059642 valid 0.20467665799744222
LOSS train 0.14084778876059642 valid 0.2046942684604445
LOSS train 0.14084778876059642 valid 0.20459565684035586
LOSS train 0.14084778876059642 valid 0.20461338301227516
LOSS train 0.14084778876059642 valid 0.20460832200415147
LOSS train 0.14084778876059642 valid 0.2046528186557728
LOSS train 0.14084778876059642 valid 0.20451301307943853
LOSS train 0.14084778876059642 valid 0.20461768864939206
EPOCH 14:
  batch 1 loss: 0.1591336578130722
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.129937082529068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13051973779996237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.1299097090959549
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13293240666389466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13299139340718588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.1327297134058816
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1300064492970705
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.130839294857449
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.1290999859571457
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.1305817555297505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.1327218065659205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.1329389329140003
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.13088366921458924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1302793726325035
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13165434869006276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.13319851589553497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13465289233459365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13563268474842372
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13568173609673978
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13611684420279094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.13522324507886713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.13319633998300717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13301025672505298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.1330082306265831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.13534660494098297
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.1359945891631974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.13645681339715207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1375714979808906
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.13716698959469795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1367809020223156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.135980716207996
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.13601588638442935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13680894826264942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.13725581062691553
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.13846192902161014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.13941654462266612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1386691080896478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.137700909223312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.13809590935707092
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13788522416498603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13786224275827408
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13789280140122703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.1380429332229224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.1378417581319809
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13777284680501276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1375367501948742
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.13826979820926985
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13841783270543936
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13826132833957672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.138019263159995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1375827926855821
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.13769367758957846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13781591146080582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.1380280310457403
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.13845816307834216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13878419681599266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13835522018629928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13919148606769108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13994218508402506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14057470149681217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.1401266502516885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.13962034632762274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13960247521754354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13976921702806766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.13950083935351082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13976026370899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13997635146712556
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13961673808702524
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.13921818978020123
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13927473411173888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13892810160501134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.13922862861662694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.1396497531315765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13958164344231289
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13960310218757704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13979404380956253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.14002593425221932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.14015076083095768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.14030981091782452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.1402759004706218
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.14042087671596828
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.1402637937880424
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.14057915702107407
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.14068395591834013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.14024001091372135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.1403902095110937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.14029990500685843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.14043433838680888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1404636955095662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.14045864466454958
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.14015122349171535
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1401899978518486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.14010686213348775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13990406668499897
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13987179356627166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.1398339843780724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13971979163435042
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.1400580790759337
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1401061477512121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.140125070541802
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.14013016815571225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1400490432833005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.14025670984903207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.14023301949103673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.14027668697372922
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.14027733105086834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.14018307315806547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.14025951764725764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.14045728221535683
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.14032327276360881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.1400991814610149
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.14001224361426007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.1400733243085836
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.14017225328994834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.14028252102434635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.14032745087503368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.14022392242894335
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.14037067242780654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.1403208425268531
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.14043568659666156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.14051045825491187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.14045870261705987
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.1404101881649225
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.1405764407515526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.14053686903346152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14069292694330215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.14062574604758993
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.14046832289575606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.14040303430878198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.14036663553414455
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.14003886158267656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.14001081234082244
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.1397848590763647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1397505211609381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.14001670709865935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.1400976880405941
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.14005349760038266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.14021715650455557
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.1400663024080651
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.14017118561141034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.14025642339822272
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.14024814926869386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.14002923605342707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.14022675485446534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.14024274418615315
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.14004022582453124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.140226822749183
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.14042311916815353
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.14037955005963643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.14050488065410133
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.14061395314178968
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.1407926540359173
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.14072450085893853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14067022819672861
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1406774234313231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.14062494703918504
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.14065312160344062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14067603760170486
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.14059444097802043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.14054316559933727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14071040628132997
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14076055165814474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.1406789715515404
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.14047626767194632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.14031536939991526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.14033389778551228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.14017275060039192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.14004897193972177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.14013797101729056
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.14017629017781097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.14007398785026962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13997625843810207
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1400334295099494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.14003543083156858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.14009548651731826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.1404532766190626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.14040663867686573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.14049183169390236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.14059233578542868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.1406328728762121
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14070767569509182
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.14077292191363422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.14071467190818943
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1406808312277536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.1407319962257339
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.14081896423177923
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14068094296182723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.14075362773010971
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14077050713332076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14074191462775176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14100251412795237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.14097607440744658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14098120853304863
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14097964545855154
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14088317641646278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.1406213777697631
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14059010497999913
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.1404331933388758
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14058402616530657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.14054012999160967
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.14052899650270395
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.1406187494033076
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1405964115013679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.1404906798426698
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.14059378488839253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.14043364076366746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1402950479577367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.14014040635961095
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.14028683719890458
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.14032070638867916
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.14053406120049503
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14065224327531778
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14064280239638882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.14058956791495167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.14060585901003192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.14061838158402026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.14061994498612684
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14071503604792024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.14050231565805998
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.14057052118611013
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.14055376765024555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.1406638346645864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.14062289763907238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.14068084044588935
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.14063196850165857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.14075680756490136
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.1406538247277862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.14055864828512657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.14060242470839748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.14062478444922022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.14063584133340368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.1405663325615195
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.14054686770352542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.14044997238732398
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.14049989777475091
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.14044951553208918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.14057642769287615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.1404605320183303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.14041833182175953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.14038892158334185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.14041597359190303
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.14037758933663858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.14036648543398889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.14034604703893466
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.14018705765890882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.14020492926782924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.14031205470523528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1403071348925671
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.14035341036319732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.1403699075558271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.14034025152287785
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.14027678348094577
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.1402234449747979
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.14027129072769015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1403002687729895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.14018695450710417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.14013419456260148
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.14018890129323172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.14020110139480005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.14036625937712147
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1404448574735918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.14030649974772233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.1403251432559707
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.14036459461697992
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.14023400229731
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.14024375443266573
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.14032877517391495
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.14020965170793817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.14027807880882864
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1402119020089452
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.14013859922723734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.14005462104802605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.14009814633287654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13997709342024542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13989576899810977
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.1400151740396496
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.13995173501132205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.1398596340610135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.1399053707186665
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13984775057997142
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13985840524447726
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13979429340931215
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.1398054166946193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13997905717084283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13999302354413312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.14008216676707883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13999484569972587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.1398603314965654
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13984254105337734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13983970716646857
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13987728335881885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.1400112060658354
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.1399587300054881
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.14003419083053784
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1401344457188168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.14008122978587745
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.14020973208966672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.1402046037059165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.14011995422343412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.14007559465708527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.13993328407604172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.1399805395565804
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.14003461150844632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1400572216168779
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.14001553327916494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13993923667005298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13993610185268637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.14001719566803533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13999560638781516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13992662878760953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13985815584564057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13991340735373786
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13995422543898509
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.14013329415567338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.14013088869426069
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.14018221098447825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.14018169366432437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.14019864739202034
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.14015902287792414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.14014079027179616
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.14028896338173322
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.1401821114699538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.1401204294582576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.14021404676712476
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.140194591363332
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.14022063094814982
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.14021442510278487
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.14028351817359316
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.14021471285007217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.14017590472975527
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1402423565601369
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.14029061894964528
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.14029399597805417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.14026405164110126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.14032172662250342
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.1403225523030015
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.14024399350997965
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1401807516593497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.1402416779057068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.14026640755427547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.1402427036729124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.14028864844808078
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.14028275981112276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.14036821077267328
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.140333940678766
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.14041409585678613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.14042445492727318
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1403240191304581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.14036970517465047
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.14048409754903907
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.14044524331323124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.1404219376129064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.14053709173606613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1404901643034438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.14046101468834984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.14039478716062231
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.14033361359931237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.14031897939762364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.14031546113805637
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.14031828314561262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.14039745830122938
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.14038975592240815
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1403739517656984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.14039986984778757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.14042992216071795
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.1403429869572538
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1404279634842406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.14043891494513205
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.14052184127472542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.1405534904118818
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14052869832163217
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.14048374330390234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14053971013602087
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14054400078455606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14055571665472172
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.1406153049962274
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.14063790006927712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.14076128716047331
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14083169122275555
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14082682547293937
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14083439809957723
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.1408470707991727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14086510958926132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14079116867347197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14082115987035895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14072396506095733
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14079012497107393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.14075889089803156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14078369394708903
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.14086315822799492
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14087088252132646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.14088243718138177
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14085692369620206
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.14093261910013005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14088946785965953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14088767262114685
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.1408738951999039
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14089509454511462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14088437573984267
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14096712187267005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14104048724272358
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1410163295808856
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14100524836616352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.14102221537887313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14102842484421918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.1410176189620899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14094836606333652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14096941441880462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14096744218614043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14105264423754962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.141052779262361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.14096080307954736
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14086724429026895
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14087215611733586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14092721220535728
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14083699247748446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.14080383088528825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.14079196852474055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14075103861590227
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.14075811658576662
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14081825074073262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1407732309390467
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14082324065549193
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14082290547735551
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1407563135797429
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.14074801110970053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14080489029951185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14079131818178928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.1408436271340348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.14084586960948536
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14094829273030715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14095076994312278
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.14101532736818911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14101708558784135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14108096025542383
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.1411250102710942
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14117307947378724
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.14117542173976508
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14120675562457605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14120777170944646
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14118837042631607
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14122460681617124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.14119680239273621
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.1411441530906752
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14111206491765954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.1410847897797623
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1410736833599263
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.14108371102398382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14104813802573415
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.14104668226117834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.1410018983173423
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1409623564835679
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14087349363491924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14079375869625219
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14079011924434126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14078453626622256
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.14073315614175588
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14064599554133572
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14066693530134533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.14067042989094708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14062827149833435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.140609183254175
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.14068459139751463
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1406336598819302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.14066007950786868
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14069884600680185
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14070136408902642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14071039023048587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.140614205059853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14066217653057883
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14060744480624543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14060744480624543 valid 0.1909467577934265
LOSS train 0.14060744480624543 valid 0.21372833847999573
LOSS train 0.14060744480624543 valid 0.22628232836723328
LOSS train 0.14060744480624543 valid 0.22311348468065262
LOSS train 0.14060744480624543 valid 0.22131654918193816
LOSS train 0.14060744480624543 valid 0.2277414177854856
LOSS train 0.14060744480624543 valid 0.2224315745489938
LOSS train 0.14060744480624543 valid 0.22257118858397007
LOSS train 0.14060744480624543 valid 0.22225559088918898
LOSS train 0.14060744480624543 valid 0.22349954545497894
LOSS train 0.14060744480624543 valid 0.21965378387407822
LOSS train 0.14060744480624543 valid 0.22170939669013023
LOSS train 0.14060744480624543 valid 0.22022431515730345
LOSS train 0.14060744480624543 valid 0.220559723675251
LOSS train 0.14060744480624543 valid 0.2192677060763041
LOSS train 0.14060744480624543 valid 0.21946629136800766
LOSS train 0.14060744480624543 valid 0.2198683970114764
LOSS train 0.14060744480624543 valid 0.2198953719602691
LOSS train 0.14060744480624543 valid 0.22168188738195518
LOSS train 0.14060744480624543 valid 0.22131165638566017
LOSS train 0.14060744480624543 valid 0.22213705806505113
LOSS train 0.14060744480624543 valid 0.22024787623773923
LOSS train 0.14060744480624543 valid 0.21856679087099823
LOSS train 0.14060744480624543 valid 0.21927971144517264
LOSS train 0.14060744480624543 valid 0.21790865421295166
LOSS train 0.14060744480624543 valid 0.21767063152331573
LOSS train 0.14060744480624543 valid 0.21740244163407219
LOSS train 0.14060744480624543 valid 0.21740640380552836
LOSS train 0.14060744480624543 valid 0.21618521110764866
LOSS train 0.14060744480624543 valid 0.21533310860395433
LOSS train 0.14060744480624543 valid 0.21447769528435123
LOSS train 0.14060744480624543 valid 0.21537130838260055
LOSS train 0.14060744480624543 valid 0.21406478637998755
LOSS train 0.14060744480624543 valid 0.21316756483386545
LOSS train 0.14060744480624543 valid 0.2135798705475671
LOSS train 0.14060744480624543 valid 0.21365124566687477
LOSS train 0.14060744480624543 valid 0.21257613840940837
LOSS train 0.14060744480624543 valid 0.2124757747116842
LOSS train 0.14060744480624543 valid 0.21212497926675355
LOSS train 0.14060744480624543 valid 0.211715554818511
LOSS train 0.14060744480624543 valid 0.2116859010080012
LOSS train 0.14060744480624543 valid 0.21286358010201228
LOSS train 0.14060744480624543 valid 0.21376281422238017
LOSS train 0.14060744480624543 valid 0.2128265036100691
LOSS train 0.14060744480624543 valid 0.2120674732658598
LOSS train 0.14060744480624543 valid 0.21037687879541647
LOSS train 0.14060744480624543 valid 0.2096580133793202
LOSS train 0.14060744480624543 valid 0.2111415353914102
LOSS train 0.14060744480624543 valid 0.21109543527875627
LOSS train 0.14060744480624543 valid 0.21145934313535691
LOSS train 0.14060744480624543 valid 0.21120337791302624
LOSS train 0.14060744480624543 valid 0.21100988439642465
LOSS train 0.14060744480624543 valid 0.2119742010561925
LOSS train 0.14060744480624543 valid 0.2118561582432853
LOSS train 0.14060744480624543 valid 0.2121194682338021
LOSS train 0.14060744480624543 valid 0.21183296984859876
LOSS train 0.14060744480624543 valid 0.21115458482190183
LOSS train 0.14060744480624543 valid 0.21206028656712894
LOSS train 0.14060744480624543 valid 0.21229617625980055
LOSS train 0.14060744480624543 valid 0.21190592547257742
LOSS train 0.14060744480624543 valid 0.21167459131264296
LOSS train 0.14060744480624543 valid 0.211295282888797
LOSS train 0.14060744480624543 valid 0.21130745775169796
LOSS train 0.14060744480624543 valid 0.21116747078485787
LOSS train 0.14060744480624543 valid 0.21050267288318047
LOSS train 0.14060744480624543 valid 0.21045168353752655
LOSS train 0.14060744480624543 valid 0.21105132908073823
LOSS train 0.14060744480624543 valid 0.21036027985460617
LOSS train 0.14060744480624543 valid 0.21071521786675937
LOSS train 0.14060744480624543 valid 0.21112282212291444
LOSS train 0.14060744480624543 valid 0.21124527173143037
LOSS train 0.14060744480624543 valid 0.21173862533436882
LOSS train 0.14060744480624543 valid 0.21199452550443884
LOSS train 0.14060744480624543 valid 0.21207033419931257
LOSS train 0.14060744480624543 valid 0.21182344297568004
LOSS train 0.14060744480624543 valid 0.21203477151299777
LOSS train 0.14060744480624543 valid 0.2120178783094728
LOSS train 0.14060744480624543 valid 0.21200060290403855
LOSS train 0.14060744480624543 valid 0.21222659888901288
LOSS train 0.14060744480624543 valid 0.21203896421939133
LOSS train 0.14060744480624543 valid 0.21215699465922366
LOSS train 0.14060744480624543 valid 0.21244404283238622
LOSS train 0.14060744480624543 valid 0.2121237335075815
LOSS train 0.14060744480624543 valid 0.21197053009555453
LOSS train 0.14060744480624543 valid 0.21219420608352213
LOSS train 0.14060744480624543 valid 0.2122613884681879
LOSS train 0.14060744480624543 valid 0.2118857309393499
LOSS train 0.14060744480624543 valid 0.21178509836847131
LOSS train 0.14060744480624543 valid 0.2119321094804935
LOSS train 0.14060744480624543 valid 0.21170313027169968
LOSS train 0.14060744480624543 valid 0.21180399499096714
LOSS train 0.14060744480624543 valid 0.21183565372358198
LOSS train 0.14060744480624543 valid 0.21214050887733377
LOSS train 0.14060744480624543 valid 0.2125746208302518
LOSS train 0.14060744480624543 valid 0.2130332720907111
LOSS train 0.14060744480624543 valid 0.2139463114241759
LOSS train 0.14060744480624543 valid 0.21412560743154938
LOSS train 0.14060744480624543 valid 0.2146993027049668
LOSS train 0.14060744480624543 valid 0.21454222319704114
LOSS train 0.14060744480624543 valid 0.214846820384264
LOSS train 0.14060744480624543 valid 0.21477287845446333
LOSS train 0.14060744480624543 valid 0.21529417660306482
LOSS train 0.14060744480624543 valid 0.21486930693816214
LOSS train 0.14060744480624543 valid 0.214664379421335
LOSS train 0.14060744480624543 valid 0.21485972106456758
LOSS train 0.14060744480624543 valid 0.21478791267804379
LOSS train 0.14060744480624543 valid 0.21458333591434442
LOSS train 0.14060744480624543 valid 0.21457614225369911
LOSS train 0.14060744480624543 valid 0.2139971023852672
LOSS train 0.14060744480624543 valid 0.21368806727907874
LOSS train 0.14060744480624543 valid 0.21367120930740424
LOSS train 0.14060744480624543 valid 0.21389318417225564
LOSS train 0.14060744480624543 valid 0.2136476871977865
LOSS train 0.14060744480624543 valid 0.21384450048208237
LOSS train 0.14060744480624543 valid 0.2142700538687084
LOSS train 0.14060744480624543 valid 0.214227013546845
LOSS train 0.14060744480624543 valid 0.21423444712263906
LOSS train 0.14060744480624543 valid 0.21425277373548282
LOSS train 0.14060744480624543 valid 0.214042647546079
LOSS train 0.14060744480624543 valid 0.21406528527537982
LOSS train 0.14060744480624543 valid 0.21407388852647513
LOSS train 0.14060744480624543 valid 0.2141038233139476
LOSS train 0.14060744480624543 valid 0.21408977838066534
LOSS train 0.14060744480624543 valid 0.21420895560614525
LOSS train 0.14060744480624543 valid 0.21447854673862457
LOSS train 0.14060744480624543 valid 0.21432835821594512
LOSS train 0.14060744480624543 valid 0.2141498221656469
LOSS train 0.14060744480624543 valid 0.21405719313770533
LOSS train 0.14060744480624543 valid 0.21396404100480929
LOSS train 0.14060744480624543 valid 0.21373604994553785
LOSS train 0.14060744480624543 valid 0.21381896950361381
LOSS train 0.14060744480624543 valid 0.21401219623106899
LOSS train 0.14060744480624543 valid 0.2139697795299659
LOSS train 0.14060744480624543 valid 0.21399549105718954
LOSS train 0.14060744480624543 valid 0.2140472247644707
LOSS train 0.14060744480624543 valid 0.21426101905458114
LOSS train 0.14060744480624543 valid 0.214080296946268
LOSS train 0.14060744480624543 valid 0.2136634516975154
LOSS train 0.14060744480624543 valid 0.2136556375369751
LOSS train 0.14060744480624543 valid 0.21348047054239683
LOSS train 0.14060744480624543 valid 0.21376200548723234
LOSS train 0.14060744480624543 valid 0.2139828199861755
LOSS train 0.14060744480624543 valid 0.2140862086227724
LOSS train 0.14060744480624543 valid 0.2140054480276174
LOSS train 0.14060744480624543 valid 0.21381433503381136
LOSS train 0.14060744480624543 valid 0.2138218628625347
LOSS train 0.14060744480624543 valid 0.21377959026365864
LOSS train 0.14060744480624543 valid 0.2135450526870586
LOSS train 0.14060744480624543 valid 0.21376505984155925
LOSS train 0.14060744480624543 valid 0.2136674129962921
LOSS train 0.14060744480624543 valid 0.21356274651375828
LOSS train 0.14060744480624543 valid 0.21362282002442762
LOSS train 0.14060744480624543 valid 0.2133836348851522
LOSS train 0.14060744480624543 valid 0.2134097460415456
LOSS train 0.14060744480624543 valid 0.21319652997678326
LOSS train 0.14060744480624543 valid 0.2133846996495357
LOSS train 0.14060744480624543 valid 0.21319064478965322
LOSS train 0.14060744480624543 valid 0.21308785252556017
LOSS train 0.14060744480624543 valid 0.21340814185967236
LOSS train 0.14060744480624543 valid 0.2133397836238146
LOSS train 0.14060744480624543 valid 0.2135232125194917
LOSS train 0.14060744480624543 valid 0.213536548393744
LOSS train 0.14060744480624543 valid 0.21331530041489866
LOSS train 0.14060744480624543 valid 0.21328559436085748
LOSS train 0.14060744480624543 valid 0.21343183643890148
LOSS train 0.14060744480624543 valid 0.21334173718848862
LOSS train 0.14060744480624543 valid 0.21344778530611963
LOSS train 0.14060744480624543 valid 0.21307944098398798
LOSS train 0.14060744480624543 valid 0.21290396965114322
LOSS train 0.14060744480624543 valid 0.21272854506969452
LOSS train 0.14060744480624543 valid 0.2125465661635873
LOSS train 0.14060744480624543 valid 0.2126492426492447
LOSS train 0.14060744480624543 valid 0.2127528466241208
LOSS train 0.14060744480624543 valid 0.21277684804009295
LOSS train 0.14060744480624543 valid 0.21247212665421622
LOSS train 0.14060744480624543 valid 0.21247206585989756
LOSS train 0.14060744480624543 valid 0.2123171499388366
LOSS train 0.14060744480624543 valid 0.21234553407752113
LOSS train 0.14060744480624543 valid 0.21234347134329087
LOSS train 0.14060744480624543 valid 0.21232543538014095
LOSS train 0.14060744480624543 valid 0.21219606368251928
LOSS train 0.14060744480624543 valid 0.2120852605490894
LOSS train 0.14060744480624543 valid 0.21225579994949487
LOSS train 0.14060744480624543 valid 0.21229845479778622
LOSS train 0.14060744480624543 valid 0.21234763055234343
LOSS train 0.14060744480624543 valid 0.21254763856369963
LOSS train 0.14060744480624543 valid 0.21249618990854782
LOSS train 0.14060744480624543 valid 0.21245343166779965
LOSS train 0.14060744480624543 valid 0.21211670197191693
LOSS train 0.14060744480624543 valid 0.2120875258194773
LOSS train 0.14060744480624543 valid 0.21220614078469302
LOSS train 0.14060744480624543 valid 0.2122059618898978
LOSS train 0.14060744480624543 valid 0.2119438102362687
LOSS train 0.14060744480624543 valid 0.21203732098807992
LOSS train 0.14060744480624543 valid 0.21194352828539334
LOSS train 0.14060744480624543 valid 0.21216293150673107
LOSS train 0.14060744480624543 valid 0.21233201473194935
LOSS train 0.14060744480624543 valid 0.21241212560973985
LOSS train 0.14060744480624543 valid 0.2124767455473617
LOSS train 0.14060744480624543 valid 0.21266001716256142
LOSS train 0.14060744480624543 valid 0.21268141744148672
LOSS train 0.14060744480624543 valid 0.21286849839852587
LOSS train 0.14060744480624543 valid 0.21300926224645136
LOSS train 0.14060744480624543 valid 0.212802931812464
LOSS train 0.14060744480624543 valid 0.21263090183095235
LOSS train 0.14060744480624543 valid 0.21257724576783413
LOSS train 0.14060744480624543 valid 0.21241826610864648
LOSS train 0.14060744480624543 valid 0.21222085340951496
LOSS train 0.14060744480624543 valid 0.21224712263168902
LOSS train 0.14060744480624543 valid 0.21249888957965943
LOSS train 0.14060744480624543 valid 0.21278987980284397
LOSS train 0.14060744480624543 valid 0.21262093802105705
LOSS train 0.14060744480624543 valid 0.21258145752647115
LOSS train 0.14060744480624543 valid 0.21253298849702995
LOSS train 0.14060744480624543 valid 0.21225698770478715
LOSS train 0.14060744480624543 valid 0.21202194890766232
LOSS train 0.14060744480624543 valid 0.21211871532251209
LOSS train 0.14060744480624543 valid 0.21204886441930718
LOSS train 0.14060744480624543 valid 0.2121799163622399
LOSS train 0.14060744480624543 valid 0.21212332167408682
LOSS train 0.14060744480624543 valid 0.21207947051363293
LOSS train 0.14060744480624543 valid 0.2122210371601689
LOSS train 0.14060744480624543 valid 0.21208704266312944
LOSS train 0.14060744480624543 valid 0.2119666430433946
LOSS train 0.14060744480624543 valid 0.21165724893411
LOSS train 0.14060744480624543 valid 0.21146674988280356
LOSS train 0.14060744480624543 valid 0.21145885854565624
LOSS train 0.14060744480624543 valid 0.21150836466174378
LOSS train 0.14060744480624543 valid 0.21145240689208936
LOSS train 0.14060744480624543 valid 0.21158088866783226
LOSS train 0.14060744480624543 valid 0.21164398998409123
LOSS train 0.14060744480624543 valid 0.2118429509473258
LOSS train 0.14060744480624543 valid 0.21197784190014196
LOSS train 0.14060744480624543 valid 0.21194319261444938
LOSS train 0.14060744480624543 valid 0.21191730511949416
LOSS train 0.14060744480624543 valid 0.21168650396294514
LOSS train 0.14060744480624543 valid 0.21172476619607788
LOSS train 0.14060744480624543 valid 0.2117768536720957
LOSS train 0.14060744480624543 valid 0.21177581495071554
LOSS train 0.14060744480624543 valid 0.21136173484846948
LOSS train 0.14060744480624543 valid 0.21126020402952844
LOSS train 0.14060744480624543 valid 0.21133982547181696
LOSS train 0.14060744480624543 valid 0.21139679220600874
LOSS train 0.14060744480624543 valid 0.2114447668255841
LOSS train 0.14060744480624543 valid 0.21162047231075715
LOSS train 0.14060744480624543 valid 0.21174451808740452
LOSS train 0.14060744480624543 valid 0.21174338107046328
LOSS train 0.14060744480624543 valid 0.211653538258566
LOSS train 0.14060744480624543 valid 0.2115534391688056
LOSS train 0.14060744480624543 valid 0.21168430259823798
LOSS train 0.14060744480624543 valid 0.2117702732107554
LOSS train 0.14060744480624543 valid 0.21181449166957347
LOSS train 0.14060744480624543 valid 0.2116985702997611
LOSS train 0.14060744480624543 valid 0.2117758582485473
LOSS train 0.14060744480624543 valid 0.21177598018856608
LOSS train 0.14060744480624543 valid 0.21180683522834443
LOSS train 0.14060744480624543 valid 0.21170438110016662
LOSS train 0.14060744480624543 valid 0.21180766650526098
LOSS train 0.14060744480624543 valid 0.2117523563932268
LOSS train 0.14060744480624543 valid 0.21168864979766883
LOSS train 0.14060744480624543 valid 0.21167141190548053
LOSS train 0.14060744480624543 valid 0.21194773812899154
LOSS train 0.14060744480624543 valid 0.21181883447637123
LOSS train 0.14060744480624543 valid 0.2118025248531591
LOSS train 0.14060744480624543 valid 0.2118811214026415
LOSS train 0.14060744480624543 valid 0.21195734323079424
LOSS train 0.14060744480624543 valid 0.2119058682286784
LOSS train 0.14060744480624543 valid 0.21193598658402465
LOSS train 0.14060744480624543 valid 0.21201590094092168
LOSS train 0.14060744480624543 valid 0.21186284039307524
LOSS train 0.14060744480624543 valid 0.21177145720517943
LOSS train 0.14060744480624543 valid 0.21158660414135633
LOSS train 0.14060744480624543 valid 0.21145082439804253
LOSS train 0.14060744480624543 valid 0.211414384227382
LOSS train 0.14060744480624543 valid 0.21151556277816946
LOSS train 0.14060744480624543 valid 0.21165349056431348
LOSS train 0.14060744480624543 valid 0.21155840822828376
LOSS train 0.14060744480624543 valid 0.21153540665404402
LOSS train 0.14060744480624543 valid 0.21162087865520976
LOSS train 0.14060744480624543 valid 0.21140690113284757
LOSS train 0.14060744480624543 valid 0.21126656438426192
LOSS train 0.14060744480624543 valid 0.2112184960844246
LOSS train 0.14060744480624543 valid 0.21118493264955682
LOSS train 0.14060744480624543 valid 0.21100110154021795
LOSS train 0.14060744480624543 valid 0.21110874134720417
LOSS train 0.14060744480624543 valid 0.2111003966546142
LOSS train 0.14060744480624543 valid 0.21123701725998822
LOSS train 0.14060744480624543 valid 0.21119531050013998
LOSS train 0.14060744480624543 valid 0.2111692845976064
LOSS train 0.14060744480624543 valid 0.21105736917976675
LOSS train 0.14060744480624543 valid 0.2110301642534659
LOSS train 0.14060744480624543 valid 0.21101831290105436
LOSS train 0.14060744480624543 valid 0.21097393605997944
LOSS train 0.14060744480624543 valid 0.21096054902997147
LOSS train 0.14060744480624543 valid 0.21095895961684696
LOSS train 0.14060744480624543 valid 0.21098549137949138
LOSS train 0.14060744480624543 valid 0.2110171925891128
LOSS train 0.14060744480624543 valid 0.21103201334008434
LOSS train 0.14060744480624543 valid 0.21106166492238093
LOSS train 0.14060744480624543 valid 0.2110707232107719
LOSS train 0.14060744480624543 valid 0.21104194810521168
LOSS train 0.14060744480624543 valid 0.21102825179696083
LOSS train 0.14060744480624543 valid 0.2110611184349548
LOSS train 0.14060744480624543 valid 0.21101770606382111
LOSS train 0.14060744480624543 valid 0.21090743094682693
LOSS train 0.14060744480624543 valid 0.21104122613087978
LOSS train 0.14060744480624543 valid 0.21095050990290284
LOSS train 0.14060744480624543 valid 0.21104941543723857
LOSS train 0.14060744480624543 valid 0.21106853982295032
LOSS train 0.14060744480624543 valid 0.2111036932997165
LOSS train 0.14060744480624543 valid 0.21112856534803795
LOSS train 0.14060744480624543 valid 0.21109639855627066
LOSS train 0.14060744480624543 valid 0.21109224920169994
LOSS train 0.14060744480624543 valid 0.211091337808568
LOSS train 0.14060744480624543 valid 0.2111392018577409
LOSS train 0.14060744480624543 valid 0.21113725650253928
LOSS train 0.14060744480624543 valid 0.2111121794261767
LOSS train 0.14060744480624543 valid 0.2111884781727626
LOSS train 0.14060744480624543 valid 0.21118582385741058
LOSS train 0.14060744480624543 valid 0.2110519747948274
LOSS train 0.14060744480624543 valid 0.21106004290213096
LOSS train 0.14060744480624543 valid 0.21098328189727683
LOSS train 0.14060744480624543 valid 0.21094834912948934
LOSS train 0.14060744480624543 valid 0.21093142674201065
LOSS train 0.14060744480624543 valid 0.21075534492731093
LOSS train 0.14060744480624543 valid 0.21084151524433328
LOSS train 0.14060744480624543 valid 0.21072419550531865
LOSS train 0.14060744480624543 valid 0.21077620912706707
LOSS train 0.14060744480624543 valid 0.21081771841980404
LOSS train 0.14060744480624543 valid 0.2108926217890147
LOSS train 0.14060744480624543 valid 0.21078500378978215
LOSS train 0.14060744480624543 valid 0.21077266773096767
LOSS train 0.14060744480624543 valid 0.21077298686221554
LOSS train 0.14060744480624543 valid 0.21071531322455692
LOSS train 0.14060744480624543 valid 0.21057987028538291
LOSS train 0.14060744480624543 valid 0.21049446712381073
LOSS train 0.14060744480624543 valid 0.21062883188621934
LOSS train 0.14060744480624543 valid 0.21057373062657886
LOSS train 0.14060744480624543 valid 0.21046420323022347
LOSS train 0.14060744480624543 valid 0.2105107536429868
LOSS train 0.14060744480624543 valid 0.21055760094759401
LOSS train 0.14060744480624543 valid 0.2105929092749169
LOSS train 0.14060744480624543 valid 0.21045852052835265
LOSS train 0.14060744480624543 valid 0.2105593889042042
LOSS train 0.14060744480624543 valid 0.21062157013709995
LOSS train 0.14060744480624543 valid 0.2105293909378479
LOSS train 0.14060744480624543 valid 0.21048701326939834
LOSS train 0.14060744480624543 valid 0.2104703687431141
LOSS train 0.14060744480624543 valid 0.21045514943350352
LOSS train 0.14060744480624543 valid 0.21049292996525765
LOSS train 0.14060744480624543 valid 0.210549676498966
LOSS train 0.14060744480624543 valid 0.21057636254805734
LOSS train 0.14060744480624543 valid 0.21066230415200377
LOSS train 0.14060744480624543 valid 0.2105319137753403
LOSS train 0.14060744480624543 valid 0.21040348589000568
LOSS train 0.14060744480624543 valid 0.21037129252060746
LOSS train 0.14060744480624543 valid 0.21040912417053176
LOSS train 0.14060744480624543 valid 0.21052838310432834
LOSS train 0.14060744480624543 valid 0.21053576230753762
LOSS train 0.14060744480624543 valid 0.2108269634553128
LOSS train 0.14060744480624543 valid 0.21077987512475566
LOSS train 0.14060744480624543 valid 0.21093863020368045
LOSS train 0.14060744480624543 valid 0.21096071442871384
LOSS train 0.14060744480624543 valid 0.21085856983867976
LOSS train 0.14060744480624543 valid 0.21087263338778117
LOSS train 0.14060744480624543 valid 0.2108647619155261
LOSS train 0.14060744480624543 valid 0.21090797192800273
LOSS train 0.14060744480624543 valid 0.21076600737464818
LOSS train 0.14060744480624543 valid 0.21086816944000197
EPOCH 15:
  batch 1 loss: 0.15836599469184875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 2 loss: 0.1314675733447075
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 3 loss: 0.13314476112524667
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 4 loss: 0.131570003926754
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 5 loss: 0.13481866121292113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 6 loss: 0.13423310220241547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 7 loss: 0.13368960363524302
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 8 loss: 0.1311250552535057
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 9 loss: 0.13161102268430921
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 10 loss: 0.12940187007188797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 11 loss: 0.13092729449272156
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 12 loss: 0.13284397373596826
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 13 loss: 0.13337248334517846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 14 loss: 0.131008295076234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 15 loss: 0.1305592894554138
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 16 loss: 0.13209422677755356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 17 loss: 0.1335727432194878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 18 loss: 0.13510588804880777
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 19 loss: 0.13590764215118006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 20 loss: 0.13607001155614853
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 21 loss: 0.13701691301096053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 22 loss: 0.1362602263689041
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 23 loss: 0.1341598354603933
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 24 loss: 0.13393979612737894
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 25 loss: 0.13383495897054673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 26 loss: 0.1359428451038324
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 27 loss: 0.136530678305361
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 28 loss: 0.1369210042591606
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 29 loss: 0.1378346267959167
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 30 loss: 0.1373544176419576
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 31 loss: 0.1367789718412584
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 32 loss: 0.13603438925929368
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 33 loss: 0.1359694137266188
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 34 loss: 0.13687167689204216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 35 loss: 0.1374316060117313
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 36 loss: 0.1386424143695169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 37 loss: 0.1396128134550275
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 38 loss: 0.1388643058507066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 39 loss: 0.1379240422676771
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 40 loss: 0.1385248616337776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 41 loss: 0.13839552496991506
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 42 loss: 0.13830746390989848
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 43 loss: 0.13827045469783072
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 44 loss: 0.13841872696172108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 45 loss: 0.13809004608127806
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 46 loss: 0.13789542748228364
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 47 loss: 0.1376027009905653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 48 loss: 0.1381607196914653
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 49 loss: 0.13833365771843462
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 50 loss: 0.13818396970629693
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 51 loss: 0.1378957662804454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 52 loss: 0.1372988956192365
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 53 loss: 0.1372970757338236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 54 loss: 0.13751734613820357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 55 loss: 0.13780607919801366
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 56 loss: 0.138127373930599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 57 loss: 0.13842318050171198
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 58 loss: 0.13793111836601948
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 59 loss: 0.13861933438959767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 60 loss: 0.13934493648509186
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 61 loss: 0.14003934271511484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 62 loss: 0.13956793181357846
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 63 loss: 0.1391896361869479
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 64 loss: 0.13915712456218898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 65 loss: 0.13922371222422672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 66 loss: 0.1390121407581098
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 67 loss: 0.13927942825787104
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 68 loss: 0.13949070739395478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 69 loss: 0.13912515201862308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 70 loss: 0.1387461800660406
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 71 loss: 0.13879077934043507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 72 loss: 0.13842713532762396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 73 loss: 0.1386367208133005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 74 loss: 0.13908690022858414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 75 loss: 0.13897796243429184
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 76 loss: 0.13893345615973599
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 77 loss: 0.13901765731635032
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 78 loss: 0.13917303992769656
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 79 loss: 0.13932115169642847
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 80 loss: 0.13946489607915283
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 81 loss: 0.13940484039945367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 82 loss: 0.13956507413489064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 83 loss: 0.13944141234619073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 84 loss: 0.13977859781256743
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 85 loss: 0.13988790240357904
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 86 loss: 0.13945665938216587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 87 loss: 0.13955016143020543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 88 loss: 0.13953264921226285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 89 loss: 0.13964932570966443
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 90 loss: 0.1397513296869066
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 91 loss: 0.13971417258073995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 92 loss: 0.13944393809398878
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 93 loss: 0.1394484154479478
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 94 loss: 0.1394241915896852
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 95 loss: 0.13916751616879514
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 96 loss: 0.13918593262011805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 97 loss: 0.13909969025665953
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 98 loss: 0.13900828863285025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 99 loss: 0.13930110136667886
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 100 loss: 0.1392994675040245
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 101 loss: 0.13930596573518053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 102 loss: 0.1393506690567615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 103 loss: 0.1393878535159583
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 104 loss: 0.1396235297792233
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 105 loss: 0.13959125975767772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 106 loss: 0.1396254648296338
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 107 loss: 0.13960955285023305
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 108 loss: 0.13949821144342422
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 109 loss: 0.13956873023181882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 110 loss: 0.13977169787341898
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 111 loss: 0.13964811464150748
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 112 loss: 0.13942837954631873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 113 loss: 0.13933844350080574
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 114 loss: 0.13938302246102116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 115 loss: 0.1394441034482873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 116 loss: 0.1395392945871271
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 117 loss: 0.139642342670351
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 118 loss: 0.13954802476248498
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 119 loss: 0.13972221026901438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 120 loss: 0.13963655332724254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 121 loss: 0.1397774143159882
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 122 loss: 0.13981731443620118
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 123 loss: 0.1397754619034325
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 124 loss: 0.13976960593173582
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 125 loss: 0.13992195987701417
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 126 loss: 0.13987698547896885
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 127 loss: 0.14003483751627405
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 128 loss: 0.13999247993342578
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 129 loss: 0.13984907383835593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 130 loss: 0.1397757158256494
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 131 loss: 0.13972370923702954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 132 loss: 0.13939156112345782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 133 loss: 0.13932349336774727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 134 loss: 0.13909462511317053
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 135 loss: 0.1390728740228547
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 136 loss: 0.13936283795491738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 137 loss: 0.13946255263838456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 138 loss: 0.13937432021982427
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 139 loss: 0.13948129498058084
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 140 loss: 0.13933779879340102
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 141 loss: 0.13942908041232022
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 142 loss: 0.13953616422876505
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 143 loss: 0.13949664973920875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 144 loss: 0.13931201475982866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 145 loss: 0.13953314952809234
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 146 loss: 0.13954962172532734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 147 loss: 0.13933090880817298
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 148 loss: 0.13955075963324792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 149 loss: 0.13975041279656775
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 150 loss: 0.13965733582774797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 151 loss: 0.13974816263314113
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 152 loss: 0.1399256903188009
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 153 loss: 0.14007455300466687
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 154 loss: 0.1400276239138919
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 155 loss: 0.14001711121489924
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 156 loss: 0.1400154969917658
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 157 loss: 0.13995904000891243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 158 loss: 0.1400281124382834
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 159 loss: 0.14002722187799477
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 160 loss: 0.1399613416288048
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 161 loss: 0.13990686468270994
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 162 loss: 0.14005874607850005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 163 loss: 0.14015010332951516
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 164 loss: 0.14004867074147956
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 165 loss: 0.13984711454673246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 166 loss: 0.1397117509181241
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 167 loss: 0.13974608748615858
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 168 loss: 0.13958604651547613
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 169 loss: 0.13948042382150005
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 170 loss: 0.13957454088856192
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 171 loss: 0.13963398171795738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 172 loss: 0.13952365236053632
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 173 loss: 0.13940021503350639
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 174 loss: 0.1394429423942648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 175 loss: 0.13950119244200843
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 176 loss: 0.13952083661305634
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 177 loss: 0.13986961977124887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 178 loss: 0.13982712565345712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 179 loss: 0.13990344411001526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 180 loss: 0.1399586782273319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 181 loss: 0.14000129448610116
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 182 loss: 0.14012882019301037
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 183 loss: 0.1402089728113732
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 184 loss: 0.1401371183683691
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 185 loss: 0.1401099526801625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 186 loss: 0.14015212514868347
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 187 loss: 0.1402478324458561
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 188 loss: 0.14012926741641887
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 189 loss: 0.1401810321621794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 190 loss: 0.14018929785019474
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 191 loss: 0.14014408932462413
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 192 loss: 0.14038270832194635
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 193 loss: 0.1403589794132376
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 194 loss: 0.14036832592382872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 195 loss: 0.14037166616091362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 196 loss: 0.14029153858368493
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 197 loss: 0.14006029515702106
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 198 loss: 0.14007749777249615
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 199 loss: 0.13992266937266643
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 200 loss: 0.14004820998758077
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 201 loss: 0.13998687271010224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 202 loss: 0.1399602517854459
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 203 loss: 0.14004275742275962
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 204 loss: 0.1400126015365708
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 205 loss: 0.13990639844318717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 206 loss: 0.13999146747502308
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 207 loss: 0.13984389088436025
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 208 loss: 0.1397056199538593
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 209 loss: 0.13958097313437165
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 210 loss: 0.13971465730241367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 211 loss: 0.13973977169578108
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 212 loss: 0.13994080028584543
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 213 loss: 0.14006545457621694
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 214 loss: 0.14005011609085252
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 215 loss: 0.1399505309587301
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 216 loss: 0.1399489086535242
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 217 loss: 0.13994690878874697
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 218 loss: 0.13993179791290825
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 219 loss: 0.14001192193325251
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 220 loss: 0.13980889913033356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 221 loss: 0.13986197312897686
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 222 loss: 0.13984478201280842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 223 loss: 0.13992458451622805
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 224 loss: 0.13987048319540918
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 225 loss: 0.13992514997720717
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 226 loss: 0.13986086017921961
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 227 loss: 0.13997554506630625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 228 loss: 0.13987558148801327
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 229 loss: 0.13976298256342068
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 230 loss: 0.13979053623650384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 231 loss: 0.13981125445721984
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 232 loss: 0.1398186548452439
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 233 loss: 0.139733769191437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 234 loss: 0.13971310927190334
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 235 loss: 0.13962047971943592
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 236 loss: 0.13968740438379473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 237 loss: 0.13960751869130236
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 238 loss: 0.1397494926359974
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 239 loss: 0.13963790169320844
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 240 loss: 0.1395823100581765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 241 loss: 0.13956009166863945
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 242 loss: 0.13957857556086928
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 243 loss: 0.13954731772956533
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 244 loss: 0.1395336734589006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 245 loss: 0.13953125069336014
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 246 loss: 0.13937099850395832
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 247 loss: 0.13942034503346995
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 248 loss: 0.13956758040454117
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 249 loss: 0.1395851418854721
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 250 loss: 0.1396516901552677
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 251 loss: 0.13967118495370287
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 252 loss: 0.1396428250545074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 253 loss: 0.1395715452112228
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 254 loss: 0.13951193773019033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 255 loss: 0.13958063040878257
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 256 loss: 0.1395921285438817
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 257 loss: 0.13949595110541652
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 258 loss: 0.13944226240580396
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 259 loss: 0.1395111319008481
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 260 loss: 0.13951804503225362
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 261 loss: 0.13969658894671333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 262 loss: 0.1397655960883348
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 263 loss: 0.13963036457282055
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 264 loss: 0.13965688151956507
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 265 loss: 0.1396949310628873
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 266 loss: 0.13954839169194824
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 267 loss: 0.1395771370900704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 268 loss: 0.13964956968244333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 269 loss: 0.1395310993586774
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 270 loss: 0.1396104603729866
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 271 loss: 0.1395486366429892
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 272 loss: 0.13947259423816027
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 273 loss: 0.13938018682347986
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 274 loss: 0.13940771590292889
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 275 loss: 0.13929863038388166
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 276 loss: 0.13921238708755243
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 277 loss: 0.13931192452296454
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 278 loss: 0.1392284058195224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 279 loss: 0.13916888424465734
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 280 loss: 0.13922073689422437
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 281 loss: 0.13914001568674617
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 282 loss: 0.13917935351636393
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 283 loss: 0.13911829378074134
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 284 loss: 0.13914642934228333
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 285 loss: 0.13931942189994612
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 286 loss: 0.13934235655136043
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 287 loss: 0.13945398747090262
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 288 loss: 0.13936442007414168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 289 loss: 0.13922641833657626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 290 loss: 0.13921757137467122
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 291 loss: 0.13920353821565196
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 292 loss: 0.13923091137756224
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 293 loss: 0.13940192263179266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 294 loss: 0.13937029544086682
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 295 loss: 0.13944025011891026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 296 loss: 0.1395530324378932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 297 loss: 0.13951284127303648
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 298 loss: 0.13967494779085152
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 299 loss: 0.139695680036792
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 300 loss: 0.1396122486392657
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 301 loss: 0.13956764122576412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 302 loss: 0.1394270613345484
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 303 loss: 0.13944657331351007
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 304 loss: 0.1394878980017414
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 305 loss: 0.1395125573531526
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 306 loss: 0.1394803662559176
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 307 loss: 0.13941886040597473
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 308 loss: 0.13942125275150521
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 309 loss: 0.1394920436888451
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 310 loss: 0.13947154879570006
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 311 loss: 0.13941615110808248
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 312 loss: 0.13934550725687772
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 313 loss: 0.13939558202847124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 314 loss: 0.13943889765602768
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 315 loss: 0.1395924402607812
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 316 loss: 0.13958000759535197
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 317 loss: 0.13960500465770626
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 318 loss: 0.13961951767311157
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 319 loss: 0.13963740821168713
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 320 loss: 0.13959946390241385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 321 loss: 0.13958822432151094
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 322 loss: 0.1397371877979788
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 323 loss: 0.13963392842849343
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 324 loss: 0.13955469721537314
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 325 loss: 0.1396518354003246
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 326 loss: 0.1396590825741642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 327 loss: 0.13968466812408664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 328 loss: 0.13966551466231666
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 329 loss: 0.13975452636241187
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 330 loss: 0.13970366368691126
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 331 loss: 0.13968775603706382
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 332 loss: 0.1397526910057269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 333 loss: 0.13978677571893813
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 334 loss: 0.1398014231713232
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 335 loss: 0.13976850078177097
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 336 loss: 0.13982417780373776
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 337 loss: 0.13982632871197664
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 338 loss: 0.1397483765461741
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 339 loss: 0.1396767852009222
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 340 loss: 0.13969871347003124
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 341 loss: 0.13972880700029586
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 342 loss: 0.13970028288793146
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 343 loss: 0.13974018301543273
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 344 loss: 0.13972592143731755
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 345 loss: 0.1397985267682352
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 346 loss: 0.13976411114452203
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 347 loss: 0.13982799565500073
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 348 loss: 0.1398372151514237
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 349 loss: 0.1397148027461033
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 350 loss: 0.13976579372371947
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 351 loss: 0.13988036694180253
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 352 loss: 0.13985298739068888
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 353 loss: 0.13983646427774565
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 354 loss: 0.13995356079043642
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 355 loss: 0.1399113654670581
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 356 loss: 0.13989346076765757
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 357 loss: 0.13983011901044712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 358 loss: 0.13977926440911587
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 359 loss: 0.13978042430698373
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 360 loss: 0.13976812983552614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 361 loss: 0.13976923432046356
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 362 loss: 0.13986740109011614
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 363 loss: 0.13986437876526647
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 364 loss: 0.1398521940131764
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 365 loss: 0.13988844442040954
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 366 loss: 0.1399128498112569
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 367 loss: 0.13983120876859254
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 368 loss: 0.1399094091080453
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 369 loss: 0.13992018686723579
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 370 loss: 0.1400189164925266
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 371 loss: 0.14006538022399911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 372 loss: 0.14005812229488485
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 373 loss: 0.1399949383160384
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 374 loss: 0.14005470682274213
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 375 loss: 0.14004880368709563
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 376 loss: 0.14006382979928178
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 377 loss: 0.14010902820278542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 378 loss: 0.1401484172653269
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 379 loss: 0.1402587671235872
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 380 loss: 0.14034713420428727
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 381 loss: 0.14034323452964542
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 382 loss: 0.14035972913834438
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 383 loss: 0.14037846094945702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 384 loss: 0.14039764173018435
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 385 loss: 0.14032680454192223
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 386 loss: 0.14035063765827238
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 387 loss: 0.14025385433312226
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 388 loss: 0.14031939402453064
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 389 loss: 0.1402663351216169
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 390 loss: 0.14027925326656074
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 391 loss: 0.1403454231369831
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 392 loss: 0.14034815913788518
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 393 loss: 0.1403472794565839
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 394 loss: 0.14031742053664276
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 395 loss: 0.1403860283614714
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 396 loss: 0.14034265654180386
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 397 loss: 0.14033820254901794
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 398 loss: 0.14032562288582026
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 399 loss: 0.14033144591072746
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 400 loss: 0.14032013380900024
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 401 loss: 0.14040591021502702
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 402 loss: 0.14048640120459432
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 403 loss: 0.1404677353078319
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 404 loss: 0.14046204560390202
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 405 loss: 0.1404782329628497
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 406 loss: 0.14047824079400212
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 407 loss: 0.14045870873427216
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 408 loss: 0.14039669719104672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 409 loss: 0.14042658160251625
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 410 loss: 0.14041862040758132
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 411 loss: 0.14050240386866594
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 412 loss: 0.14049169786491442
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 413 loss: 0.1403967497856796
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 414 loss: 0.14030694466642135
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 415 loss: 0.14030239461775285
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 416 loss: 0.14034663697776312
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 417 loss: 0.14023774553188603
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 418 loss: 0.140208097041081
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 419 loss: 0.1402066701529703
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 420 loss: 0.14016894304326602
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 421 loss: 0.1401788979556385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 422 loss: 0.14024154361672875
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 423 loss: 0.1401955442718862
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 424 loss: 0.14023911640188605
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 425 loss: 0.14024372745962704
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 426 loss: 0.1401888527710673
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 427 loss: 0.1401886837055309
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 428 loss: 0.14024562148429523
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 429 loss: 0.14022605096821464
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 430 loss: 0.14027485660342284
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 431 loss: 0.1402693843412842
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 432 loss: 0.14037926588207483
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 433 loss: 0.14039209017026782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 434 loss: 0.1404579053139357
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 435 loss: 0.14045651418039168
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 436 loss: 0.14051685041380585
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 437 loss: 0.14056042488025036
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 438 loss: 0.14061491126747436
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 439 loss: 0.1406047323841712
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 440 loss: 0.14063968397676946
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 441 loss: 0.14065636856620814
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 442 loss: 0.14064054595668932
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 443 loss: 0.14067617119300446
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 444 loss: 0.140645783562381
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 445 loss: 0.14059635176417534
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 446 loss: 0.14056016436272672
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 447 loss: 0.14052879441971214
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 448 loss: 0.1405308793930869
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 449 loss: 0.140532432609385
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 450 loss: 0.14049050134089258
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 451 loss: 0.1404858111112715
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 452 loss: 0.14044131055078676
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 453 loss: 0.1403986392563279
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 454 loss: 0.14031236994568472
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 455 loss: 0.14022993699207412
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 456 loss: 0.14023289550095797
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 457 loss: 0.14021659730495456
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 458 loss: 0.1401764447251782
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 459 loss: 0.14009697547016062
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 460 loss: 0.14011760159679082
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 461 loss: 0.1401294155001899
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 462 loss: 0.14008428317643865
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 463 loss: 0.1400648286288552
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 464 loss: 0.1401285753296367
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 465 loss: 0.1400992682864589
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 466 loss: 0.1401235699461765
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 467 loss: 0.14016228540934128
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 468 loss: 0.14016557745953911
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 469 loss: 0.14017327692208767
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 470 loss: 0.14008311511988336
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 471 loss: 0.14012738856666912
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
  batch 472 loss: 0.14007956244177738
####################
Base Model Weight Norm = tensor(0.)
Model Weight Norm = tensor(0.)
####################
LOSS train 0.14007956244177738 valid 0.18866445124149323
LOSS train 0.14007956244177738 valid 0.2129204273223877
LOSS train 0.14007956244177738 valid 0.22509197890758514
LOSS train 0.14007956244177738 valid 0.2226334549486637
LOSS train 0.14007956244177738 valid 0.2217085838317871
LOSS train 0.14007956244177738 valid 0.22832372784614563
LOSS train 0.14007956244177738 valid 0.22279589942523412
LOSS train 0.14007956244177738 valid 0.22258274629712105
LOSS train 0.14007956244177738 valid 0.22262700233194563
LOSS train 0.14007956244177738 valid 0.2234686866402626
LOSS train 0.14007956244177738 valid 0.21970226954330097
LOSS train 0.14007956244177738 valid 0.2218960995475451
LOSS train 0.14007956244177738 valid 0.22052134917332575
LOSS train 0.14007956244177738 valid 0.22081047083650315
LOSS train 0.14007956244177738 valid 0.2195062557856242
LOSS train 0.14007956244177738 valid 0.21966586634516716
LOSS train 0.14007956244177738 valid 0.22018100145985098
LOSS train 0.14007956244177738 valid 0.22019630836115944
LOSS train 0.14007956244177738 valid 0.22212137360321849
LOSS train 0.14007956244177738 valid 0.22169090509414674
LOSS train 0.14007956244177738 valid 0.2225195240406763
LOSS train 0.14007956244177738 valid 0.2206711322069168
LOSS train 0.14007956244177738 valid 0.21897938912329468
LOSS train 0.14007956244177738 valid 0.21961290575563908
LOSS train 0.14007956244177738 valid 0.21821033775806428
LOSS train 0.14007956244177738 valid 0.2178526549385144
LOSS train 0.14007956244177738 valid 0.2175339107160215
LOSS train 0.14007956244177738 valid 0.21760087034532002
LOSS train 0.14007956244177738 valid 0.2162815162847782
LOSS train 0.14007956244177738 valid 0.2154869019985199
LOSS train 0.14007956244177738 valid 0.21463896286103032
LOSS train 0.14007956244177738 valid 0.21555807394906878
LOSS train 0.14007956244177738 valid 0.21422874069575107
LOSS train 0.14007956244177738 valid 0.21343720353701534
LOSS train 0.14007956244177738 valid 0.2137777077300208
LOSS train 0.14007956244177738 valid 0.21386967640784052
LOSS train 0.14007956244177738 valid 0.21275943598231753
LOSS train 0.14007956244177738 valid 0.2126796053428399
LOSS train 0.14007956244177738 valid 0.21236302149601471
LOSS train 0.14007956244177738 valid 0.21193903721868992
LOSS train 0.14007956244177738 valid 0.21194282864652028
LOSS train 0.14007956244177738 valid 0.21314809826158343
LOSS train 0.14007956244177738 valid 0.2140066911314809
LOSS train 0.14007956244177738 valid 0.2131196124987169
LOSS train 0.14007956244177738 valid 0.2124267942375607
LOSS train 0.14007956244177738 valid 0.21073803661958032
LOSS train 0.14007956244177738 valid 0.21000253773750144
LOSS train 0.14007956244177738 valid 0.2114491437872251
LOSS train 0.14007956244177738 valid 0.21136597832854914
LOSS train 0.14007956244177738 valid 0.21172620266675948
LOSS train 0.14007956244177738 valid 0.21145674323334412
LOSS train 0.14007956244177738 valid 0.21124668505329353
LOSS train 0.14007956244177738 valid 0.21223848973805048
LOSS train 0.14007956244177738 valid 0.21211970183584425
LOSS train 0.14007956244177738 valid 0.21237156364050777
LOSS train 0.14007956244177738 valid 0.2120442816189357
LOSS train 0.14007956244177738 valid 0.2113541699292367
LOSS train 0.14007956244177738 valid 0.21223163810269585
LOSS train 0.14007956244177738 valid 0.21244833656286788
LOSS train 0.14007956244177738 valid 0.21206954196095468
LOSS train 0.14007956244177738 valid 0.21184157079360524
LOSS train 0.14007956244177738 valid 0.21144620064766176
LOSS train 0.14007956244177738 valid 0.2114825795094172
LOSS train 0.14007956244177738 valid 0.2113473613280803
LOSS train 0.14007956244177738 valid 0.21066497610165522
LOSS train 0.14007956244177738 valid 0.2106003470041535
LOSS train 0.14007956244177738 valid 0.2111785013729067
LOSS train 0.14007956244177738 valid 0.2105049313429524
LOSS train 0.14007956244177738 valid 0.21084694521150726
LOSS train 0.14007956244177738 valid 0.21127715408802034
LOSS train 0.14007956244177738 valid 0.21142179823257554
LOSS train 0.14007956244177738 valid 0.21190377697348595
LOSS train 0.14007956244177738 valid 0.21216180614412647
LOSS train 0.14007956244177738 valid 0.21219317514348673
LOSS train 0.14007956244177738 valid 0.2119370526075363
LOSS train 0.14007956244177738 valid 0.21213025580111303
LOSS train 0.14007956244177738 valid 0.21210565524441855
LOSS train 0.14007956244177738 valid 0.2120731356434333
LOSS train 0.14007956244177738 valid 0.21231886574739142
LOSS train 0.14007956244177738 valid 0.2121099131181836
LOSS train 0.14007956244177738 valid 0.21224862723438828
LOSS train 0.14007956244177738 valid 0.21255762493465005
LOSS train 0.14007956244177738 valid 0.2122015769941261
LOSS train 0.14007956244177738 valid 0.21203966722601936
LOSS train 0.14007956244177738 valid 0.21226325806449442
LOSS train 0.14007956244177738 valid 0.2123512604901957
LOSS train 0.14007956244177738 valid 0.21194543098581248
LOSS train 0.14007956244177738 valid 0.2118445744907314
LOSS train 0.14007956244177738 valid 0.2119918256328347
LOSS train 0.14007956244177738 valid 0.21177822401126226
LOSS train 0.14007956244177738 valid 0.21187597283950219
LOSS train 0.14007956244177738 valid 0.21192454773446787
LOSS train 0.14007956244177738 valid 0.2122116531095197
LOSS train 0.14007956244177738 valid 0.21264133554823855
LOSS train 0.14007956244177738 valid 0.21309157108005725
LOSS train 0.14007956244177738 valid 0.21403421213229498
LOSS train 0.14007956244177738 valid 0.21421075420281321
LOSS train 0.14007956244177738 valid 0.21478867257127957
LOSS train 0.14007956244177738 valid 0.2146273920632372
LOSS train 0.14007956244177738 valid 0.21495852053165435
LOSS train 0.14007956244177738 valid 0.2148735278313703
LOSS train 0.14007956244177738 valid 0.21542461683937147
LOSS train 0.14007956244177738 valid 0.21500143698118265
LOSS train 0.14007956244177738 valid 0.2148081259085582
LOSS train 0.14007956244177738 valid 0.21503386937436603
LOSS train 0.14007956244177738 valid 0.21496606871204557
LOSS train 0.14007956244177738 valid 0.21477448606045446
LOSS train 0.14007956244177738 valid 0.21475829322029044
LOSS train 0.14007956244177738 valid 0.21417552697549172
LOSS train 0.14007956244177738 valid 0.2138767743652517
LOSS train 0.14007956244177738 valid 0.21386676436071997
LOSS train 0.14007956244177738 valid 0.21409553622028657
LOSS train 0.14007956244177738 valid 0.21385593250789472
LOSS train 0.14007956244177738 valid 0.21407051567445723
LOSS train 0.14007956244177738 valid 0.21449863055477972
LOSS train 0.14007956244177738 valid 0.21446214012544731
LOSS train 0.14007956244177738 valid 0.21449084707304963
LOSS train 0.14007956244177738 valid 0.21452333626605696
LOSS train 0.14007956244177738 valid 0.21431176214158035
LOSS train 0.14007956244177738 valid 0.21432231900592644
LOSS train 0.14007956244177738 valid 0.21431924033263497
LOSS train 0.14007956244177738 valid 0.21435027887098124
LOSS train 0.14007956244177738 valid 0.21433472148771207
LOSS train 0.14007956244177738 valid 0.21447119477295107
LOSS train 0.14007956244177738 valid 0.21471698594093322
LOSS train 0.14007956244177738 valid 0.21456268926461539
LOSS train 0.14007956244177738 valid 0.2143771114546483
LOSS train 0.14007956244177738 valid 0.21429114963393658
LOSS train 0.14007956244177738 valid 0.21420098714126173
LOSS train 0.14007956244177738 valid 0.21394956707954407
LOSS train 0.14007956244177738 valid 0.21403020825094848
LOSS train 0.14007956244177738 valid 0.21421220063260107
LOSS train 0.14007956244177738 valid 0.21416571355403813
LOSS train 0.14007956244177738 valid 0.21421217751591953
LOSS train 0.14007956244177738 valid 0.2142709528958356
LOSS train 0.14007956244177738 valid 0.21448257653152242
LOSS train 0.14007956244177738 valid 0.21429355312002837
LOSS train 0.14007956244177738 valid 0.21386832365955133
LOSS train 0.14007956244177738 valid 0.2138632180021821
LOSS train 0.14007956244177738 valid 0.21367895816053664
LOSS train 0.14007956244177738 valid 0.21396485541729218
LOSS train 0.14007956244177738 valid 0.214194505147531
LOSS train 0.14007956244177738 valid 0.21431856659742501
LOSS train 0.14007956244177738 valid 0.21423467248678207
LOSS train 0.14007956244177738 valid 0.21403738506909076
LOSS train 0.14007956244177738 valid 0.2140627827954619
LOSS train 0.14007956244177738 valid 0.21402848537276392
LOSS train 0.14007956244177738 valid 0.21377340845159581
LOSS train 0.14007956244177738 valid 0.21398297672303732
LOSS train 0.14007956244177738 valid 0.2138851837317149
LOSS train 0.14007956244177738 valid 0.21378090849380618
LOSS train 0.14007956244177738 valid 0.21384157514885851
LOSS train 0.14007956244177738 valid 0.21360056891160853
LOSS train 0.14007956244177738 valid 0.21363577517596158
LOSS train 0.14007956244177738 valid 0.2134312556635949
LOSS train 0.14007956244177738 valid 0.21364242975146341
LOSS train 0.14007956244177738 valid 0.2134530225377174
LOSS train 0.14007956244177738 valid 0.21334676557703863
LOSS train 0.14007956244177738 valid 0.21365645565326857
LOSS train 0.14007956244177738 valid 0.21360549172386528
LOSS train 0.14007956244177738 valid 0.21378203735958715
LOSS train 0.14007956244177738 valid 0.21379652187044237
LOSS train 0.14007956244177738 valid 0.21356909022740792
LOSS train 0.14007956244177738 valid 0.2135550934912228
LOSS train 0.14007956244177738 valid 0.21370190661964994
LOSS train 0.14007956244177738 valid 0.213605738188847
LOSS train 0.14007956244177738 valid 0.21372269700744195
LOSS train 0.14007956244177738 valid 0.21336006293339388
LOSS train 0.14007956244177738 valid 0.21318985751042
LOSS train 0.14007956244177738 valid 0.21301569500390222
LOSS train 0.14007956244177738 valid 0.21284056921102848
LOSS train 0.14007956244177738 valid 0.21294625805214393
LOSS train 0.14007956244177738 valid 0.2130559544170523
LOSS train 0.14007956244177738 valid 0.21308336578223896
LOSS train 0.14007956244177738 valid 0.21277773686817714
LOSS train 0.14007956244177738 valid 0.21276149145242843
LOSS train 0.14007956244177738 valid 0.21260879656015816
LOSS train 0.14007956244177738 valid 0.21263418321529132
LOSS train 0.14007956244177738 valid 0.21261607884694744
LOSS train 0.14007956244177738 valid 0.21260470988021957
LOSS train 0.14007956244177738 valid 0.21247627367960156
LOSS train 0.14007956244177738 valid 0.2123590566136025
LOSS train 0.14007956244177738 valid 0.21251894224211165
LOSS train 0.14007956244177738 valid 0.21256204419161961
LOSS train 0.14007956244177738 valid 0.21262061724791656
LOSS train 0.14007956244177738 valid 0.2128299444272954
LOSS train 0.14007956244177738 valid 0.21277784018274298
LOSS train 0.14007956244177738 valid 0.21273263092053699
LOSS train 0.14007956244177738 valid 0.21239553723070356
LOSS train 0.14007956244177738 valid 0.21235442130189192
LOSS train 0.14007956244177738 valid 0.21247607719211678
LOSS train 0.14007956244177738 valid 0.2124812942929566
LOSS train 0.14007956244177738 valid 0.21221805869606492
LOSS train 0.14007956244177738 valid 0.21231077718980534
LOSS train 0.14007956244177738 valid 0.21221644236491277
LOSS train 0.14007956244177738 valid 0.2124419683704571
LOSS train 0.14007956244177738 valid 0.21261527748579906
LOSS train 0.14007956244177738 valid 0.21269588330478378
LOSS train 0.14007956244177738 valid 0.21277281035430468
LOSS train 0.14007956244177738 valid 0.2129692444950342
LOSS train 0.14007956244177738 valid 0.2129969247686329
LOSS train 0.14007956244177738 valid 0.21318375584807728
LOSS train 0.14007956244177738 valid 0.2133132802882218
LOSS train 0.14007956244177738 valid 0.21310584064500004
LOSS train 0.14007956244177738 valid 0.21293333252755606
LOSS train 0.14007956244177738 valid 0.21288542514576494
LOSS train 0.14007956244177738 valid 0.21272659503319413
LOSS train 0.14007956244177738 valid 0.21253155537236196
LOSS train 0.14007956244177738 valid 0.21255685367652674
LOSS train 0.14007956244177738 valid 0.21281486934139615
LOSS train 0.14007956244177738 valid 0.21309986484559226
LOSS train 0.14007956244177738 valid 0.21294934436116578
LOSS train 0.14007956244177738 valid 0.21290341180534988
LOSS train 0.14007956244177738 valid 0.21284256074751648
LOSS train 0.14007956244177738 valid 0.2125676644402881
LOSS train 0.14007956244177738 valid 0.21233478498955569
LOSS train 0.14007956244177738 valid 0.21242173537955306
LOSS train 0.14007956244177738 valid 0.21235434957053684
LOSS train 0.14007956244177738 valid 0.2124714294116791
LOSS train 0.14007956244177738 valid 0.21239866146987135
LOSS train 0.14007956244177738 valid 0.21235818318112404
LOSS train 0.14007956244177738 valid 0.21251038367952313
LOSS train 0.14007956244177738 valid 0.21237400381287116
LOSS train 0.14007956244177738 valid 0.21225300131897842
LOSS train 0.14007956244177738 valid 0.21193965905242496
LOSS train 0.14007956244177738 valid 0.21173504219118472
LOSS train 0.14007956244177738 valid 0.21172527916368408
LOSS train 0.14007956244177738 valid 0.21178416901251726
LOSS train 0.14007956244177738 valid 0.211729971147
LOSS train 0.14007956244177738 valid 0.21186093221540037
LOSS train 0.14007956244177738 valid 0.2119285146266351
LOSS train 0.14007956244177738 valid 0.21213326493984666
LOSS train 0.14007956244177738 valid 0.21226514779958602
LOSS train 0.14007956244177738 valid 0.21223270842152783
LOSS train 0.14007956244177738 valid 0.21220164235602035
LOSS train 0.14007956244177738 valid 0.21197297960772352
LOSS train 0.14007956244177738 valid 0.2120185335351445
LOSS train 0.14007956244177738 valid 0.21208424133663417
LOSS train 0.14007956244177738 valid 0.21208572425104086
LOSS train 0.14007956244177738 valid 0.2116735201018552
LOSS train 0.14007956244177738 valid 0.21159281692935222
LOSS train 0.14007956244177738 valid 0.21166830118899502
LOSS train 0.14007956244177738 valid 0.21172302657807315
LOSS train 0.14007956244177738 valid 0.2117750850796211
LOSS train 0.14007956244177738 valid 0.2119585223039802
LOSS train 0.14007956244177738 valid 0.21208555242273866
LOSS train 0.14007956244177738 valid 0.2120836514994683
LOSS train 0.14007956244177738 valid 0.21200051846643608
LOSS train 0.14007956244177738 valid 0.21189963270023646
LOSS train 0.14007956244177738 valid 0.21202797177433969
LOSS train 0.14007956244177738 valid 0.21211472031960923
LOSS train 0.14007956244177738 valid 0.2121664806904774
LOSS train 0.14007956244177738 valid 0.21205622655718695
LOSS train 0.14007956244177738 valid 0.21213535151965035
LOSS train 0.14007956244177738 valid 0.2121461842866505
LOSS train 0.14007956244177738 valid 0.2121917084150482
LOSS train 0.14007956244177738 valid 0.21209231926425423
LOSS train 0.14007956244177738 valid 0.21219143516911093
LOSS train 0.14007956244177738 valid 0.21211844058455648
LOSS train 0.14007956244177738 valid 0.21204035881047065
LOSS train 0.14007956244177738 valid 0.2120277570753262
LOSS train 0.14007956244177738 valid 0.21231026455532503
LOSS train 0.14007956244177738 valid 0.2121857299040932
LOSS train 0.14007956244177738 valid 0.2121782372564529
LOSS train 0.14007956244177738 valid 0.212270846203813
LOSS train 0.14007956244177738 valid 0.21236258434285796
LOSS train 0.14007956244177738 valid 0.21232027303748363
LOSS train 0.14007956244177738 valid 0.21235142061625845
LOSS train 0.14007956244177738 valid 0.21242789112613103
LOSS train 0.14007956244177738 valid 0.212276175369819
LOSS train 0.14007956244177738 valid 0.21219118745014676
LOSS train 0.14007956244177738 valid 0.2120000526911634
LOSS train 0.14007956244177738 valid 0.21186925506307966
LOSS train 0.14007956244177738 valid 0.2118309951883598
LOSS train 0.14007956244177738 valid 0.2119206895882433
LOSS train 0.14007956244177738 valid 0.2120534312519906
LOSS train 0.14007956244177738 valid 0.21195682961265103
LOSS train 0.14007956244177738 valid 0.21193951295970156
LOSS train 0.14007956244177738 valid 0.2120191003312774
LOSS train 0.14007956244177738 valid 0.2118204254390938
LOSS train 0.14007956244177738 valid 0.21167765173305397
LOSS train 0.14007956244177738 valid 0.2116280328878697
LOSS train 0.14007956244177738 valid 0.21159727336977058
LOSS train 0.14007956244177738 valid 0.21140463673837587
LOSS train 0.14007956244177738 valid 0.2115130051447634
LOSS train 0.14007956244177738 valid 0.21150804308431964
LOSS train 0.14007956244177738 valid 0.2116474278972124
LOSS train 0.14007956244177738 valid 0.2116023331311428
LOSS train 0.14007956244177738 valid 0.2115811371215487
LOSS train 0.14007956244177738 valid 0.2114667625005903
LOSS train 0.14007956244177738 valid 0.21143738262329725
LOSS train 0.14007956244177738 valid 0.21142330977504384
LOSS train 0.14007956244177738 valid 0.21137306152655402
LOSS train 0.14007956244177738 valid 0.21136008205563844
LOSS train 0.14007956244177738 valid 0.21135216145697286
LOSS train 0.14007956244177738 valid 0.21138427056674217
LOSS train 0.14007956244177738 valid 0.21141268385680836
LOSS train 0.14007956244177738 valid 0.2114331615501202
LOSS train 0.14007956244177738 valid 0.21145300604527612
LOSS train 0.14007956244177738 valid 0.21146520383656026
LOSS train 0.14007956244177738 valid 0.2114374548246298
LOSS train 0.14007956244177738 valid 0.21142098931780715
LOSS train 0.14007956244177738 valid 0.21146035329638535
LOSS train 0.14007956244177738 valid 0.21142145008534977
LOSS train 0.14007956244177738 valid 0.21130877369251408
LOSS train 0.14007956244177738 valid 0.21144394087148646
LOSS train 0.14007956244177738 valid 0.21135299181996417
LOSS train 0.14007956244177738 valid 0.21144974055131535
LOSS train 0.14007956244177738 valid 0.2114729876705358
LOSS train 0.14007956244177738 valid 0.2115036283048891
LOSS train 0.14007956244177738 valid 0.21153988646061858
LOSS train 0.14007956244177738 valid 0.21151277033659893
LOSS train 0.14007956244177738 valid 0.21150422270019975
LOSS train 0.14007956244177738 valid 0.21149865497544312
LOSS train 0.14007956244177738 valid 0.21153861002789603
LOSS train 0.14007956244177738 valid 0.2115382911236603
LOSS train 0.14007956244177738 valid 0.21151210304220391
LOSS train 0.14007956244177738 valid 0.2115832544019762
LOSS train 0.14007956244177738 valid 0.2115842342423421
LOSS train 0.14007956244177738 valid 0.21144680238794536
LOSS train 0.14007956244177738 valid 0.21145343211859557
LOSS train 0.14007956244177738 valid 0.2113769550732574
LOSS train 0.14007956244177738 valid 0.21133288360509342
LOSS train 0.14007956244177738 valid 0.21131274345572348
LOSS train 0.14007956244177738 valid 0.211133302289706
LOSS train 0.14007956244177738 valid 0.21122037987310463
LOSS train 0.14007956244177738 valid 0.21110083775691665
LOSS train 0.14007956244177738 valid 0.21116089582352376
LOSS train 0.14007956244177738 valid 0.21120254226640364
LOSS train 0.14007956244177738 valid 0.2112856991137519
LOSS train 0.14007956244177738 valid 0.21118334019022045
LOSS train 0.14007956244177738 valid 0.21116678845361772
LOSS train 0.14007956244177738 valid 0.21115905088973833
LOSS train 0.14007956244177738 valid 0.21110507571500933
LOSS train 0.14007956244177738 valid 0.21096834413596052
LOSS train 0.14007956244177738 valid 0.21088103601886404
LOSS train 0.14007956244177738 valid 0.21101784960364023
LOSS train 0.14007956244177738 valid 0.21096373832172896
LOSS train 0.14007956244177738 valid 0.21085545449218215
LOSS train 0.14007956244177738 valid 0.210903747620828
LOSS train 0.14007956244177738 valid 0.21094978609480117
LOSS train 0.14007956244177738 valid 0.21098362822194547
LOSS train 0.14007956244177738 valid 0.21084808705933936
LOSS train 0.14007956244177738 valid 0.21095667322447828
LOSS train 0.14007956244177738 valid 0.2110188284429951
LOSS train 0.14007956244177738 valid 0.21092382114315997
LOSS train 0.14007956244177738 valid 0.21087899849205263
LOSS train 0.14007956244177738 valid 0.2108611947289486
LOSS train 0.14007956244177738 valid 0.2108469004062322
LOSS train 0.14007956244177738 valid 0.21089165459786144
LOSS train 0.14007956244177738 valid 0.2109475449321956
LOSS train 0.14007956244177738 valid 0.21097996701825072
LOSS train 0.14007956244177738 valid 0.2110694770730251
LOSS train 0.14007956244177738 valid 0.21093925330682664
LOSS train 0.14007956244177738 valid 0.21081368518127522
LOSS train 0.14007956244177738 valid 0.21078203658290792
LOSS train 0.14007956244177738 valid 0.21081438403920966
LOSS train 0.14007956244177738 valid 0.21093386097410538
LOSS train 0.14007956244177738 valid 0.21093708046382517
LOSS train 0.14007956244177738 valid 0.21123121813353565
LOSS train 0.14007956244177738 valid 0.2111835604600629
LOSS train 0.14007956244177738 valid 0.21134578558045197
LOSS train 0.14007956244177738 valid 0.2113631003364387
LOSS train 0.14007956244177738 valid 0.21125949430023575
LOSS train 0.14007956244177738 valid 0.21127853203717975
LOSS train 0.14007956244177738 valid 0.21127653229839163
LOSS train 0.14007956244177738 valid 0.21131775576915662
LOSS train 0.14007956244177738 valid 0.21117183685545687
LOSS train 0.14007956244177738 valid 0.2112819724249323
Selected network (scan_genome.py) = bimodal
bimodal selected
bichrom_seq(
  (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
  (relu): ReLU()
  (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
  (lstm): LSTM(256, 32, batch_first=True)
  (tanh): Tanh()
  (model_dense_repeat): Sequential(
    (0): Linear(in_features=32, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=512, out_features=512, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.5, inplace=False)
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
bimodal_network(
  (base_model): bichrom_seq(
    (conv1d): Conv1d(4, 256, kernel_size=(24,), stride=(1,))
    (relu): ReLU()
    (batchNorm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (maxPool1d): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=True)
    (lstm): LSTM(256, 32, batch_first=True)
    (tanh): Tanh()
    (model_dense_repeat): Sequential(
      (0): Linear(in_features=32, out_features=512, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=512, out_features=512, bias=True)
      (7): ReLU()
      (8): Dropout(p=0.5, inplace=False)
    )
    (linear): Linear(in_features=512, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
  (linear): Linear(in_features=512, out_features=1, bias=True)
  (tanh): Tanh()
  (model): bichrom_chrom(
    (_reshape): _reshape()
    (conv1d): Conv1d(12, 15, kernel_size=(1,), stride=(1,), padding=valid)
    (relu): ReLU()
    (lstm): LSTM(15, 5, batch_first=True)
    (relu2): ReLU()
    (linear): Linear(in_features=5, out_features=1, bias=True)
    (tanh): Tanh()
  )
  (linear2): Linear(in_features=2, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
0.4061461794019934
0.6727574750830565
Elapsed: 6hrs 18min 12sec
